{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from collections import Counter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import platform\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM\n",
    "from keras_contrib.layers import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train = _parse_data(open('data/train_data.txt', 'rb'))\n",
    "    test = _parse_data(open('data/test_data.txt', 'rb'))\n",
    "\n",
    "    word_counts = Counter(row[0].lower() for sample in train for row in sample)\n",
    "    vocab = [w for w, f in iter(word_counts.items()) if f >= 1]\n",
    "    chunk_tags = ['B-K'\n",
    "    ,'E-K'\n",
    "    ,'I-K'\n",
    "    ,'O']\n",
    "\n",
    "    train = _process_data(train, vocab, chunk_tags)\n",
    "    test = _process_data(test, vocab, chunk_tags)\n",
    "    return train, test, (vocab, chunk_tags)\n",
    "\n",
    "\n",
    "def _parse_data(fh):\n",
    "    split_text = '\\n'\n",
    "    string = fh.read().decode('utf-8')\n",
    "    data = [[row.split() for row in sample.split(split_text)] for sample in string.strip().split(split_text + split_text)]\n",
    "    fh.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "def _process_data(data, vocab, chunk_tags, maxlen=None):\n",
    "    if maxlen is None:\n",
    "        maxlen = max(len(s) for s in data)\n",
    "    word2idx = dict((w, i) for i, w in enumerate(vocab))\n",
    "    x = [[word2idx.get(w[0].lower(), 1) for w in s] for s in data]  \n",
    "\n",
    "    y_chunk = [[chunk_tags.index(w[1]) for w in s] for s in data]\n",
    "\n",
    "    x = pad_sequences(x, maxlen)  # left padding\n",
    "\n",
    "    y_chunk = pad_sequences(y_chunk, maxlen, value=-1)\n",
    "\n",
    "    y_chunk = numpy.expand_dims(y_chunk, 2)\n",
    "    \n",
    "    return x, y_chunk\n",
    "\n",
    "\n",
    "def process_data(data, vocab, maxlen=100):\n",
    "    word2idx = dict((w, i) for i, w in enumerate(vocab))\n",
    "    x = [word2idx.get(w[0].lower(), 1) for w in data]\n",
    "    length = len(x)\n",
    "    x = pad_sequences([x], maxlen)  # left padding\n",
    "    return x, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 20\n",
    "BiRNN_UNITS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y), (vocab, chunk_tags) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras_contrib.layers import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 20)          13300     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, None, 200)         96800     \n",
      "_________________________________________________________________\n",
      "crf_2 (CRF)                  (None, None, 4)           828       \n",
      "=================================================================\n",
      "Total params: 110,928\n",
      "Trainable params: 110,928\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(vocab), EMBED_DIM, mask_zero=True))  # 输入层，隐藏层\n",
    "model.add(Bidirectional(LSTM(BiRNN_UNITS, return_sequences=True)))\n",
    "crf = CRF(len(chunk_tags), sparse_target=True)\n",
    "model.add(crf)\n",
    "model.summary()\n",
    "model.compile('adam', loss=crf.loss_function, metrics=[crf.accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 556 samples, validate on 166 samples\n",
      "Epoch 1/15\n",
      "556/556 [==============================] - 6s 11ms/step - loss: 1.8328 - acc: 0.7388 - val_loss: 1.3959 - val_acc: 0.7994\n",
      "Epoch 2/15\n",
      "556/556 [==============================] - 3s 5ms/step - loss: 1.5715 - acc: 0.7310 - val_loss: 1.3323 - val_acc: 0.8148\n",
      "Epoch 3/15\n",
      "556/556 [==============================] - 3s 6ms/step - loss: 1.4064 - acc: 0.8331 - val_loss: 1.2556 - val_acc: 0.8721\n",
      "Epoch 4/15\n",
      "556/556 [==============================] - 3s 6ms/step - loss: 1.2669 - acc: 0.9072 - val_loss: 1.1761 - val_acc: 0.9384\n",
      "Epoch 5/15\n",
      "556/556 [==============================] - 4s 7ms/step - loss: 1.2027 - acc: 0.9449 - val_loss: 1.1279 - val_acc: 0.9621\n",
      "Epoch 6/15\n",
      "556/556 [==============================] - 4s 7ms/step - loss: 1.1527 - acc: 0.9611 - val_loss: 1.0827 - val_acc: 0.9707\n",
      "Epoch 7/15\n",
      "556/556 [==============================] - 3s 6ms/step - loss: 1.1221 - acc: 0.9693 - val_loss: 1.0647 - val_acc: 0.9773\n",
      "Epoch 8/15\n",
      "556/556 [==============================] - 3s 6ms/step - loss: 1.1017 - acc: 0.9767 - val_loss: 1.0349 - val_acc: 0.9855\n",
      "Epoch 9/15\n",
      "556/556 [==============================] - 3s 6ms/step - loss: 1.0812 - acc: 0.9810 - val_loss: 1.0235 - val_acc: 0.9859\n",
      "Epoch 10/15\n",
      "556/556 [==============================] - 3s 6ms/step - loss: 1.0679 - acc: 0.9862 - val_loss: 1.0061 - val_acc: 0.9913\n",
      "Epoch 11/15\n",
      "556/556 [==============================] - 3s 5ms/step - loss: 1.0553 - acc: 0.9888 - val_loss: 1.0021 - val_acc: 0.9923\n",
      "Epoch 12/15\n",
      "556/556 [==============================] - 3s 5ms/step - loss: 1.0472 - acc: 0.9909 - val_loss: 0.9933 - val_acc: 0.9943\n",
      "Epoch 13/15\n",
      "556/556 [==============================] - 3s 6ms/step - loss: 1.0390 - acc: 0.9921 - val_loss: 0.9925 - val_acc: 0.9940\n",
      "Epoch 14/15\n",
      "556/556 [==============================] - 3s 5ms/step - loss: 1.0346 - acc: 0.9930 - val_loss: 0.9888 - val_acc: 0.9936\n",
      "Epoch 15/15\n",
      "556/556 [==============================] - 3s 6ms/step - loss: 1.0329 - acc: 0.9942 - val_loss: 0.9839 - val_acc: 0.9965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11cbe25f8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y,batch_size=16,epochs=15, validation_data=[test_x, test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model/config.pkl', 'wb') as outp:\n",
    "    pickle.dump((vocab, chunk_tags), outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/crf_company.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 北京美丽屋房产经纪有限公司 -->  美丽屋\n",
    "# 1. 正则匹配+字典：\n",
    "# 2. HMM \n",
    "# 3. tf-idf\n",
    "# 4. LR \n",
    "# 5. BiLSTM+CRF\n",
    "# 6. CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/_t/wschnxms2rlgr_txwwx6p1g40000gn/T/jieba.cache\n",
      "Loading model cost 1.825 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['北京', '美丽', '屋', '房产', '经纪', '有限公司']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.lcut('北京美丽屋房产经纪有限公司')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HMM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
