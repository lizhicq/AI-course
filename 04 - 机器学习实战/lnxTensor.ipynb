{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "train=pd.read_csv('./data/train1.csv')\n",
    "y=np.asarray(train['SalePrice'])\n",
    "train1=train.drop(['Id','SalePrice'],axis=1)\n",
    "X=np.asarray(pd.get_dummies(train1).reset_index(drop=True))\n",
    "X_train,X_test,y_train,y_test=tts(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lizhi/anaconda2/envs/python3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=X_train.shape[1], activation='relu' ))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam',loss='mean_squared_error')\n",
    "    return model \n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                3050      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30)                330       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 3,411\n",
      "Trainable params: 3,411\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lizhi/anaconda2/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/3350\n",
      "1168/1168 [==============================] - 0s 246us/step - loss: 38790977900.7123 - val_loss: 36551574906.7397\n",
      "Epoch 2/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 36623220343.2329 - val_loss: 33419534167.6712\n",
      "Epoch 3/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 31848129493.9178 - val_loss: 27361158887.4521\n",
      "Epoch 4/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 24202411583.1233 - val_loss: 18998172601.8630\n",
      "Epoch 5/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 15786975807.1233 - val_loss: 11020484923.6164\n",
      "Epoch 6/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 10088822208.8767 - val_loss: 6476146337.3151\n",
      "Epoch 7/3350\n",
      "1168/1168 [==============================] - 0s 55us/step - loss: 7947121004.7123 - val_loss: 4871216529.5342\n",
      "Epoch 8/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 7230004339.7260 - val_loss: 4364645642.5205\n",
      "Epoch 9/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 6806492717.5890 - val_loss: 4209903377.5342\n",
      "Epoch 10/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 6420502489.4247 - val_loss: 3941824113.9726\n",
      "Epoch 11/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 6120157150.6849 - val_loss: 3827782643.7260\n",
      "Epoch 12/3350\n",
      "1168/1168 [==============================] - 0s 54us/step - loss: 5872194987.8356 - val_loss: 3698849569.3151\n",
      "Epoch 13/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 5606701445.2603 - val_loss: 3556484485.2603\n",
      "Epoch 14/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 5359917171.7260 - val_loss: 3492526160.6575\n",
      "Epoch 15/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 5177957188.3836 - val_loss: 3387352931.9452\n",
      "Epoch 16/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 4977590231.6712 - val_loss: 3357965120.8767\n",
      "Epoch 17/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 4813999475.7260 - val_loss: 3284745998.0274\n",
      "Epoch 18/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 4674091327.1233 - val_loss: 3232987192.1096\n",
      "Epoch 19/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 4504936588.2740 - val_loss: 3140888318.2466\n",
      "Epoch 20/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 4379063257.4247 - val_loss: 3093616163.9452\n",
      "Epoch 21/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 4240608636.4932 - val_loss: 3035822392.1096\n",
      "Epoch 22/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 4107514147.0685 - val_loss: 3006664628.6027\n",
      "Epoch 23/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 3974032352.4384 - val_loss: 2925086890.9589\n",
      "Epoch 24/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 3844982580.6027 - val_loss: 2882054373.6986\n",
      "Epoch 25/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 3709587820.7123 - val_loss: 2820437158.5753\n",
      "Epoch 26/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 3600887148.7123 - val_loss: 2773803328.8767\n",
      "Epoch 27/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 3497263719.4521 - val_loss: 2700780323.0685\n",
      "Epoch 28/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 3399750287.7808 - val_loss: 2652186615.2329\n",
      "Epoch 29/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 3308039795.7260 - val_loss: 2590038966.3562\n",
      "Epoch 30/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 3229236818.4110 - val_loss: 2537071870.2466\n",
      "Epoch 31/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 3146476361.6438 - val_loss: 2479070382.4658\n",
      "Epoch 32/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 3076318749.8082 - val_loss: 2447351359.1233\n",
      "Epoch 33/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 3020649805.1507 - val_loss: 2374763113.2055\n",
      "Epoch 34/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 2951695068.9315 - val_loss: 2320274161.9726\n",
      "Epoch 35/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 2894507902.2466 - val_loss: 2265850876.4932\n",
      "Epoch 36/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 2842923470.9041 - val_loss: 2218630127.3425\n",
      "Epoch 37/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 2791441660.4932 - val_loss: 2162309890.6301\n",
      "Epoch 38/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 2745388459.8356 - val_loss: 2106921426.4110\n",
      "Epoch 39/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 2686950976.8767 - val_loss: 2073956455.4521\n",
      "Epoch 40/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 2649888166.5753 - val_loss: 2029829492.6027\n",
      "Epoch 41/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 2599544709.2603 - val_loss: 1983877983.5616\n",
      "Epoch 42/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 2557964508.9315 - val_loss: 1942138040.9863\n",
      "Epoch 43/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 2514322230.3562 - val_loss: 1930514333.8082\n",
      "Epoch 44/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 2494612602.7397 - val_loss: 1858609848.1096\n",
      "Epoch 45/3350\n",
      "1168/1168 [==============================] - 0s 55us/step - loss: 2442797022.6849 - val_loss: 1812907864.5479\n",
      "Epoch 46/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 2436436064.4384 - val_loss: 1778204245.9178\n",
      "Epoch 47/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 2384787475.2877 - val_loss: 1747388293.2603\n",
      "Epoch 48/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 2341360086.3562 - val_loss: 1743752190.2466\n",
      "Epoch 49/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 2311573253.2603 - val_loss: 1687883036.0548\n",
      "Epoch 50/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 2288000619.8356 - val_loss: 1695242978.1918\n",
      "Epoch 51/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 2287723937.3151 - val_loss: 1684702911.1233\n",
      "Epoch 52/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 2262451980.2740 - val_loss: 1613683252.6027\n",
      "Epoch 53/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 2241841951.5616 - val_loss: 1615709455.7808\n",
      "Epoch 54/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 2212638402.6301 - val_loss: 1583882977.3151\n",
      "Epoch 55/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 2209477598.6849 - val_loss: 1605630250.0822\n",
      "Epoch 56/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 2196892959.5616 - val_loss: 1551732461.5890\n",
      "Epoch 57/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 2173866186.5205 - val_loss: 1549855339.8356\n",
      "Epoch 58/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 2172418467.0685 - val_loss: 1518982682.3014\n",
      "Epoch 59/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 2170655263.5616 - val_loss: 1513860900.8219\n",
      "Epoch 60/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 2150115419.1781 - val_loss: 1523167720.3288\n",
      "Epoch 61/3350\n",
      "1168/1168 [==============================] - 0s 86us/step - loss: 2144915005.3699 - val_loss: 1563423463.4521\n",
      "Epoch 62/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 2140955097.4247 - val_loss: 1472379853.1507\n",
      "Epoch 63/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 2137863563.3973 - val_loss: 1546786422.3562\n",
      "Epoch 64/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 2130775388.9315 - val_loss: 1455177827.9452\n",
      "Epoch 65/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 2138364733.3699 - val_loss: 1497297832.3288\n",
      "Epoch 66/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 2134580081.9726 - val_loss: 1534838288.6575\n",
      "Epoch 67/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 2142248546.1918 - val_loss: 1480697852.4932\n",
      "Epoch 68/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 2120063139.0685 - val_loss: 1482625441.3151\n",
      "Epoch 69/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 2125752088.5479 - val_loss: 1542029524.1644\n",
      "Epoch 70/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 2139855680.8767 - val_loss: 1425538558.2466\n",
      "Epoch 71/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 2112945604.3836 - val_loss: 1462510131.7260\n",
      "Epoch 72/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 2107709442.6301 - val_loss: 1512792129.7534\n",
      "Epoch 73/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 2128116842.0822 - val_loss: 1472394848.4384\n",
      "Epoch 74/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 2127923997.8082 - val_loss: 1464144599.6712\n",
      "Epoch 75/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 2129670165.9178 - val_loss: 1489401385.2055\n",
      "Epoch 76/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 2100013862.5753 - val_loss: 1411921386.9589\n",
      "Epoch 77/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 2098042420.6027 - val_loss: 1402418560.0000\n",
      "Epoch 78/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 2098876189.8082 - val_loss: 1403925603.9452\n",
      "Epoch 79/3350\n",
      "1168/1168 [==============================] - 0s 79us/step - loss: 2096595782.1370 - val_loss: 1436428579.0685\n",
      "Epoch 80/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 2105258108.4932 - val_loss: 1451795054.4658\n",
      "Epoch 81/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 2091131050.9589 - val_loss: 1400950031.7808\n",
      "Epoch 82/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 2094298757.2603 - val_loss: 1430089422.0274\n",
      "Epoch 83/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 2124236635.1781 - val_loss: 1489208293.6986\n",
      "Epoch 84/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 2087888753.9726 - val_loss: 1389502684.9315\n",
      "Epoch 85/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 2117868500.1644 - val_loss: 1457155431.4521\n",
      "Epoch 86/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 2089216071.8904 - val_loss: 1398077836.2740\n",
      "Epoch 87/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 2090452980.6027 - val_loss: 1372753055.5616\n",
      "Epoch 88/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 2101701027.0685 - val_loss: 1380287091.7260\n",
      "Epoch 89/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 2090168847.7808 - val_loss: 1401698790.5753\n",
      "Epoch 90/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 2103378602.9589 - val_loss: 1367458122.5205\n",
      "Epoch 91/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 2091871245.1507 - val_loss: 1379998819.9452\n",
      "Epoch 92/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 2078470129.9726 - val_loss: 1378860426.5205\n",
      "Epoch 93/3350\n",
      "1168/1168 [==============================] - 0s 48us/step - loss: 2082085270.7945 - val_loss: 1413005171.7260\n",
      "Epoch 94/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 2077448686.4658 - val_loss: 1363290534.5753\n",
      "Epoch 95/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 2098029950.2466 - val_loss: 1399025000.3288\n",
      "Epoch 96/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 2071262660.3836 - val_loss: 1364871169.7534\n",
      "Epoch 97/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 2081157722.3014 - val_loss: 1377123456.0000\n",
      "Epoch 98/3350\n",
      "1168/1168 [==============================] - 0s 56us/step - loss: 2077650938.7397 - val_loss: 1360685720.5479\n",
      "Epoch 99/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 2073040997.6986 - val_loss: 1380841952.4384\n",
      "Epoch 100/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 2077575883.3973 - val_loss: 1371278338.6301\n",
      "Epoch 101/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 2088395320.1096 - val_loss: 1416633971.7260\n",
      "Epoch 102/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 2064760268.2740 - val_loss: 1378572731.6164\n",
      "Epoch 103/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 2073890805.4795 - val_loss: 1442486017.7534\n",
      "Epoch 104/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 2062628444.9315 - val_loss: 1370957187.5068\n",
      "Epoch 105/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 2065232020.1644 - val_loss: 1349186582.7945\n",
      "Epoch 106/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 2089140169.6438 - val_loss: 1349951611.6164\n",
      "Epoch 107/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 2060584149.9178 - val_loss: 1493870244.8219\n",
      "Epoch 108/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 2070304561.9726 - val_loss: 1350810362.7397\n",
      "Epoch 109/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 2101884235.3973 - val_loss: 1383709846.7945\n",
      "Epoch 110/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 2054145941.9178 - val_loss: 1350594041.8630\n",
      "Epoch 111/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 2055130656.4384 - val_loss: 1349329876.1644\n",
      "Epoch 112/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 2058190008.1096 - val_loss: 1364170026.0822\n",
      "Epoch 113/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 2046799768.5479 - val_loss: 1445650108.4932\n",
      "Epoch 114/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 2075496479.5616 - val_loss: 1351194674.8493\n",
      "Epoch 115/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 2056139046.5753 - val_loss: 1375220636.0548\n",
      "Epoch 116/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 2041155277.1507 - val_loss: 1400670800.6575\n",
      "Epoch 117/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 2065871991.2329 - val_loss: 1347605267.2877\n",
      "Epoch 118/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 2055006074.7397 - val_loss: 1361325187.5068\n",
      "Epoch 119/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 2042080718.0274 - val_loss: 1370379654.1370\n",
      "Epoch 120/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 2046294604.2740 - val_loss: 1351915970.6301\n",
      "Epoch 121/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 2051702250.9589 - val_loss: 1370667756.7123\n",
      "Epoch 122/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 2042732123.1781 - val_loss: 1352650604.7123\n",
      "Epoch 123/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 2035813341.8082 - val_loss: 1395570140.0548\n",
      "Epoch 124/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 2051697322.9589 - val_loss: 1332520185.8630\n",
      "Epoch 125/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 2032380354.6301 - val_loss: 1403288518.1370\n",
      "Epoch 126/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 2055883754.9589 - val_loss: 1354092878.9041\n",
      "Epoch 127/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 2031548930.6301 - val_loss: 1357516828.0548\n",
      "Epoch 128/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 2036977797.2603 - val_loss: 1395667928.5479\n",
      "Epoch 129/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 2052292060.9315 - val_loss: 1327102448.2192\n",
      "Epoch 130/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 2075988053.9178 - val_loss: 1326320604.9315\n",
      "Epoch 131/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 2029400214.7945 - val_loss: 1415485247.1233\n",
      "Epoch 132/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 2032552213.0411 - val_loss: 1344625909.4795\n",
      "Epoch 133/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 2021872401.5342 - val_loss: 1356919911.4521\n",
      "Epoch 134/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 2018806980.3836 - val_loss: 1383274013.8082\n",
      "Epoch 135/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 2020091635.7260 - val_loss: 1319890759.0137\n",
      "Epoch 136/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 2026786843.1781 - val_loss: 1362264106.9589\n",
      "Epoch 137/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 2026642440.7671 - val_loss: 1325177440.4384\n",
      "Epoch 138/3350\n",
      "1168/1168 [==============================] - 0s 50us/step - loss: 2027253491.7260 - val_loss: 1397824531.2877\n",
      "Epoch 139/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 2014356311.6712 - val_loss: 1329284387.9452\n",
      "Epoch 140/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 2013729262.4658 - val_loss: 1405038599.0137\n",
      "Epoch 141/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 2034562837.0411 - val_loss: 1366859306.0822\n",
      "Epoch 142/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 2021049244.0548 - val_loss: 1361430908.4932\n",
      "Epoch 143/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 2014767459.9452 - val_loss: 1336520558.4658\n",
      "Epoch 144/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 2015572080.2192 - val_loss: 1392177990.1370\n",
      "Epoch 145/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 2020100604.4932 - val_loss: 1335316113.5342\n",
      "Epoch 146/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 2017153287.8904 - val_loss: 1324889152.8767\n",
      "Epoch 147/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 2017937341.3699 - val_loss: 1383273142.3562\n",
      "Epoch 148/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 2015050595.9452 - val_loss: 1352228589.5890\n",
      "Epoch 149/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 2007143516.9315 - val_loss: 1382118657.7534\n",
      "Epoch 150/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 2012033414.1370 - val_loss: 1308844369.5342\n",
      "Epoch 151/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1995403982.9041 - val_loss: 1382631031.2329\n",
      "Epoch 152/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1993511774.6849 - val_loss: 1304566534.1370\n",
      "Epoch 153/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 2002825503.5616 - val_loss: 1359565825.7534\n",
      "Epoch 154/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 2004627043.0685 - val_loss: 1347333057.7534\n",
      "Epoch 155/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 2006089966.4658 - val_loss: 1323119016.3288\n",
      "Epoch 156/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 2005210107.6164 - val_loss: 1299757904.6575\n",
      "Epoch 157/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 2004037876.6027 - val_loss: 1302123968.8767\n",
      "Epoch 158/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1998615466.0822 - val_loss: 1431876326.5753\n",
      "Epoch 159/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 2028971064.1096 - val_loss: 1329667506.8493\n",
      "Epoch 160/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1998973389.1507 - val_loss: 1305033605.2603\n",
      "Epoch 161/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1999134131.7260 - val_loss: 1373453819.6164\n",
      "Epoch 162/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 2011762829.1507 - val_loss: 1397490599.4521\n",
      "Epoch 163/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 2004523997.3699 - val_loss: 1353471512.5479\n",
      "Epoch 164/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1993015132.0548 - val_loss: 1294570483.7260\n",
      "Epoch 165/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1992738686.2466 - val_loss: 1310118424.5479\n",
      "Epoch 166/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 1989388477.3699 - val_loss: 1321019175.4521\n",
      "Epoch 167/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1988951246.9041 - val_loss: 1294983087.3425\n",
      "Epoch 168/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1999685335.6712 - val_loss: 1308292224.0000\n",
      "Epoch 169/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1985401387.8356 - val_loss: 1315469620.6027\n",
      "Epoch 170/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 2009630023.8904 - val_loss: 1347314548.6027\n",
      "Epoch 171/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1974026038.3562 - val_loss: 1310422640.2192\n",
      "Epoch 172/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1994868750.0274 - val_loss: 1311944858.3014\n",
      "Epoch 173/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1988316904.7671 - val_loss: 1342056730.3014\n",
      "Epoch 174/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1982716809.6438 - val_loss: 1351189642.5205\n",
      "Epoch 175/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1977728077.1507 - val_loss: 1283423250.4110\n",
      "Epoch 176/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1986053233.0959 - val_loss: 1318522413.5890\n",
      "Epoch 177/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1994825859.5068 - val_loss: 1287721824.4384\n",
      "Epoch 178/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 1975979079.8904 - val_loss: 1345009502.6849\n",
      "Epoch 179/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1978253952.0000 - val_loss: 1311518651.6164\n",
      "Epoch 180/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1974966401.7534 - val_loss: 1331208367.3425\n",
      "Epoch 181/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1990444296.7671 - val_loss: 1398540699.1781\n",
      "Epoch 182/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1974578333.8082 - val_loss: 1289298628.3836\n",
      "Epoch 183/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1983983571.2877 - val_loss: 1282069398.7945\n",
      "Epoch 184/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1956036394.0822 - val_loss: 1320897178.3014\n",
      "Epoch 185/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1959662406.1370 - val_loss: 1276193556.1644\n",
      "Epoch 186/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1989083244.7123 - val_loss: 1331565738.0822\n",
      "Epoch 187/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1952543644.0548 - val_loss: 1286599691.3973\n",
      "Epoch 188/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1958367433.6438 - val_loss: 1320557489.0959\n",
      "Epoch 189/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1974019780.3836 - val_loss: 1274561399.2329\n",
      "Epoch 190/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 1966212581.6986 - val_loss: 1292766355.2877\n",
      "Epoch 191/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1958103918.4658 - val_loss: 1276455189.0411\n",
      "Epoch 192/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1962786190.0274 - val_loss: 1274252719.3425\n",
      "Epoch 193/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1963835987.2877 - val_loss: 1269284876.2740\n",
      "Epoch 194/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1945797819.6164 - val_loss: 1362112195.5068\n",
      "Epoch 195/3350\n",
      "1168/1168 [==============================] - 0s 81us/step - loss: 1964183601.0959 - val_loss: 1274029461.0411\n",
      "Epoch 196/3350\n",
      "1168/1168 [==============================] - 0s 43us/step - loss: 1956740458.9589 - val_loss: 1319715392.8767\n",
      "Epoch 197/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1942046129.0959 - val_loss: 1269372407.2329\n",
      "Epoch 198/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1960091096.5479 - val_loss: 1290000355.9452\n",
      "Epoch 199/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1943090267.1781 - val_loss: 1357593893.6986\n",
      "Epoch 200/3350\n",
      "1168/1168 [==============================] - 0s 48us/step - loss: 1944365800.3288 - val_loss: 1267855727.3425\n",
      "Epoch 201/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1953711943.8904 - val_loss: 1291200433.0959\n",
      "Epoch 202/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 1943463948.2740 - val_loss: 1260562915.9452\n",
      "Epoch 203/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1965610313.6438 - val_loss: 1345504727.6712\n",
      "Epoch 204/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1915574671.7808 - val_loss: 1260606158.9041\n",
      "Epoch 205/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1975148771.9452 - val_loss: 1272257458.8493\n",
      "Epoch 206/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1927436188.9315 - val_loss: 1322771939.9452\n",
      "Epoch 207/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1957172420.3836 - val_loss: 1300045289.2055\n",
      "Epoch 208/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1951446385.9726 - val_loss: 1282609374.6849\n",
      "Epoch 209/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 1923067924.1644 - val_loss: 1303355473.5342\n",
      "Epoch 210/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1936090091.8356 - val_loss: 1280566328.1096\n",
      "Epoch 211/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1974871653.6986 - val_loss: 1258470550.7945\n",
      "Epoch 212/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 1977893534.6849 - val_loss: 1257101613.5890\n",
      "Epoch 213/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 1949938650.7397 - val_loss: 1268650714.3014\n",
      "Epoch 214/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1950024955.6164 - val_loss: 1304559602.8493\n",
      "Epoch 215/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 1913227044.3836 - val_loss: 1290714454.7945\n",
      "Epoch 216/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1936815940.3836 - val_loss: 1367771754.9589\n",
      "Epoch 217/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1915200724.1644 - val_loss: 1251878508.7123\n",
      "Epoch 218/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 1936897204.6027 - val_loss: 1309129163.3973\n",
      "Epoch 219/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1918415134.0274 - val_loss: 1260604667.6164\n",
      "Epoch 220/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1913482083.9452 - val_loss: 1270685526.7945\n",
      "Epoch 221/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1910278764.7123 - val_loss: 1294344519.8904\n",
      "Epoch 222/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1906143630.0274 - val_loss: 1247929593.8630\n",
      "Epoch 223/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1921792038.5753 - val_loss: 1263845508.3836\n",
      "Epoch 224/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1932181723.1781 - val_loss: 1405237967.7808\n",
      "Epoch 225/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1904486936.5479 - val_loss: 1248766848.0000\n",
      "Epoch 226/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1910932222.2466 - val_loss: 1249285408.4384\n",
      "Epoch 227/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1925581855.5616 - val_loss: 1264749594.3014\n",
      "Epoch 228/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1907060159.1233 - val_loss: 1244894739.2877\n",
      "Epoch 229/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1918262232.5479 - val_loss: 1244218149.6986\n",
      "Epoch 230/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 1906078191.3425 - val_loss: 1281300074.0822\n",
      "Epoch 231/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 1904037489.9726 - val_loss: 1246262613.0411\n",
      "Epoch 232/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1943299512.9863 - val_loss: 1243733103.3425\n",
      "Epoch 233/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1916161612.2740 - val_loss: 1240105621.0411\n",
      "Epoch 234/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1912710407.0137 - val_loss: 1242979009.7534\n",
      "Epoch 235/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1901351327.5616 - val_loss: 1241745046.7945\n",
      "Epoch 236/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1904921310.6849 - val_loss: 1255335544.9863\n",
      "Epoch 237/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1895606776.1096 - val_loss: 1243677508.3836\n",
      "Epoch 238/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1924846910.2466 - val_loss: 1339685761.7534\n",
      "Epoch 239/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 1901257819.1781 - val_loss: 1270830200.1096\n",
      "Epoch 240/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1891149623.2329 - val_loss: 1247407984.2192\n",
      "Epoch 241/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1886905657.8630 - val_loss: 1236035761.0959\n",
      "Epoch 242/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 1907069340.9315 - val_loss: 1316777217.7534\n",
      "Epoch 243/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1904392355.0685 - val_loss: 1277520638.2466\n",
      "Epoch 244/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1889520595.2877 - val_loss: 1240335756.2740\n",
      "Epoch 245/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1874582802.4110 - val_loss: 1256057615.7808\n",
      "Epoch 246/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1876258864.2192 - val_loss: 1235470786.6301\n",
      "Epoch 247/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1893719849.2055 - val_loss: 1280349382.1370\n",
      "Epoch 248/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1892342412.2740 - val_loss: 1254245520.6575\n",
      "Epoch 249/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1890362855.4521 - val_loss: 1230017128.3288\n",
      "Epoch 250/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 1887018709.9178 - val_loss: 1248754440.7671\n",
      "Epoch 251/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1868963366.5753 - val_loss: 1231393801.6438\n",
      "Epoch 252/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1928872148.1644 - val_loss: 1227717670.5753\n",
      "Epoch 253/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1888657871.7808 - val_loss: 1230313132.7123\n",
      "Epoch 254/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1897687160.9863 - val_loss: 1243135246.0274\n",
      "Epoch 255/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1899356048.6575 - val_loss: 1248100195.0685\n",
      "Epoch 256/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1875216462.9041 - val_loss: 1286153070.4658\n",
      "Epoch 257/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1868331932.9315 - val_loss: 1247207098.7397\n",
      "Epoch 258/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1884972754.4110 - val_loss: 1224261361.0959\n",
      "Epoch 259/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 1866641625.4247 - val_loss: 1235967367.0137\n",
      "Epoch 260/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1861345976.1096 - val_loss: 1222726937.4247\n",
      "Epoch 261/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1865016962.6301 - val_loss: 1296548480.0000\n",
      "Epoch 262/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1906921010.8493 - val_loss: 1316532480.0000\n",
      "Epoch 263/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1866925429.4795 - val_loss: 1241519355.6164\n",
      "Epoch 264/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1865117983.5616 - val_loss: 1220207552.8767\n",
      "Epoch 265/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1855735867.6164 - val_loss: 1242411499.8356\n",
      "Epoch 266/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1860075159.6712 - val_loss: 1222788144.2192\n",
      "Epoch 267/3350\n",
      "1168/1168 [==============================] - 0s 51us/step - loss: 1851721240.5479 - val_loss: 1221770223.3425\n",
      "Epoch 268/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1852392323.5068 - val_loss: 1223043459.5068\n",
      "Epoch 269/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1853625400.1096 - val_loss: 1282579021.1507\n",
      "Epoch 270/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1845484702.6849 - val_loss: 1212065058.1918\n",
      "Epoch 271/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1858391189.9178 - val_loss: 1234265413.2603\n",
      "Epoch 272/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 1843081713.0959 - val_loss: 1215613142.7945\n",
      "Epoch 273/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1836390524.4932 - val_loss: 1216941077.9178\n",
      "Epoch 274/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1838354797.5890 - val_loss: 1230737327.3425\n",
      "Epoch 275/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 1841686902.3562 - val_loss: 1233762628.3836\n",
      "Epoch 276/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 1842700433.5342 - val_loss: 1208025054.6849\n",
      "Epoch 277/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1847964788.6027 - val_loss: 1243226199.6712\n",
      "Epoch 278/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1836408432.2192 - val_loss: 1240155253.4795\n",
      "Epoch 279/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1839989326.4658 - val_loss: 1216658070.7945\n",
      "Epoch 280/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1833433286.1370 - val_loss: 1211774204.4932\n",
      "Epoch 281/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1861562403.0685 - val_loss: 1205406895.3425\n",
      "Epoch 282/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1863555769.8630 - val_loss: 1220901537.3151\n",
      "Epoch 283/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1835522210.1918 - val_loss: 1208295466.0822\n",
      "Epoch 284/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1880415126.7945 - val_loss: 1233642413.5890\n",
      "Epoch 285/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1822923317.4795 - val_loss: 1211900377.4247\n",
      "Epoch 286/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1834938968.5479 - val_loss: 1212654971.6164\n",
      "Epoch 287/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1831242741.4795 - val_loss: 1217310827.8356\n",
      "Epoch 288/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 1843068467.7260 - val_loss: 1213903995.6164\n",
      "Epoch 289/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1818736245.4795 - val_loss: 1217763616.4384\n",
      "Epoch 290/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1815161963.8356 - val_loss: 1209796559.7808\n",
      "Epoch 291/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1829315326.2466 - val_loss: 1203100601.8630\n",
      "Epoch 292/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1810190997.0411 - val_loss: 1236124471.2329\n",
      "Epoch 293/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1839991310.0274 - val_loss: 1205334140.4932\n",
      "Epoch 294/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1826817314.1918 - val_loss: 1209718774.3562\n",
      "Epoch 295/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1817269199.7808 - val_loss: 1267583760.6575\n",
      "Epoch 296/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1817800662.7945 - val_loss: 1205410146.1918\n",
      "Epoch 297/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1840727795.7260 - val_loss: 1201725103.3425\n",
      "Epoch 298/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1832395611.1781 - val_loss: 1317133885.3699\n",
      "Epoch 299/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1826125758.2466 - val_loss: 1242834652.0548\n",
      "Epoch 300/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1806155169.3151 - val_loss: 1224069411.0685\n",
      "Epoch 301/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1805015232.8767 - val_loss: 1208308215.6712\n",
      "Epoch 302/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1808482139.1781 - val_loss: 1198893052.4932\n",
      "Epoch 303/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1832075427.9452 - val_loss: 1194487927.2329\n",
      "Epoch 304/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1835309512.7671 - val_loss: 1251148735.1233\n",
      "Epoch 305/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1801342277.2603 - val_loss: 1236630266.7397\n",
      "Epoch 306/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1798215569.5342 - val_loss: 1205221596.4932\n",
      "Epoch 307/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1821854589.8082 - val_loss: 1198128384.0000\n",
      "Epoch 308/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1813266316.2740 - val_loss: 1190655805.3699\n",
      "Epoch 309/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 1823184413.8082 - val_loss: 1192985612.2740\n",
      "Epoch 310/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1809385158.1370 - val_loss: 1206100772.8219\n",
      "Epoch 311/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1817138350.4658 - val_loss: 1200633804.2740\n",
      "Epoch 312/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1793688693.4795 - val_loss: 1205467093.0411\n",
      "Epoch 313/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1792459666.4110 - val_loss: 1269250170.7397\n",
      "Epoch 314/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1819105023.1233 - val_loss: 1242869699.5068\n",
      "Epoch 315/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1798653804.7123 - val_loss: 1199005572.3836\n",
      "Epoch 316/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1805436233.6438 - val_loss: 1214632988.4932\n",
      "Epoch 317/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1792017052.4932 - val_loss: 1196947804.9315\n",
      "Epoch 318/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1785564037.2603 - val_loss: 1215098359.6712\n",
      "Epoch 319/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1786945608.7671 - val_loss: 1211104222.6849\n",
      "Epoch 320/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1807375243.3973 - val_loss: 1192579149.1507\n",
      "Epoch 321/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1799795748.8219 - val_loss: 1191748103.4521\n",
      "Epoch 322/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 1781466146.1918 - val_loss: 1192550430.6849\n",
      "Epoch 323/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1780101637.2603 - val_loss: 1199264672.4384\n",
      "Epoch 324/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1779510354.4110 - val_loss: 1196023402.0822\n",
      "Epoch 325/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1794278684.0548 - val_loss: 1186625904.2192\n",
      "Epoch 326/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1776482695.0137 - val_loss: 1204181448.7671\n",
      "Epoch 327/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1792575161.8630 - val_loss: 1229970615.2329\n",
      "Epoch 328/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1772502649.8630 - val_loss: 1215753452.7123\n",
      "Epoch 329/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1784933432.1096 - val_loss: 1181913357.1507\n",
      "Epoch 330/3350\n",
      "1168/1168 [==============================] - 0s 79us/step - loss: 1775825043.2877 - val_loss: 1184866294.3562\n",
      "Epoch 331/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1782693657.4247 - val_loss: 1235195366.5753\n",
      "Epoch 332/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1778469019.1781 - val_loss: 1249883762.8493\n",
      "Epoch 333/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1768676050.4110 - val_loss: 1180667083.3973\n",
      "Epoch 334/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1754182042.3014 - val_loss: 1215424412.0548\n",
      "Epoch 335/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1798360005.6986 - val_loss: 1204611126.3562\n",
      "Epoch 336/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1766082954.5205 - val_loss: 1219047046.5753\n",
      "Epoch 337/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1763344999.4521 - val_loss: 1193996625.9726\n",
      "Epoch 338/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1775303219.7260 - val_loss: 1211743772.9315\n",
      "Epoch 339/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1760045852.9315 - val_loss: 1228701131.3973\n",
      "Epoch 340/3350\n",
      "1168/1168 [==============================] - 0s 50us/step - loss: 1769570180.3836 - val_loss: 1186865671.0137\n",
      "Epoch 341/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1743702413.1507 - val_loss: 1245294478.0274\n",
      "Epoch 342/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 1754651167.5616 - val_loss: 1182870324.6027\n",
      "Epoch 343/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1748813137.5342 - val_loss: 1220833735.8904\n",
      "Epoch 344/3350\n",
      "1168/1168 [==============================] - 0s 54us/step - loss: 1756728284.9315 - val_loss: 1177152479.5616\n",
      "Epoch 345/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1769006520.9863 - val_loss: 1180708935.8904\n",
      "Epoch 346/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1768270402.6301 - val_loss: 1179115680.4384\n",
      "Epoch 347/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1748565022.6849 - val_loss: 1201759813.2603\n",
      "Epoch 348/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1741919425.7534 - val_loss: 1180222912.8767\n",
      "Epoch 349/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1745286995.2877 - val_loss: 1247169848.1096\n",
      "Epoch 350/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1762005142.7945 - val_loss: 1202714797.5890\n",
      "Epoch 351/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1738741035.8356 - val_loss: 1171135428.3836\n",
      "Epoch 352/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1764931096.5479 - val_loss: 1225832802.1918\n",
      "Epoch 353/3350\n",
      "1168/1168 [==============================] - 0s 46us/step - loss: 1746192869.6986 - val_loss: 1231896123.1781\n",
      "Epoch 354/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1741934594.1918 - val_loss: 1167370536.3288\n",
      "Epoch 355/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 1750544146.4110 - val_loss: 1178881715.7260\n",
      "Epoch 356/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1746277740.7123 - val_loss: 1177535672.9863\n",
      "Epoch 357/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1732037087.5616 - val_loss: 1171831325.8082\n",
      "Epoch 358/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1749515398.1370 - val_loss: 1164942337.7534\n",
      "Epoch 359/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1735316927.1233 - val_loss: 1233104422.5753\n",
      "Epoch 360/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1750854293.0411 - val_loss: 1192177087.1233\n",
      "Epoch 361/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1738359127.6712 - val_loss: 1227696885.0411\n",
      "Epoch 362/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1736498465.3151 - val_loss: 1194378093.5890\n",
      "Epoch 363/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1734813372.9315 - val_loss: 1184898337.3151\n",
      "Epoch 364/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1777370269.8082 - val_loss: 1165380792.1096\n",
      "Epoch 365/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1726508786.8493 - val_loss: 1173716601.8630\n",
      "Epoch 366/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1721970463.5616 - val_loss: 1236542890.0822\n",
      "Epoch 367/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1744217792.8767 - val_loss: 1239489794.6301\n",
      "Epoch 368/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1734477337.4247 - val_loss: 1184779979.3973\n",
      "Epoch 369/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 1724156763.1781 - val_loss: 1194990533.6986\n",
      "Epoch 370/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1719204003.0685 - val_loss: 1170051662.0274\n",
      "Epoch 371/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1714139783.0137 - val_loss: 1196139756.2740\n",
      "Epoch 372/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1715212039.0137 - val_loss: 1183856102.1370\n",
      "Epoch 373/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1754533377.7534 - val_loss: 1155800277.9178\n",
      "Epoch 374/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1734019060.6027 - val_loss: 1158366945.3151\n",
      "Epoch 375/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1749377566.6849 - val_loss: 1292765193.6438\n",
      "Epoch 376/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1738543324.0548 - val_loss: 1183578322.4110\n",
      "Epoch 377/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1741529694.6849 - val_loss: 1158761316.8219\n",
      "Epoch 378/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1738447755.3973 - val_loss: 1158583448.5479\n",
      "Epoch 379/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1714426668.7123 - val_loss: 1252289539.0685\n",
      "Epoch 380/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 1728108480.8767 - val_loss: 1181474212.3836\n",
      "Epoch 381/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1699352164.8219 - val_loss: 1164621491.2877\n",
      "Epoch 382/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1713913401.8630 - val_loss: 1163776954.7397\n",
      "Epoch 383/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1700578819.5068 - val_loss: 1161583236.3836\n",
      "Epoch 384/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1707482266.3014 - val_loss: 1170719994.3014\n",
      "Epoch 385/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1696849858.6301 - val_loss: 1164372022.3562\n",
      "Epoch 386/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 1697763605.0411 - val_loss: 1157256999.4521\n",
      "Epoch 387/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1689938396.9315 - val_loss: 1200042313.6438\n",
      "Epoch 388/3350\n",
      "1168/1168 [==============================] - 0s 50us/step - loss: 1703749291.8356 - val_loss: 1161817805.5890\n",
      "Epoch 389/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1692909878.3562 - val_loss: 1191437772.7123\n",
      "Epoch 390/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1692428485.2603 - val_loss: 1165499306.9589\n",
      "Epoch 391/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 1707655515.1781 - val_loss: 1166778355.7260\n",
      "Epoch 392/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1694617147.6164 - val_loss: 1156123437.1507\n",
      "Epoch 393/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1693212568.5479 - val_loss: 1159921515.8356\n",
      "Epoch 394/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1702162724.3836 - val_loss: 1151932172.2740\n",
      "Epoch 395/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1690885034.9589 - val_loss: 1146016615.4521\n",
      "Epoch 396/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 1709029084.0548 - val_loss: 1151701907.7260\n",
      "Epoch 397/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1707236349.3699 - val_loss: 1153555202.6301\n",
      "Epoch 398/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1686676503.6712 - val_loss: 1170113925.6986\n",
      "Epoch 399/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1681465188.8219 - val_loss: 1156783929.8630\n",
      "Epoch 400/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1680003080.7671 - val_loss: 1178651649.7534\n",
      "Epoch 401/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1783789541.6986 - val_loss: 1159300259.0685\n",
      "Epoch 402/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1699106395.6164 - val_loss: 1150493424.2192\n",
      "Epoch 403/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1680193365.9178 - val_loss: 1158694228.1644\n",
      "Epoch 404/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1687241097.2055 - val_loss: 1151245250.1918\n",
      "Epoch 405/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1689849375.5616 - val_loss: 1165584326.5753\n",
      "Epoch 406/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 1686997895.0137 - val_loss: 1180464490.0822\n",
      "Epoch 407/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1690858384.6575 - val_loss: 1141177042.4110\n",
      "Epoch 408/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1673092190.2466 - val_loss: 1161342864.6575\n",
      "Epoch 409/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1683578069.9178 - val_loss: 1151885601.3151\n",
      "Epoch 410/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1673252836.8219 - val_loss: 1155443959.6712\n",
      "Epoch 411/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1667503569.5342 - val_loss: 1157645315.5068\n",
      "Epoch 412/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1675543009.3151 - val_loss: 1202708517.6986\n",
      "Epoch 413/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1666883490.1918 - val_loss: 1145692761.8630\n",
      "Epoch 414/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1691439537.9726 - val_loss: 1152737095.4521\n",
      "Epoch 415/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1671685010.4110 - val_loss: 1150852223.1233\n",
      "Epoch 416/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 1672739733.0411 - val_loss: 1145227035.1781\n",
      "Epoch 417/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1663782526.2466 - val_loss: 1166664899.5068\n",
      "Epoch 418/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1677949536.4384 - val_loss: 1155482542.0274\n",
      "Epoch 419/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1671378040.1096 - val_loss: 1176621589.0411\n",
      "Epoch 420/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1662212293.2603 - val_loss: 1152158340.3836\n",
      "Epoch 421/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1660483474.4110 - val_loss: 1170422372.8219\n",
      "Epoch 422/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1645773446.1370 - val_loss: 1150656156.0548\n",
      "Epoch 423/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1657639617.7534 - val_loss: 1154949623.2329\n",
      "Epoch 424/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1656821048.5479 - val_loss: 1148954824.7671\n",
      "Epoch 425/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1694981840.6575 - val_loss: 1142885708.7123\n",
      "Epoch 426/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1672534534.1370 - val_loss: 1141862832.2192\n",
      "Epoch 427/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1651768082.4110 - val_loss: 1149946684.9315\n",
      "Epoch 428/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1690722442.9589 - val_loss: 1150422103.6712\n",
      "Epoch 429/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 1652233941.9178 - val_loss: 1163619430.1370\n",
      "Epoch 430/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1644604899.9452 - val_loss: 1155722049.7534\n",
      "Epoch 431/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1647987384.9863 - val_loss: 1144275946.5205\n",
      "Epoch 432/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1665205179.6164 - val_loss: 1170257647.7808\n",
      "Epoch 433/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1638191738.7397 - val_loss: 1150233909.4795\n",
      "Epoch 434/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1645981433.8630 - val_loss: 1185949918.6849\n",
      "Epoch 435/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1665360349.8082 - val_loss: 1243342821.6986\n",
      "Epoch 436/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1687858089.2055 - val_loss: 1152071022.4658\n",
      "Epoch 437/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1638280695.2329 - val_loss: 1162322920.3288\n",
      "Epoch 438/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1645447825.5342 - val_loss: 1168451619.0685\n",
      "Epoch 439/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1647030636.2740 - val_loss: 1174417884.0548\n",
      "Epoch 440/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1657217081.8630 - val_loss: 1150011719.0137\n",
      "Epoch 441/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1665843042.1918 - val_loss: 1207320156.9315\n",
      "Epoch 442/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1649443377.9726 - val_loss: 1195099294.6849\n",
      "Epoch 443/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1633341279.1233 - val_loss: 1161523706.3014\n",
      "Epoch 444/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1659347875.9452 - val_loss: 1147318684.9315\n",
      "Epoch 445/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1630178405.6986 - val_loss: 1170369232.6575\n",
      "Epoch 446/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1661246062.4658 - val_loss: 1151763702.3562\n",
      "Epoch 447/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1628051310.0274 - val_loss: 1187561765.6986\n",
      "Epoch 448/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1646720462.0274 - val_loss: 1160752156.0548\n",
      "Epoch 449/3350\n",
      "1168/1168 [==============================] - 0s 50us/step - loss: 1638078799.7808 - val_loss: 1162693462.7945\n",
      "Epoch 450/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1628391846.5753 - val_loss: 1141558430.6849\n",
      "Epoch 451/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1637868917.4795 - val_loss: 1184264330.5205\n",
      "Epoch 452/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 1627818439.8904 - val_loss: 1210875008.0000\n",
      "Epoch 453/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1658273294.9041 - val_loss: 1151953585.0959\n",
      "Epoch 454/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1636244974.4658 - val_loss: 1142268691.7260\n",
      "Epoch 455/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1622253241.4247 - val_loss: 1172126326.3562\n",
      "Epoch 456/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 1637597484.2740 - val_loss: 1166631559.0137\n",
      "Epoch 457/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1644960538.3014 - val_loss: 1216744353.3151\n",
      "Epoch 458/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1615594878.2466 - val_loss: 1147988814.0274\n",
      "Epoch 459/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1627169901.5890 - val_loss: 1152254809.8630\n",
      "Epoch 460/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1617221184.8767 - val_loss: 1219482493.8082\n",
      "Epoch 461/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1618243312.2192 - val_loss: 1146129434.3014\n",
      "Epoch 462/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1631789445.2603 - val_loss: 1144549964.2740\n",
      "Epoch 463/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 1610997737.2055 - val_loss: 1211798626.1918\n",
      "Epoch 464/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1621535859.7260 - val_loss: 1153990178.1918\n",
      "Epoch 465/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1611393665.7534 - val_loss: 1161767176.7671\n",
      "Epoch 466/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 1610795508.6027 - val_loss: 1142428232.7671\n",
      "Epoch 467/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 1635006740.1644 - val_loss: 1152313178.3014\n",
      "Epoch 468/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1609082581.9178 - val_loss: 1152898349.5890\n",
      "Epoch 469/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1620352633.8630 - val_loss: 1205016680.3288\n",
      "Epoch 470/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1615608675.9452 - val_loss: 1182848396.2740\n",
      "Epoch 471/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1625933758.2466 - val_loss: 1148715077.2603\n",
      "Epoch 472/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1601410609.9726 - val_loss: 1152246501.6986\n",
      "Epoch 473/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1651872888.1096 - val_loss: 1181628273.0959\n",
      "Epoch 474/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1624797945.8630 - val_loss: 1183418355.7260\n",
      "Epoch 475/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 1622697175.6712 - val_loss: 1202921578.9589\n",
      "Epoch 476/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1608594599.4521 - val_loss: 1148801633.3151\n",
      "Epoch 477/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1605799864.9863 - val_loss: 1144416379.6164\n",
      "Epoch 478/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1615253167.3425 - val_loss: 1162917823.1233\n",
      "Epoch 479/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1589711915.8356 - val_loss: 1144248777.6438\n",
      "Epoch 480/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1629588530.8493 - val_loss: 1154564958.6849\n",
      "Epoch 481/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 1597334269.3699 - val_loss: 1145621002.5205\n",
      "Epoch 482/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1636176564.6027 - val_loss: 1235526574.4658\n",
      "Epoch 483/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1616643224.5479 - val_loss: 1173584711.0137\n",
      "Epoch 484/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1605682008.5479 - val_loss: 1151196281.4247\n",
      "Epoch 485/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1614389174.3562 - val_loss: 1148172494.0274\n",
      "Epoch 486/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1639031273.2055 - val_loss: 1178365432.5479\n",
      "Epoch 487/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1627304153.4247 - val_loss: 1163885134.9041\n",
      "Epoch 488/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1596244210.8493 - val_loss: 1149135750.1370\n",
      "Epoch 489/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1596297827.0685 - val_loss: 1152699111.0137\n",
      "Epoch 490/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1591372504.5479 - val_loss: 1157240109.5890\n",
      "Epoch 491/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1612669528.5479 - val_loss: 1210280488.3288\n",
      "Epoch 492/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1613172554.5205 - val_loss: 1216818517.0411\n",
      "Epoch 493/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1638033731.5068 - val_loss: 1141775096.9863\n",
      "Epoch 494/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1606881800.7671 - val_loss: 1169746868.6027\n",
      "Epoch 495/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1594940057.4247 - val_loss: 1144521890.1918\n",
      "Epoch 496/3350\n",
      "1168/1168 [==============================] - 0s 90us/step - loss: 1586245013.0411 - val_loss: 1142951294.2466\n",
      "Epoch 497/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1591084989.3699 - val_loss: 1144318961.0959\n",
      "Epoch 498/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1573157521.0959 - val_loss: 1178970070.7945\n",
      "Epoch 499/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1589615272.3288 - val_loss: 1141509899.3973\n",
      "Epoch 500/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1612587648.0000 - val_loss: 1184516928.0000\n",
      "Epoch 501/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1578221405.8082 - val_loss: 1150267987.2877\n",
      "Epoch 502/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 1576279425.7534 - val_loss: 1139145176.5479\n",
      "Epoch 503/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1580540365.1507 - val_loss: 1168396723.2877\n",
      "Epoch 504/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1588125927.4521 - val_loss: 1218348271.7808\n",
      "Epoch 505/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1602038629.6986 - val_loss: 1151403113.2055\n",
      "Epoch 506/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1591434736.2192 - val_loss: 1159132221.8082\n",
      "Epoch 507/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1570319463.4521 - val_loss: 1142166139.6164\n",
      "Epoch 508/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1586893397.9178 - val_loss: 1143362440.7671\n",
      "Epoch 509/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1636302540.2740 - val_loss: 1182027664.6575\n",
      "Epoch 510/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1576946482.8493 - val_loss: 1167368419.0685\n",
      "Epoch 511/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 1565329554.4110 - val_loss: 1151907449.4247\n",
      "Epoch 512/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1571697455.3425 - val_loss: 1144309281.3151\n",
      "Epoch 513/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1573369052.9315 - val_loss: 1195240008.3288\n",
      "Epoch 514/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1572001329.0959 - val_loss: 1139011407.3425\n",
      "Epoch 515/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1565286291.2877 - val_loss: 1149447306.5205\n",
      "Epoch 516/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 1568266947.5068 - val_loss: 1146356274.8493\n",
      "Epoch 517/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 1575339391.1233 - val_loss: 1141545476.3836\n",
      "Epoch 518/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1561305580.7123 - val_loss: 1145778200.5479\n",
      "Epoch 519/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1576498715.1781 - val_loss: 1156159736.9863\n",
      "Epoch 520/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1573369901.5890 - val_loss: 1138298306.1918\n",
      "Epoch 521/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1561589579.3973 - val_loss: 1222663785.2055\n",
      "Epoch 522/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1572383065.4247 - val_loss: 1172054114.1918\n",
      "Epoch 523/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1571306527.5616 - val_loss: 1137749387.3973\n",
      "Epoch 524/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1557716882.4110 - val_loss: 1147438325.4795\n",
      "Epoch 525/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 1558394227.2877 - val_loss: 1166270330.7397\n",
      "Epoch 526/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1562693370.7397 - val_loss: 1143372545.7534\n",
      "Epoch 527/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1551529476.3836 - val_loss: 1144399495.4521\n",
      "Epoch 528/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1564583830.7945 - val_loss: 1147583352.5479\n",
      "Epoch 529/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1553795093.0411 - val_loss: 1147521411.9452\n",
      "Epoch 530/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1554283761.0959 - val_loss: 1133918147.5068\n",
      "Epoch 531/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1556273316.8219 - val_loss: 1150663772.0548\n",
      "Epoch 532/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1550528847.7808 - val_loss: 1138064330.9589\n",
      "Epoch 533/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 1540908085.4795 - val_loss: 1237198819.9452\n",
      "Epoch 534/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1561442920.3288 - val_loss: 1135078993.0959\n",
      "Epoch 535/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1556473660.4932 - val_loss: 1143505777.0959\n",
      "Epoch 536/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1553047946.0822 - val_loss: 1137144584.7671\n",
      "Epoch 537/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1573190347.8356 - val_loss: 1127489102.0274\n",
      "Epoch 538/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1547822718.2466 - val_loss: 1162335800.1096\n",
      "Epoch 539/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1560847234.6301 - val_loss: 1130890723.0685\n",
      "Epoch 540/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1551289353.6438 - val_loss: 1133038002.8493\n",
      "Epoch 541/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1555532458.9589 - val_loss: 1132462891.8356\n",
      "Epoch 542/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1536180914.8493 - val_loss: 1162633562.3014\n",
      "Epoch 543/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1548314232.9863 - val_loss: 1150867737.8630\n",
      "Epoch 544/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1558860377.4247 - val_loss: 1135639408.2192\n",
      "Epoch 545/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1548157297.9726 - val_loss: 1134798070.3562\n",
      "Epoch 546/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1538973760.0000 - val_loss: 1135293085.8082\n",
      "Epoch 547/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1546354218.0822 - val_loss: 1186733770.5205\n",
      "Epoch 548/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1548266240.8767 - val_loss: 1147921463.2329\n",
      "Epoch 549/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1540324272.2192 - val_loss: 1137940316.4932\n",
      "Epoch 550/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1536725817.4247 - val_loss: 1134120809.2055\n",
      "Epoch 551/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1550133736.3288 - val_loss: 1140473002.9589\n",
      "Epoch 552/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1531582536.7671 - val_loss: 1166725760.8767\n",
      "Epoch 553/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1540861800.7671 - val_loss: 1158045378.6301\n",
      "Epoch 554/3350\n",
      "1168/1168 [==============================] - 0s 46us/step - loss: 1531166144.0000 - val_loss: 1145614655.5616\n",
      "Epoch 555/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1533134717.3699 - val_loss: 1130079246.0274\n",
      "Epoch 556/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1573669658.3014 - val_loss: 1158153320.3288\n",
      "Epoch 557/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1541097393.9726 - val_loss: 1141463364.3836\n",
      "Epoch 558/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1535115619.9452 - val_loss: 1159980960.0000\n",
      "Epoch 559/3350\n",
      "1168/1168 [==============================] - 0s 52us/step - loss: 1532318380.7123 - val_loss: 1133092125.8082\n",
      "Epoch 560/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1545156810.9589 - val_loss: 1121468461.5890\n",
      "Epoch 561/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1537453802.9589 - val_loss: 1127132102.1370\n",
      "Epoch 562/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 1538531105.3151 - val_loss: 1136426104.1096\n",
      "Epoch 563/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 1537833429.0411 - val_loss: 1155155556.8219\n",
      "Epoch 564/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1553454590.2466 - val_loss: 1124328660.1644\n",
      "Epoch 565/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 1522147817.2055 - val_loss: 1158046739.2877\n",
      "Epoch 566/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 1542725630.2466 - val_loss: 1128601983.5616\n",
      "Epoch 567/3350\n",
      "1168/1168 [==============================] - 0s 47us/step - loss: 1515049180.9315 - val_loss: 1186420755.2877\n",
      "Epoch 568/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1529950786.6301 - val_loss: 1131102586.7397\n",
      "Epoch 569/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 1536471440.6575 - val_loss: 1135515336.7671\n",
      "Epoch 570/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1525167236.3836 - val_loss: 1124939253.4795\n",
      "Epoch 571/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1522060608.0000 - val_loss: 1151606450.8493\n",
      "Epoch 572/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1529759836.0548 - val_loss: 1128308625.0959\n",
      "Epoch 573/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 1524538156.7123 - val_loss: 1126599771.1781\n",
      "Epoch 574/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1517100210.8493 - val_loss: 1129428702.6849\n",
      "Epoch 575/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1537055382.7945 - val_loss: 1125776088.5479\n",
      "Epoch 576/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1517137936.6575 - val_loss: 1157456107.3973\n",
      "Epoch 577/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1531539839.1233 - val_loss: 1129266766.0274\n",
      "Epoch 578/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1554867682.1918 - val_loss: 1130689955.5068\n",
      "Epoch 579/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1545389422.4658 - val_loss: 1136542742.7945\n",
      "Epoch 580/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1522520977.0959 - val_loss: 1226036917.9178\n",
      "Epoch 581/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1501521201.0959 - val_loss: 1135200243.7260\n",
      "Epoch 582/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1517618631.8904 - val_loss: 1137873393.9726\n",
      "Epoch 583/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1510321719.2329 - val_loss: 1127915071.5616\n",
      "Epoch 584/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1513462105.4247 - val_loss: 1123407866.7397\n",
      "Epoch 585/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1516299534.0274 - val_loss: 1134853698.6301\n",
      "Epoch 586/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1515262914.6301 - val_loss: 1124556365.1507\n",
      "Epoch 587/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1563017319.4521 - val_loss: 1133658815.1233\n",
      "Epoch 588/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1544756051.2877 - val_loss: 1149565589.0411\n",
      "Epoch 589/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1528770247.8904 - val_loss: 1128434012.9315\n",
      "Epoch 590/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1502232199.0137 - val_loss: 1136230595.0685\n",
      "Epoch 591/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1511478874.3014 - val_loss: 1167498688.8767\n",
      "Epoch 592/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1520955435.8356 - val_loss: 1146788406.3562\n",
      "Epoch 593/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1502099748.8219 - val_loss: 1124230927.3425\n",
      "Epoch 594/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 1502723462.1370 - val_loss: 1127398407.4521\n",
      "Epoch 595/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1498839157.4795 - val_loss: 1173837311.1233\n",
      "Epoch 596/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1519424762.7397 - val_loss: 1121423260.4932\n",
      "Epoch 597/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1493912611.0685 - val_loss: 1122423997.3699\n",
      "Epoch 598/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1524545877.9178 - val_loss: 1133439840.8767\n",
      "Epoch 599/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1511194840.5479 - val_loss: 1120427347.7260\n",
      "Epoch 600/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 1505707609.4247 - val_loss: 1119568042.9589\n",
      "Epoch 601/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1496216141.1507 - val_loss: 1121339396.8219\n",
      "Epoch 602/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1511581686.7945 - val_loss: 1123193283.0685\n",
      "Epoch 603/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1497424827.6164 - val_loss: 1125414953.6438\n",
      "Epoch 604/3350\n",
      "1168/1168 [==============================] - 0s 55us/step - loss: 1489530269.8082 - val_loss: 1119663715.0685\n",
      "Epoch 605/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 1510179709.3699 - val_loss: 1128041536.0000\n",
      "Epoch 606/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1490764209.0959 - val_loss: 1119329692.9315\n",
      "Epoch 607/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1492607948.2740 - val_loss: 1112995876.8219\n",
      "Epoch 608/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1496907626.0822 - val_loss: 1143958180.8219\n",
      "Epoch 609/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1502558200.1096 - val_loss: 1111712037.6986\n",
      "Epoch 610/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1541524999.0137 - val_loss: 1123931064.9863\n",
      "Epoch 611/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1489471224.1096 - val_loss: 1113024110.9041\n",
      "Epoch 612/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1496298587.1781 - val_loss: 1117932004.3836\n",
      "Epoch 613/3350\n",
      "1168/1168 [==============================] - 0s 55us/step - loss: 1485675520.8767 - val_loss: 1185037832.7671\n",
      "Epoch 614/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1495331203.5068 - val_loss: 1113485390.9041\n",
      "Epoch 615/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 1497986806.3562 - val_loss: 1119438609.9726\n",
      "Epoch 616/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1494642208.4384 - val_loss: 1112517483.8356\n",
      "Epoch 617/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1493082191.7808 - val_loss: 1116831543.2329\n",
      "Epoch 618/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1517628840.3288 - val_loss: 1105305251.5068\n",
      "Epoch 619/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1498763666.4110 - val_loss: 1118153203.7260\n",
      "Epoch 620/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1484095160.9863 - val_loss: 1122796397.5890\n",
      "Epoch 621/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1499168091.1781 - val_loss: 1114679731.7260\n",
      "Epoch 622/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1516731621.6986 - val_loss: 1134044836.8219\n",
      "Epoch 623/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1485083262.2466 - val_loss: 1116331062.7945\n",
      "Epoch 624/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1469216694.3562 - val_loss: 1114067091.2877\n",
      "Epoch 625/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1500217102.0274 - val_loss: 1110790389.0411\n",
      "Epoch 626/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1489794342.5753 - val_loss: 1113422513.9726\n",
      "Epoch 627/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 1484683605.9178 - val_loss: 1112011178.9589\n",
      "Epoch 628/3350\n",
      "1168/1168 [==============================] - 0s 47us/step - loss: 1475529445.2603 - val_loss: 1151583564.2740\n",
      "Epoch 629/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1498252714.9589 - val_loss: 1114355989.4795\n",
      "Epoch 630/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 1461461778.4110 - val_loss: 1203615302.5753\n",
      "Epoch 631/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1496984944.2192 - val_loss: 1171147836.4932\n",
      "Epoch 632/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1500527248.6575 - val_loss: 1121982398.2466\n",
      "Epoch 633/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1479211022.0274 - val_loss: 1137170103.2329\n",
      "Epoch 634/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1501173122.6301 - val_loss: 1122956269.5890\n",
      "Epoch 635/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1484400630.3562 - val_loss: 1161437202.4110\n",
      "Epoch 636/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1476250835.2877 - val_loss: 1115398740.6027\n",
      "Epoch 637/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1476716949.9178 - val_loss: 1164431197.3699\n",
      "Epoch 638/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1493333557.4795 - val_loss: 1114068940.2740\n",
      "Epoch 639/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1478938752.8767 - val_loss: 1154387915.3973\n",
      "Epoch 640/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1469484082.8493 - val_loss: 1104929181.8082\n",
      "Epoch 641/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1472464880.2192 - val_loss: 1186505113.4247\n",
      "Epoch 642/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 1463143160.9863 - val_loss: 1104353235.2877\n",
      "Epoch 643/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1481891432.3288 - val_loss: 1106824301.5890\n",
      "Epoch 644/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1455119984.2192 - val_loss: 1132776952.1096\n",
      "Epoch 645/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1460993410.1918 - val_loss: 1121628240.2192\n",
      "Epoch 646/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1472465116.9315 - val_loss: 1106163862.7945\n",
      "Epoch 647/3350\n",
      "1168/1168 [==============================] - 0s 56us/step - loss: 1455437793.3151 - val_loss: 1119801245.8082\n",
      "Epoch 648/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1458245275.1781 - val_loss: 1113740728.1096\n",
      "Epoch 649/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1483308931.5068 - val_loss: 1205157922.1918\n",
      "Epoch 650/3350\n",
      "1168/1168 [==============================] - 0s 49us/step - loss: 1464482888.7671 - val_loss: 1110174499.0685\n",
      "Epoch 651/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1467622290.4110 - val_loss: 1142668293.6986\n",
      "Epoch 652/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 1468911932.4932 - val_loss: 1100496169.2055\n",
      "Epoch 653/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1503768377.8630 - val_loss: 1096462501.6986\n",
      "Epoch 654/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1494929179.1781 - val_loss: 1119778013.3699\n",
      "Epoch 655/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1464835825.0959 - val_loss: 1142784864.0000\n",
      "Epoch 656/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1457334881.3151 - val_loss: 1125760927.5616\n",
      "Epoch 657/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1456825794.1918 - val_loss: 1113691892.6027\n",
      "Epoch 658/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1470744627.7260 - val_loss: 1109046671.7808\n",
      "Epoch 659/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1459823776.4384 - val_loss: 1111535485.3699\n",
      "Epoch 660/3350\n",
      "1168/1168 [==============================] - 0s 55us/step - loss: 1453026607.3425 - val_loss: 1104036499.7260\n",
      "Epoch 661/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1468322619.6164 - val_loss: 1141239973.6986\n",
      "Epoch 662/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 1483386165.4795 - val_loss: 1214289486.9041\n",
      "Epoch 663/3350\n",
      "1168/1168 [==============================] - 0s 48us/step - loss: 1513658509.1507 - val_loss: 1147153868.2740\n",
      "Epoch 664/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 1466741409.3151 - val_loss: 1099478767.3425\n",
      "Epoch 665/3350\n",
      "1168/1168 [==============================] - 0s 56us/step - loss: 1449168689.0959 - val_loss: 1115629213.8082\n",
      "Epoch 666/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1460325004.2740 - val_loss: 1094590184.3288\n",
      "Epoch 667/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1447262001.5342 - val_loss: 1113784401.5342\n",
      "Epoch 668/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1448414720.8767 - val_loss: 1105767850.0822\n",
      "Epoch 669/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1449098617.4247 - val_loss: 1123905332.6027\n",
      "Epoch 670/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1486693449.6438 - val_loss: 1099786950.1370\n",
      "Epoch 671/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1458275200.8767 - val_loss: 1092722042.7397\n",
      "Epoch 672/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 1439976312.9863 - val_loss: 1099159509.9178\n",
      "Epoch 673/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1463451278.9041 - val_loss: 1100023526.1370\n",
      "Epoch 674/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1453751803.1781 - val_loss: 1111292708.8219\n",
      "Epoch 675/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1480956741.2603 - val_loss: 1126047751.8904\n",
      "Epoch 676/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 1436844507.1781 - val_loss: 1098540367.7808\n",
      "Epoch 677/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1447049003.8356 - val_loss: 1101336120.1096\n",
      "Epoch 678/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1464641299.2877 - val_loss: 1097541936.2192\n",
      "Epoch 679/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 1452216265.6438 - val_loss: 1101121098.0822\n",
      "Epoch 680/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1446570200.5479 - val_loss: 1097310926.9041\n",
      "Epoch 681/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1433530730.0822 - val_loss: 1094855427.5068\n",
      "Epoch 682/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 1442899331.5068 - val_loss: 1105311847.4521\n",
      "Epoch 683/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 1428950560.4384 - val_loss: 1169666172.9315\n",
      "Epoch 684/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 1476531783.0137 - val_loss: 1137850271.5616\n",
      "Epoch 685/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1455630188.7123 - val_loss: 1102049256.3288\n",
      "Epoch 686/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1455551696.6575 - val_loss: 1090354106.3014\n",
      "Epoch 687/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 1440896898.1918 - val_loss: 1096950797.1507\n",
      "Epoch 688/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 1431800958.2466 - val_loss: 1114892599.2329\n",
      "Epoch 689/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 1436724546.6301 - val_loss: 1095564995.5068\n",
      "Epoch 690/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1433224460.7123 - val_loss: 1094820852.6027\n",
      "Epoch 691/3350\n",
      "1168/1168 [==============================] - 0s 83us/step - loss: 1429812970.0822 - val_loss: 1100990971.1781\n",
      "Epoch 692/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1424783265.3151 - val_loss: 1083838031.7808\n",
      "Epoch 693/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1449255655.4521 - val_loss: 1106288518.5753\n",
      "Epoch 694/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1426347576.9863 - val_loss: 1091146384.6575\n",
      "Epoch 695/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1438833589.4795 - val_loss: 1091577572.8219\n",
      "Epoch 696/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 1463884921.8630 - val_loss: 1111990233.8630\n",
      "Epoch 697/3350\n",
      "1168/1168 [==============================] - 0s 51us/step - loss: 1447207417.8630 - val_loss: 1086985120.8767\n",
      "Epoch 698/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 1434175772.9315 - val_loss: 1086007077.6986\n",
      "Epoch 699/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1447780319.5616 - val_loss: 1108650176.4384\n",
      "Epoch 700/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1442906041.8630 - val_loss: 1099114252.7123\n",
      "Epoch 701/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1457903652.8219 - val_loss: 1091579619.9452\n",
      "Epoch 702/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 1431638066.8493 - val_loss: 1099217426.4110\n",
      "Epoch 703/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 1422163358.6849 - val_loss: 1092677347.0685\n",
      "Epoch 704/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1455154806.3562 - val_loss: 1082805856.8767\n",
      "Epoch 705/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1427953318.5753 - val_loss: 1098141453.1507\n",
      "Epoch 706/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1432837098.9589 - val_loss: 1101315422.6849\n",
      "Epoch 707/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1424868392.3288 - val_loss: 1091086945.3151\n",
      "Epoch 708/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1430160905.6438 - val_loss: 1106409234.4110\n",
      "Epoch 709/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1429114491.6164 - val_loss: 1081754452.6027\n",
      "Epoch 710/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1437656872.3288 - val_loss: 1081466366.6849\n",
      "Epoch 711/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1434180058.3014 - val_loss: 1093822259.7260\n",
      "Epoch 712/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 1433925491.7260 - val_loss: 1096896440.1096\n",
      "Epoch 713/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1417163386.7397 - val_loss: 1085795663.7808\n",
      "Epoch 714/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1423100195.0685 - val_loss: 1090142431.5616\n",
      "Epoch 715/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1431808977.5342 - val_loss: 1086122557.3699\n",
      "Epoch 716/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 1440197043.7260 - val_loss: 1120039543.2329\n",
      "Epoch 717/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 1391536308.6027 - val_loss: 1157411663.7808\n",
      "Epoch 718/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 1440892657.9726 - val_loss: 1090851569.0959\n",
      "Epoch 719/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1402547932.9315 - val_loss: 1095036347.6164\n",
      "Epoch 720/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1429133932.7123 - val_loss: 1084880156.9315\n",
      "Epoch 721/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1409219187.7260 - val_loss: 1077625181.3699\n",
      "Epoch 722/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1407543092.6027 - val_loss: 1100418618.7397\n",
      "Epoch 723/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 1424320274.4110 - val_loss: 1099802612.6027\n",
      "Epoch 724/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1409845117.8082 - val_loss: 1082592425.6438\n",
      "Epoch 725/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1407989688.1096 - val_loss: 1087884124.4932\n",
      "Epoch 726/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1403634004.1644 - val_loss: 1103140750.9041\n",
      "Epoch 727/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1411987863.6712 - val_loss: 1076034147.0685\n",
      "Epoch 728/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 1398355485.3699 - val_loss: 1095051265.7534\n",
      "Epoch 729/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1405694395.1781 - val_loss: 1089514819.0685\n",
      "Epoch 730/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1422372687.7808 - val_loss: 1086139125.4795\n",
      "Epoch 731/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1408481041.5342 - val_loss: 1080628316.4932\n",
      "Epoch 732/3350\n",
      "1168/1168 [==============================] - 0s 49us/step - loss: 1423279416.9863 - val_loss: 1064555871.5616\n",
      "Epoch 733/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 1401990980.3836 - val_loss: 1090204986.3014\n",
      "Epoch 734/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 1390779490.1918 - val_loss: 1095696758.3562\n",
      "Epoch 735/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 1400208181.4795 - val_loss: 1092010140.0548\n",
      "Epoch 736/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1397780424.7671 - val_loss: 1076793340.4932\n",
      "Epoch 737/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1411991964.0548 - val_loss: 1080471172.3836\n",
      "Epoch 738/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1411079775.5616 - val_loss: 1077502641.0959\n",
      "Epoch 739/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1391292202.0822 - val_loss: 1066757768.3288\n",
      "Epoch 740/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1399330099.7260 - val_loss: 1074654479.7808\n",
      "Epoch 741/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1422751551.1233 - val_loss: 1073081288.7671\n",
      "Epoch 742/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1397143480.9863 - val_loss: 1190857925.2603\n",
      "Epoch 743/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1415435417.4247 - val_loss: 1063128643.5068\n",
      "Epoch 744/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1387726286.0274 - val_loss: 1095408567.2329\n",
      "Epoch 745/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1413889336.1096 - val_loss: 1060926966.3562\n",
      "Epoch 746/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 1394992000.8767 - val_loss: 1087069053.8082\n",
      "Epoch 747/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1391841578.9589 - val_loss: 1109739108.8219\n",
      "Epoch 748/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1416996743.8904 - val_loss: 1075869610.5205\n",
      "Epoch 749/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1396034096.2192 - val_loss: 1074283694.4658\n",
      "Epoch 750/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1382679717.6986 - val_loss: 1062867153.5342\n",
      "Epoch 751/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1388690925.1507 - val_loss: 1076345044.6027\n",
      "Epoch 752/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1386081413.2603 - val_loss: 1067559412.6027\n",
      "Epoch 753/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1381300964.8219 - val_loss: 1068928875.3973\n",
      "Epoch 754/3350\n",
      "1168/1168 [==============================] - 0s 48us/step - loss: 1387768010.5205 - val_loss: 1059511675.6164\n",
      "Epoch 755/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1381375407.3425 - val_loss: 1095502728.5479\n",
      "Epoch 756/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1435671560.7671 - val_loss: 1089304904.3288\n",
      "Epoch 757/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 1407365560.9863 - val_loss: 1100412964.3836\n",
      "Epoch 758/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1410747855.7808 - val_loss: 1118370715.6164\n",
      "Epoch 759/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1452815167.1233 - val_loss: 1073007288.5479\n",
      "Epoch 760/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1385859800.5479 - val_loss: 1063965108.6027\n",
      "Epoch 761/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 1371817614.0274 - val_loss: 1078467338.0822\n",
      "Epoch 762/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 1405669875.7260 - val_loss: 1072927044.3836\n",
      "Epoch 763/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1388210247.0137 - val_loss: 1068951149.5890\n",
      "Epoch 764/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1375530542.4658 - val_loss: 1087207838.6849\n",
      "Epoch 765/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1382571005.3699 - val_loss: 1057676538.3014\n",
      "Epoch 766/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1391086433.3151 - val_loss: 1135017795.5068\n",
      "Epoch 767/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 1379377723.1781 - val_loss: 1078675973.2603\n",
      "Epoch 768/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 1378877614.4658 - val_loss: 1054432169.6438\n",
      "Epoch 769/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1370045084.9315 - val_loss: 1052416070.1370\n",
      "Epoch 770/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1372740811.3973 - val_loss: 1051866545.9726\n",
      "Epoch 771/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1390401903.3425 - val_loss: 1103583422.6849\n",
      "Epoch 772/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1389066396.9315 - val_loss: 1106254535.0137\n",
      "Epoch 773/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1382622689.3151 - val_loss: 1059046735.7808\n",
      "Epoch 774/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1384627169.3151 - val_loss: 1067893184.8767\n",
      "Epoch 775/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 1385792328.7671 - val_loss: 1054944274.8493\n",
      "Epoch 776/3350\n",
      "1168/1168 [==============================] - 0s 48us/step - loss: 1379687701.9178 - val_loss: 1055129201.0959\n",
      "Epoch 777/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 1368675747.9452 - val_loss: 1046221568.8767\n",
      "Epoch 778/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1363618189.1507 - val_loss: 1088967590.5753\n",
      "Epoch 779/3350\n",
      "1168/1168 [==============================] - 0s 47us/step - loss: 1387956110.9041 - val_loss: 1067916334.4658\n",
      "Epoch 780/3350\n",
      "1168/1168 [==============================] - 0s 101us/step - loss: 1366341344.4384 - val_loss: 1175451618.1918\n",
      "Epoch 781/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1387048497.0959 - val_loss: 1054426116.8219\n",
      "Epoch 782/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1369381457.5342 - val_loss: 1065806666.0822\n",
      "Epoch 783/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1374566509.5890 - val_loss: 1047941009.5342\n",
      "Epoch 784/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1365545396.6027 - val_loss: 1057418266.7397\n",
      "Epoch 785/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1366264092.0548 - val_loss: 1050484012.7123\n",
      "Epoch 786/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1364260055.6712 - val_loss: 1060113221.2603\n",
      "Epoch 787/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1362816127.1233 - val_loss: 1053810717.8082\n",
      "Epoch 788/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1351149210.3014 - val_loss: 1072453754.7397\n",
      "Epoch 789/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1377028292.8219 - val_loss: 1047827598.4658\n",
      "Epoch 790/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1363000849.5342 - val_loss: 1043628827.1781\n",
      "Epoch 791/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1349884214.3562 - val_loss: 1050875396.3836\n",
      "Epoch 792/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1369276854.3562 - val_loss: 1051165778.8493\n",
      "Epoch 793/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1351857820.0548 - val_loss: 1097336484.8219\n",
      "Epoch 794/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1365055294.2466 - val_loss: 1042639447.6712\n",
      "Epoch 795/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1359824376.1096 - val_loss: 1061537636.3836\n",
      "Epoch 796/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1356142836.6027 - val_loss: 1045840292.3836\n",
      "Epoch 797/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1350575137.7534 - val_loss: 1067753130.9589\n",
      "Epoch 798/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1357170332.0548 - val_loss: 1087433509.6986\n",
      "Epoch 799/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1356815582.6849 - val_loss: 1046099828.6027\n",
      "Epoch 800/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 1363182126.4658 - val_loss: 1045221397.0411\n",
      "Epoch 801/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1358066901.0411 - val_loss: 1082557680.6575\n",
      "Epoch 802/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1391684854.3562 - val_loss: 1030011284.1644\n",
      "Epoch 803/3350\n",
      "1168/1168 [==============================] - 0s 48us/step - loss: 1350337911.2329 - val_loss: 1063607389.8082\n",
      "Epoch 804/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 1377955643.6164 - val_loss: 1035761235.7260\n",
      "Epoch 805/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1351958417.5342 - val_loss: 1051929872.2192\n",
      "Epoch 806/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1346318470.1370 - val_loss: 1110172497.0959\n",
      "Epoch 807/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1379215505.5342 - val_loss: 1049881591.2329\n",
      "Epoch 808/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 1356228361.2055 - val_loss: 1043865678.0274\n",
      "Epoch 809/3350\n",
      "1168/1168 [==============================] - 0s 56us/step - loss: 1340092542.2466 - val_loss: 1041811638.7945\n",
      "Epoch 810/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1376653880.9863 - val_loss: 1030094560.4384\n",
      "Epoch 811/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1354628179.5068 - val_loss: 1053614820.8219\n",
      "Epoch 812/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1349587433.2055 - val_loss: 1056915387.1781\n",
      "Epoch 813/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1344244998.1370 - val_loss: 1035965061.6986\n",
      "Epoch 814/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1365274297.8630 - val_loss: 1144427201.7534\n",
      "Epoch 815/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1360751763.2877 - val_loss: 1083151543.2329\n",
      "Epoch 816/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1341469643.3973 - val_loss: 1036468563.2877\n",
      "Epoch 817/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1340749148.0548 - val_loss: 1048414426.3014\n",
      "Epoch 818/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1342728316.4932 - val_loss: 1029180561.5342\n",
      "Epoch 819/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1377814451.7260 - val_loss: 1082741246.2466\n",
      "Epoch 820/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1345653873.9726 - val_loss: 1025302782.6849\n",
      "Epoch 821/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 1340166095.7808 - val_loss: 1075155710.6849\n",
      "Epoch 822/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1351613435.6164 - val_loss: 1043210887.0137\n",
      "Epoch 823/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1333114830.9041 - val_loss: 1047348433.5342\n",
      "Epoch 824/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1374223946.5205 - val_loss: 1032411740.0548\n",
      "Epoch 825/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1349973153.3151 - val_loss: 1045152357.9178\n",
      "Epoch 826/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1357148651.8356 - val_loss: 1027320415.5616\n",
      "Epoch 827/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1331877768.7671 - val_loss: 1021009040.6575\n",
      "Epoch 828/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1338080530.8493 - val_loss: 1027339141.6986\n",
      "Epoch 829/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1323633850.7397 - val_loss: 1023103840.8767\n",
      "Epoch 830/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1359378210.6301 - val_loss: 1046633474.6301\n",
      "Epoch 831/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1345995567.3425 - val_loss: 1024901297.0959\n",
      "Epoch 832/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1328661610.7397 - val_loss: 1026143948.9315\n",
      "Epoch 833/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1351042510.9041 - val_loss: 1017475805.8082\n",
      "Epoch 834/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1330523847.8904 - val_loss: 1030647859.5068\n",
      "Epoch 835/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1345808542.6849 - val_loss: 1017649550.0274\n",
      "Epoch 836/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1335118195.7260 - val_loss: 1029359635.7260\n",
      "Epoch 837/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1328964727.2329 - val_loss: 1016916122.7397\n",
      "Epoch 838/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1324566437.6986 - val_loss: 1020543340.7123\n",
      "Epoch 839/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1325784874.0822 - val_loss: 1021213525.0411\n",
      "Epoch 840/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 1319955957.4795 - val_loss: 1068299491.5068\n",
      "Epoch 841/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1341450443.8356 - val_loss: 1037408083.2877\n",
      "Epoch 842/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1329254949.6986 - val_loss: 1018168217.4247\n",
      "Epoch 843/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1332368554.0822 - val_loss: 1030382805.9178\n",
      "Epoch 844/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1335269567.1233 - val_loss: 1029306697.6438\n",
      "Epoch 845/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1319741162.9589 - val_loss: 1022305321.4247\n",
      "Epoch 846/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1319549981.8082 - val_loss: 1018639114.5205\n",
      "Epoch 847/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1322826583.6712 - val_loss: 1046518298.3014\n",
      "Epoch 848/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1318704458.5205 - val_loss: 1019947324.4932\n",
      "Epoch 849/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1315300640.8767 - val_loss: 1022774177.7534\n",
      "Epoch 850/3350\n",
      "1168/1168 [==============================] - 0s 49us/step - loss: 1315092111.7808 - val_loss: 1015747836.0548\n",
      "Epoch 851/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1315031648.4384 - val_loss: 1057773749.0411\n",
      "Epoch 852/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 1361123571.7260 - val_loss: 1017859992.1096\n",
      "Epoch 853/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1337508782.0274 - val_loss: 1052166811.1781\n",
      "Epoch 854/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1320613355.8356 - val_loss: 1028272883.7260\n",
      "Epoch 855/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1307903807.5616 - val_loss: 1008026893.1507\n",
      "Epoch 856/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1318777823.1233 - val_loss: 1054870028.2740\n",
      "Epoch 857/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1319570286.4658 - val_loss: 1011840875.8356\n",
      "Epoch 858/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1307115667.7260 - val_loss: 1012536054.7945\n",
      "Epoch 859/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1309728456.7671 - val_loss: 1035750720.8767\n",
      "Epoch 860/3350\n",
      "1168/1168 [==============================] - 0s 53us/step - loss: 1326819521.7534 - val_loss: 1064172772.3836\n",
      "Epoch 861/3350\n",
      "1168/1168 [==============================] - 0s 55us/step - loss: 1308369681.5342 - val_loss: 1011265767.4521\n",
      "Epoch 862/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1315185502.6849 - val_loss: 1018231959.2329\n",
      "Epoch 863/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1313204862.2466 - val_loss: 1012256612.3836\n",
      "Epoch 864/3350\n",
      "1168/1168 [==============================] - 0s 56us/step - loss: 1300375267.9452 - val_loss: 1014327873.3151\n",
      "Epoch 865/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 1296452161.7534 - val_loss: 1013160605.1507\n",
      "Epoch 866/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1295913771.8356 - val_loss: 1005480346.7397\n",
      "Epoch 867/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1298777823.5616 - val_loss: 1011833417.6438\n",
      "Epoch 868/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1302237675.8356 - val_loss: 1015413974.7945\n",
      "Epoch 869/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1300188728.9863 - val_loss: 1005867081.6438\n",
      "Epoch 870/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1297438442.0822 - val_loss: 1007977230.6849\n",
      "Epoch 871/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1358659228.9315 - val_loss: 1042615917.1507\n",
      "Epoch 872/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1306394434.6301 - val_loss: 1003496945.0959\n",
      "Epoch 873/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1288230939.1781 - val_loss: 1002768522.5205\n",
      "Epoch 874/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1272608022.7945 - val_loss: 1084781223.4521\n",
      "Epoch 875/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1315600975.7808 - val_loss: 1000084736.0000\n",
      "Epoch 876/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1302431474.8493 - val_loss: 1003625342.2466\n",
      "Epoch 877/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1304031161.8630 - val_loss: 999281714.8493\n",
      "Epoch 878/3350\n",
      "1168/1168 [==============================] - 0s 54us/step - loss: 1300555964.4932 - val_loss: 997138722.1918\n",
      "Epoch 879/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 1314156313.4247 - val_loss: 1018497261.8082\n",
      "Epoch 880/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1292205704.7671 - val_loss: 1007059019.8356\n",
      "Epoch 881/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1308992309.4795 - val_loss: 995727420.9315\n",
      "Epoch 882/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1283013487.3425 - val_loss: 1007254513.9726\n",
      "Epoch 883/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1284293908.1644 - val_loss: 1014694773.9178\n",
      "Epoch 884/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1296131854.9041 - val_loss: 996269811.2877\n",
      "Epoch 885/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1292564625.0959 - val_loss: 1003021227.1781\n",
      "Epoch 886/3350\n",
      "1168/1168 [==============================] - 0s 56us/step - loss: 1288784934.5753 - val_loss: 996152635.1781\n",
      "Epoch 887/3350\n",
      "1168/1168 [==============================] - 0s 47us/step - loss: 1289727201.3151 - val_loss: 998673782.3562\n",
      "Epoch 888/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1295921235.2877 - val_loss: 1013960099.5068\n",
      "Epoch 889/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1292596427.3973 - val_loss: 1006360069.4795\n",
      "Epoch 890/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1275297157.2603 - val_loss: 1006103736.1096\n",
      "Epoch 891/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 1289984341.9178 - val_loss: 1005226728.3288\n",
      "Epoch 892/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1287129386.9589 - val_loss: 994189612.2740\n",
      "Epoch 893/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1294044200.3288 - val_loss: 1038094508.7123\n",
      "Epoch 894/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1274186731.8356 - val_loss: 1007814127.3425\n",
      "Epoch 895/3350\n",
      "1168/1168 [==============================] - 0s 56us/step - loss: 1273567893.4795 - val_loss: 993643428.1644\n",
      "Epoch 896/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1287118043.1781 - val_loss: 990177703.8904\n",
      "Epoch 897/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1286203066.7397 - val_loss: 994347361.7534\n",
      "Epoch 898/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1273151844.8219 - val_loss: 984450556.0548\n",
      "Epoch 899/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1277933317.2603 - val_loss: 1085298793.6438\n",
      "Epoch 900/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1288736905.6438 - val_loss: 989994784.0000\n",
      "Epoch 901/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 1305966970.3014 - val_loss: 1040701963.3973\n",
      "Epoch 902/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1313122020.8219 - val_loss: 1004921815.6712\n",
      "Epoch 903/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1283386025.2055 - val_loss: 981042560.6575\n",
      "Epoch 904/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1270907955.7260 - val_loss: 987937550.4658\n",
      "Epoch 905/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1285611893.4795 - val_loss: 998190595.9452\n",
      "Epoch 906/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1283202517.0411 - val_loss: 993516196.6027\n",
      "Epoch 907/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1326390131.7260 - val_loss: 1023346863.7808\n",
      "Epoch 908/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1265971792.6575 - val_loss: 993650798.9041\n",
      "Epoch 909/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1281648975.7808 - val_loss: 984729452.2740\n",
      "Epoch 910/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1264444747.3973 - val_loss: 989726610.6301\n",
      "Epoch 911/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1269522655.5616 - val_loss: 1007041557.4795\n",
      "Epoch 912/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1267219686.5753 - val_loss: 998848039.6712\n",
      "Epoch 913/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1263242001.5342 - val_loss: 978733169.5342\n",
      "Epoch 914/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1284958467.5068 - val_loss: 980778432.0000\n",
      "Epoch 915/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 1291973852.4932 - val_loss: 973638334.2466\n",
      "Epoch 916/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1260567334.5753 - val_loss: 978474572.7123\n",
      "Epoch 917/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1259191134.6849 - val_loss: 984495796.3836\n",
      "Epoch 918/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1266574614.7945 - val_loss: 970643061.9178\n",
      "Epoch 919/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1258411285.0411 - val_loss: 1001803683.0685\n",
      "Epoch 920/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1274897583.7808 - val_loss: 1024734543.7808\n",
      "Epoch 921/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1268121892.8219 - val_loss: 979856568.3288\n",
      "Epoch 922/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1277013684.6027 - val_loss: 963810227.2877\n",
      "Epoch 923/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1250031452.9315 - val_loss: 987969492.1644\n",
      "Epoch 924/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1244428512.4384 - val_loss: 977602688.8767\n",
      "Epoch 925/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1255669926.5753 - val_loss: 987110303.5616\n",
      "Epoch 926/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1264466015.5616 - val_loss: 972993760.0000\n",
      "Epoch 927/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1258335337.2055 - val_loss: 963458990.4658\n",
      "Epoch 928/3350\n",
      "1168/1168 [==============================] - 0s 50us/step - loss: 1253601080.1096 - val_loss: 976755153.5342\n",
      "Epoch 929/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 1251050191.7808 - val_loss: 973365973.9178\n",
      "Epoch 930/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1280895261.8082 - val_loss: 972570414.4658\n",
      "Epoch 931/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1266179955.7260 - val_loss: 989068701.3699\n",
      "Epoch 932/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1289046231.6712 - val_loss: 992446103.8904\n",
      "Epoch 933/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1257732673.7534 - val_loss: 958600670.6849\n",
      "Epoch 934/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1241666735.1233 - val_loss: 966251889.5342\n",
      "Epoch 935/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1236556602.7397 - val_loss: 961633545.6438\n",
      "Epoch 936/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 1245733640.7671 - val_loss: 981273661.5890\n",
      "Epoch 937/3350\n",
      "1168/1168 [==============================] - 0s 47us/step - loss: 1270702225.0959 - val_loss: 977490192.6575\n",
      "Epoch 938/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1249973018.3014 - val_loss: 970296286.2466\n",
      "Epoch 939/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 1237834092.7123 - val_loss: 966780702.2466\n",
      "Epoch 940/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1228297085.3699 - val_loss: 1035057951.1233\n",
      "Epoch 941/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1260939262.2466 - val_loss: 957697864.3288\n",
      "Epoch 942/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1235722628.3836 - val_loss: 986688704.8767\n",
      "Epoch 943/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1241176226.4110 - val_loss: 983328243.9452\n",
      "Epoch 944/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1229826578.4110 - val_loss: 971940268.7123\n",
      "Epoch 945/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1286765728.4384 - val_loss: 969345728.8767\n",
      "Epoch 946/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1242485368.1096 - val_loss: 971464510.2466\n",
      "Epoch 947/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1226140430.9041 - val_loss: 971081185.0959\n",
      "Epoch 948/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1235352066.6301 - val_loss: 956956706.4110\n",
      "Epoch 949/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1238168906.5205 - val_loss: 971553941.0411\n",
      "Epoch 950/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1237434478.4658 - val_loss: 979096113.0959\n",
      "Epoch 951/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 1240843608.5479 - val_loss: 984619695.7808\n",
      "Epoch 952/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1256966543.3425 - val_loss: 977981356.9315\n",
      "Epoch 953/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1229899996.0548 - val_loss: 965905428.1644\n",
      "Epoch 954/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1229433777.9726 - val_loss: 959385788.9315\n",
      "Epoch 955/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1227514474.0822 - val_loss: 956111552.0000\n",
      "Epoch 956/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 1242871501.1507 - val_loss: 958743701.0411\n",
      "Epoch 957/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1253917888.8767 - val_loss: 954559772.4932\n",
      "Epoch 958/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1281861397.9178 - val_loss: 947820715.8356\n",
      "Epoch 959/3350\n",
      "1168/1168 [==============================] - 0s 51us/step - loss: 1253230293.0411 - val_loss: 1002827552.8767\n",
      "Epoch 960/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 1228455676.4932 - val_loss: 969990852.6027\n",
      "Epoch 961/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1238384281.4247 - val_loss: 953250897.9726\n",
      "Epoch 962/3350\n",
      "1168/1168 [==============================] - 0s 49us/step - loss: 1229875534.0274 - val_loss: 968134318.4658\n",
      "Epoch 963/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 1247334985.6438 - val_loss: 958009464.9863\n",
      "Epoch 964/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1220037255.0137 - val_loss: 963372320.0000\n",
      "Epoch 965/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1237507149.1507 - val_loss: 955078542.6849\n",
      "Epoch 966/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1233548565.9178 - val_loss: 958314932.1644\n",
      "Epoch 967/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1222794843.1781 - val_loss: 950041989.0411\n",
      "Epoch 968/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1252374304.8767 - val_loss: 953788815.3425\n",
      "Epoch 969/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1230550755.9452 - val_loss: 987348528.4384\n",
      "Epoch 970/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1232996051.2877 - val_loss: 960233407.3425\n",
      "Epoch 971/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1218626282.0822 - val_loss: 949280254.4658\n",
      "Epoch 972/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1242848849.5342 - val_loss: 945353276.0548\n",
      "Epoch 973/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1217625052.9315 - val_loss: 972250201.6438\n",
      "Epoch 974/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1218006197.4795 - val_loss: 949848410.7397\n",
      "Epoch 975/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 1214702590.2466 - val_loss: 946800799.1233\n",
      "Epoch 976/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1230870371.5068 - val_loss: 975212648.7671\n",
      "Epoch 977/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1212177813.9178 - val_loss: 950877837.8082\n",
      "Epoch 978/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1219507759.3425 - val_loss: 958516878.2466\n",
      "Epoch 979/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1210854905.8630 - val_loss: 953994353.5342\n",
      "Epoch 980/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1224626949.2603 - val_loss: 951970187.3973\n",
      "Epoch 981/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1205722241.7534 - val_loss: 957554258.4110\n",
      "Epoch 982/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1204180971.8356 - val_loss: 945843911.8904\n",
      "Epoch 983/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1216932387.9452 - val_loss: 945614732.7123\n",
      "Epoch 984/3350\n",
      "1168/1168 [==============================] - 0s 49us/step - loss: 1212674848.4384 - val_loss: 959503674.7397\n",
      "Epoch 985/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 1213751186.4110 - val_loss: 948738638.0274\n",
      "Epoch 986/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1195682147.9452 - val_loss: 958059744.4384\n",
      "Epoch 987/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1207861774.0274 - val_loss: 947861538.6301\n",
      "Epoch 988/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1216910897.0959 - val_loss: 966685269.0411\n",
      "Epoch 989/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1216188529.0959 - val_loss: 1061058723.0685\n",
      "Epoch 990/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1255595211.3973 - val_loss: 1083505275.6164\n",
      "Epoch 991/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1247032547.0685 - val_loss: 953752354.6301\n",
      "Epoch 992/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1196074420.6027 - val_loss: 962208280.1096\n",
      "Epoch 993/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1199725355.8356 - val_loss: 944925304.1096\n",
      "Epoch 994/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1192210360.5479 - val_loss: 945380359.6712\n",
      "Epoch 995/3350\n",
      "1168/1168 [==============================] - 0s 51us/step - loss: 1208216557.5890 - val_loss: 949325957.0411\n",
      "Epoch 996/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1213222373.2603 - val_loss: 939224862.2466\n",
      "Epoch 997/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1212755084.2740 - val_loss: 944741589.4795\n",
      "Epoch 998/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1188330924.9315 - val_loss: 1001660028.9315\n",
      "Epoch 999/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 1195292610.6301 - val_loss: 944057227.6164\n",
      "Epoch 1000/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1210334385.9726 - val_loss: 971242794.5205\n",
      "Epoch 1001/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1197198222.0274 - val_loss: 946173463.2329\n",
      "Epoch 1002/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1229884566.7945 - val_loss: 961048941.5890\n",
      "Epoch 1003/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1195951590.1370 - val_loss: 947786004.1644\n",
      "Epoch 1004/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 1227987684.8219 - val_loss: 933068993.7534\n",
      "Epoch 1005/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1197095554.6301 - val_loss: 937503854.2466\n",
      "Epoch 1006/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1199191757.1507 - val_loss: 951079564.2740\n",
      "Epoch 1007/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1227772358.5753 - val_loss: 931411483.3973\n",
      "Epoch 1008/3350\n",
      "1168/1168 [==============================] - 0s 47us/step - loss: 1184309060.3836 - val_loss: 953337692.2740\n",
      "Epoch 1009/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 1210278424.5479 - val_loss: 938915116.0548\n",
      "Epoch 1010/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1191989298.8493 - val_loss: 957486063.1233\n",
      "Epoch 1011/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1177117572.8219 - val_loss: 942327879.8904\n",
      "Epoch 1012/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1191769889.3151 - val_loss: 935310700.9315\n",
      "Epoch 1013/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1205575576.5479 - val_loss: 929540429.1507\n",
      "Epoch 1014/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1197586025.2055 - val_loss: 940097825.7534\n",
      "Epoch 1015/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1179261168.6575 - val_loss: 929534802.8493\n",
      "Epoch 1016/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1175256715.3973 - val_loss: 949864796.7123\n",
      "Epoch 1017/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1180878366.2466 - val_loss: 928751351.2329\n",
      "Epoch 1018/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1166059759.7808 - val_loss: 974253791.1233\n",
      "Epoch 1019/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 1195396003.0685 - val_loss: 925783512.5479\n",
      "Epoch 1020/3350\n",
      "1168/1168 [==============================] - 0s 46us/step - loss: 1168793638.5753 - val_loss: 929735067.1781\n",
      "Epoch 1021/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 1185284390.5753 - val_loss: 985654320.2192\n",
      "Epoch 1022/3350\n",
      "1168/1168 [==============================] - 0s 50us/step - loss: 1229242716.0548 - val_loss: 922976389.6986\n",
      "Epoch 1023/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1177040806.1370 - val_loss: 930116679.4521\n",
      "Epoch 1024/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1177193372.9315 - val_loss: 924830567.8904\n",
      "Epoch 1025/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1205285262.0274 - val_loss: 927308798.2466\n",
      "Epoch 1026/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1169855932.9315 - val_loss: 989142220.2740\n",
      "Epoch 1027/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1178273568.4384 - val_loss: 938019376.8767\n",
      "Epoch 1028/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1175478849.7534 - val_loss: 917497945.8630\n",
      "Epoch 1029/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1163237668.3836 - val_loss: 929018826.5205\n",
      "Epoch 1030/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1173974627.9452 - val_loss: 913704070.5753\n",
      "Epoch 1031/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1207494032.6575 - val_loss: 951887829.4795\n",
      "Epoch 1032/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1168592214.7945 - val_loss: 938903294.0274\n",
      "Epoch 1033/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1161686521.8630 - val_loss: 940687839.5616\n",
      "Epoch 1034/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1162629643.3973 - val_loss: 948690479.3425\n",
      "Epoch 1035/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1220446141.3699 - val_loss: 954986378.0822\n",
      "Epoch 1036/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1196228981.4795 - val_loss: 904035650.4110\n",
      "Epoch 1037/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1156868437.0411 - val_loss: 914303871.5616\n",
      "Epoch 1038/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1190093930.5205 - val_loss: 932773567.5616\n",
      "Epoch 1039/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1172931782.1370 - val_loss: 958326861.1507\n",
      "Epoch 1040/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1171813085.8082 - val_loss: 907740210.8493\n",
      "Epoch 1041/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1154977496.5479 - val_loss: 922595653.0411\n",
      "Epoch 1042/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1153847292.4932 - val_loss: 922835028.6027\n",
      "Epoch 1043/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 1190111485.3699 - val_loss: 957772599.8904\n",
      "Epoch 1044/3350\n",
      "1168/1168 [==============================] - 0s 55us/step - loss: 1163248960.0000 - val_loss: 910605744.6575\n",
      "Epoch 1045/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 1160426774.7945 - val_loss: 949824733.1507\n",
      "Epoch 1046/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1162099375.3425 - val_loss: 979912957.8082\n",
      "Epoch 1047/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1189963577.8630 - val_loss: 903772205.1507\n",
      "Epoch 1048/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1165118159.7808 - val_loss: 904526839.4521\n",
      "Epoch 1049/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1155169902.4658 - val_loss: 910855620.8219\n",
      "Epoch 1050/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1161673565.8082 - val_loss: 930951457.7534\n",
      "Epoch 1051/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1176347634.8493 - val_loss: 916111376.4384\n",
      "Epoch 1052/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1141212623.7808 - val_loss: 909125538.1918\n",
      "Epoch 1053/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1143982277.6986 - val_loss: 920435498.3014\n",
      "Epoch 1054/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1146594204.0548 - val_loss: 908398328.1096\n",
      "Epoch 1055/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1153537498.3014 - val_loss: 899369742.2466\n",
      "Epoch 1056/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1134818839.2329 - val_loss: 907031658.0822\n",
      "Epoch 1057/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1166725700.3836 - val_loss: 919086216.7671\n",
      "Epoch 1058/3350\n",
      "1168/1168 [==============================] - 0s 48us/step - loss: 1141648308.6027 - val_loss: 975751591.4521\n",
      "Epoch 1059/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1177019183.7808 - val_loss: 917350569.4247\n",
      "Epoch 1060/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1133985330.8493 - val_loss: 939682645.2603\n",
      "Epoch 1061/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 1159764024.9863 - val_loss: 1021617115.6164\n",
      "Epoch 1062/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1180094373.6986 - val_loss: 901089974.7945\n",
      "Epoch 1063/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1147174055.4521 - val_loss: 901885706.0822\n",
      "Epoch 1064/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1150081333.9178 - val_loss: 927564443.3973\n",
      "Epoch 1065/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1146351440.2192 - val_loss: 922081865.2055\n",
      "Epoch 1066/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1145824041.2055 - val_loss: 901713961.2055\n",
      "Epoch 1067/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1163293558.3562 - val_loss: 915857843.7260\n",
      "Epoch 1068/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1147145975.6712 - val_loss: 916054906.7397\n",
      "Epoch 1069/3350\n",
      "1168/1168 [==============================] - 0s 53us/step - loss: 1157224192.8767 - val_loss: 892247944.3288\n",
      "Epoch 1070/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 1139216975.7808 - val_loss: 905207252.8219\n",
      "Epoch 1071/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1155393433.8630 - val_loss: 895568919.2329\n",
      "Epoch 1072/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1134000652.2740 - val_loss: 893654307.2877\n",
      "Epoch 1073/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1160932339.7260 - val_loss: 906211913.8630\n",
      "Epoch 1074/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1120592754.8493 - val_loss: 891322799.3425\n",
      "Epoch 1075/3350\n",
      "1168/1168 [==============================] - 0s 56us/step - loss: 1132252298.5205 - val_loss: 905677624.1096\n",
      "Epoch 1076/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 1122024475.6164 - val_loss: 912001646.9041\n",
      "Epoch 1077/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1150522019.9452 - val_loss: 883979279.7808\n",
      "Epoch 1078/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1196637358.4658 - val_loss: 936041521.3151\n",
      "Epoch 1079/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1136881476.3836 - val_loss: 941809871.3425\n",
      "Epoch 1080/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1141936128.8767 - val_loss: 908050119.8904\n",
      "Epoch 1081/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1146698256.6575 - val_loss: 890733434.3014\n",
      "Epoch 1082/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1120394053.6986 - val_loss: 898785786.9589\n",
      "Epoch 1083/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1118624607.5616 - val_loss: 906893001.2055\n",
      "Epoch 1084/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1114377060.8219 - val_loss: 912094339.9452\n",
      "Epoch 1085/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1126074305.7534 - val_loss: 909300909.3699\n",
      "Epoch 1086/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 1119315128.5479 - val_loss: 894252776.3288\n",
      "Epoch 1087/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1140701987.0685 - val_loss: 888713375.5616\n",
      "Epoch 1088/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1142846010.7397 - val_loss: 889064386.6301\n",
      "Epoch 1089/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1129721073.0959 - val_loss: 891846696.7671\n",
      "Epoch 1090/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1114175248.6575 - val_loss: 889149197.1507\n",
      "Epoch 1091/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1106535096.9863 - val_loss: 894831512.5479\n",
      "Epoch 1092/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1116915325.8082 - val_loss: 896117016.5479\n",
      "Epoch 1093/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1127784908.2740 - val_loss: 890990952.9863\n",
      "Epoch 1094/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1109901858.1918 - val_loss: 896464934.5753\n",
      "Epoch 1095/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1110748832.4384 - val_loss: 876498627.0685\n",
      "Epoch 1096/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1119463903.5616 - val_loss: 887709262.0274\n",
      "Epoch 1097/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1137476848.6575 - val_loss: 884830606.9041\n",
      "Epoch 1098/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 1133166546.4110 - val_loss: 887691371.3973\n",
      "Epoch 1099/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1136829305.8630 - val_loss: 891577898.0822\n",
      "Epoch 1100/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1139092459.8356 - val_loss: 890484519.2329\n",
      "Epoch 1101/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1145255597.5890 - val_loss: 898568005.2603\n",
      "Epoch 1102/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 1110728059.6164 - val_loss: 889153045.9178\n",
      "Epoch 1103/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1107903458.1918 - val_loss: 964398702.9041\n",
      "Epoch 1104/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1124034394.3014 - val_loss: 888338902.7945\n",
      "Epoch 1105/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1121514099.7260 - val_loss: 881630655.5616\n",
      "Epoch 1106/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 1121122744.5479 - val_loss: 886393733.0411\n",
      "Epoch 1107/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1119968834.1918 - val_loss: 880090502.5753\n",
      "Epoch 1108/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1104178497.7534 - val_loss: 883890286.9041\n",
      "Epoch 1109/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1105943774.6849 - val_loss: 885668315.6164\n",
      "Epoch 1110/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1122057305.4247 - val_loss: 877586951.0137\n",
      "Epoch 1111/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1105769852.4932 - val_loss: 872492369.7534\n",
      "Epoch 1112/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1118745280.8767 - val_loss: 871226598.1370\n",
      "Epoch 1113/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 1109653067.8356 - val_loss: 879590916.1644\n",
      "Epoch 1114/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1101308958.6849 - val_loss: 862545578.0822\n",
      "Epoch 1115/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1090887509.9178 - val_loss: 886599543.6712\n",
      "Epoch 1116/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1129081224.3288 - val_loss: 939818644.1644\n",
      "Epoch 1117/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1111760815.3425 - val_loss: 870061404.7123\n",
      "Epoch 1118/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1105409701.6986 - val_loss: 978312586.0822\n",
      "Epoch 1119/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1110010509.1507 - val_loss: 880687212.7123\n",
      "Epoch 1120/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 1150395590.1370 - val_loss: 897463387.3973\n",
      "Epoch 1121/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1098384428.7123 - val_loss: 876913989.4795\n",
      "Epoch 1122/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1087557261.1507 - val_loss: 883583276.4932\n",
      "Epoch 1123/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1116844640.4384 - val_loss: 958265688.9863\n",
      "Epoch 1124/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1088149003.8356 - val_loss: 975769685.4795\n",
      "Epoch 1125/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1122914999.6712 - val_loss: 875580105.8630\n",
      "Epoch 1126/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1102835837.3699 - val_loss: 879037392.6575\n",
      "Epoch 1127/3350\n",
      "1168/1168 [==============================] - 0s 79us/step - loss: 1125430786.6301 - val_loss: 870182755.5068\n",
      "Epoch 1128/3350\n",
      "1168/1168 [==============================] - 0s 50us/step - loss: 1097721630.6849 - val_loss: 874419351.2329\n",
      "Epoch 1129/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 1078560459.3973 - val_loss: 898133091.2877\n",
      "Epoch 1130/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1079251265.7534 - val_loss: 859387437.1507\n",
      "Epoch 1131/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1081897221.2603 - val_loss: 874528900.3836\n",
      "Epoch 1132/3350\n",
      "1168/1168 [==============================] - 0s 53us/step - loss: 1075375176.3288 - val_loss: 870883563.3973\n",
      "Epoch 1133/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1095316547.5068 - val_loss: 861813166.2466\n",
      "Epoch 1134/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1102948351.1233 - val_loss: 882733946.7397\n",
      "Epoch 1135/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 1098510859.3973 - val_loss: 867374709.4795\n",
      "Epoch 1136/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1067728091.6164 - val_loss: 887719497.2055\n",
      "Epoch 1137/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1090809216.0000 - val_loss: 866299374.6849\n",
      "Epoch 1138/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1092151919.3425 - val_loss: 866421315.0685\n",
      "Epoch 1139/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1085104708.3836 - val_loss: 861764655.7808\n",
      "Epoch 1140/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1093426071.6712 - val_loss: 894066323.7260\n",
      "Epoch 1141/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1075674117.2603 - val_loss: 878179913.6438\n",
      "Epoch 1142/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1067368519.8904 - val_loss: 939059398.1370\n",
      "Epoch 1143/3350\n",
      "1168/1168 [==============================] - 0s 88us/step - loss: 1124431936.0000 - val_loss: 852023014.7945\n",
      "Epoch 1144/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1062281025.7534 - val_loss: 940878262.7945\n",
      "Epoch 1145/3350\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 1085699667.2877 - val_loss: 868094011.6164\n",
      "Epoch 1146/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1084644806.1370 - val_loss: 848020234.5205\n",
      "Epoch 1147/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1065699273.6438 - val_loss: 859109026.8493\n",
      "Epoch 1148/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 1064329883.1781 - val_loss: 856127013.6986\n",
      "Epoch 1149/3350\n",
      "1168/1168 [==============================] - 0s 51us/step - loss: 1081562284.7123 - val_loss: 853284374.3562\n",
      "Epoch 1150/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 1070860387.0685 - val_loss: 863508335.3425\n",
      "Epoch 1151/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1097805703.0137 - val_loss: 872139441.7534\n",
      "Epoch 1152/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1097931626.5205 - val_loss: 847791807.1233\n",
      "Epoch 1153/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1063805016.5479 - val_loss: 850806812.0548\n",
      "Epoch 1154/3350\n",
      "1168/1168 [==============================] - 0s 85us/step - loss: 1060347786.9589 - val_loss: 855994626.1918\n",
      "Epoch 1155/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 1072512894.2466 - val_loss: 853172864.4384\n",
      "Epoch 1156/3350\n",
      "1168/1168 [==============================] - 0s 89us/step - loss: 1077701128.7671 - val_loss: 852807255.0137\n",
      "Epoch 1157/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 1067190207.5616 - val_loss: 847789504.8767\n",
      "Epoch 1158/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 1058273294.9041 - val_loss: 853660959.5616\n",
      "Epoch 1159/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1067678488.1096 - val_loss: 845026450.8493\n",
      "Epoch 1160/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1069826137.4247 - val_loss: 850313059.9452\n",
      "Epoch 1161/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1078178823.8904 - val_loss: 852554815.5616\n",
      "Epoch 1162/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 1069426295.6712 - val_loss: 848714805.2603\n",
      "Epoch 1163/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 1055988388.8219 - val_loss: 892464965.2603\n",
      "Epoch 1164/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1060883591.4521 - val_loss: 841907441.7534\n",
      "Epoch 1165/3350\n",
      "1168/1168 [==============================] - 0s 80us/step - loss: 1059117170.4110 - val_loss: 914698046.2466\n",
      "Epoch 1166/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1057086197.4795 - val_loss: 854670543.3425\n",
      "Epoch 1167/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 1064809362.4110 - val_loss: 849182689.0959\n",
      "Epoch 1168/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 1052821587.2877 - val_loss: 862488878.0274\n",
      "Epoch 1169/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 1055993742.9041 - val_loss: 865900207.7808\n",
      "Epoch 1170/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1052711491.9452 - val_loss: 869307068.9315\n",
      "Epoch 1171/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1060546792.7671 - val_loss: 853226498.1918\n",
      "Epoch 1172/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1043186208.4384 - val_loss: 857576243.2877\n",
      "Epoch 1173/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 1050785611.3973 - val_loss: 834914826.5205\n",
      "Epoch 1174/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1050742753.3151 - val_loss: 848245243.1781\n",
      "Epoch 1175/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 1053311643.1781 - val_loss: 842487429.6986\n",
      "Epoch 1176/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 1046964194.1918 - val_loss: 902812007.0137\n",
      "Epoch 1177/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1072609713.9726 - val_loss: 841727314.8493\n",
      "Epoch 1178/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 1057676987.6164 - val_loss: 838092608.8767\n",
      "Epoch 1179/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 1038993739.3973 - val_loss: 838013330.8493\n",
      "Epoch 1180/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 1036220252.9315 - val_loss: 859401835.1781\n",
      "Epoch 1181/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1040139176.7671 - val_loss: 845341102.0274\n",
      "Epoch 1182/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1043764660.6027 - val_loss: 850480638.4658\n",
      "Epoch 1183/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1087307249.0959 - val_loss: 834925298.8493\n",
      "Epoch 1184/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 1039430744.9863 - val_loss: 837025802.5205\n",
      "Epoch 1185/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1040990984.3288 - val_loss: 833592047.5616\n",
      "Epoch 1186/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1048772786.8493 - val_loss: 832719700.8219\n",
      "Epoch 1187/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 1036577234.4110 - val_loss: 856548123.6164\n",
      "Epoch 1188/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1063584242.8493 - val_loss: 836726928.4384\n",
      "Epoch 1189/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1047634446.4658 - val_loss: 841210985.6438\n",
      "Epoch 1190/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1036628489.6438 - val_loss: 841561620.1644\n",
      "Epoch 1191/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1050299988.1644 - val_loss: 828203778.6301\n",
      "Epoch 1192/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 1035230262.3562 - val_loss: 833997883.6164\n",
      "Epoch 1193/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 1051383057.5342 - val_loss: 832738045.3699\n",
      "Epoch 1194/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1039241998.9041 - val_loss: 842565617.3151\n",
      "Epoch 1195/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1027587396.3836 - val_loss: 847121493.6986\n",
      "Epoch 1196/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1028849223.8904 - val_loss: 828315624.9863\n",
      "Epoch 1197/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1046183503.7808 - val_loss: 846879975.2329\n",
      "Epoch 1198/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 1051794388.1644 - val_loss: 841036956.0548\n",
      "Epoch 1199/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1043519918.9041 - val_loss: 848678764.9315\n",
      "Epoch 1200/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1046867508.6027 - val_loss: 831605066.0822\n",
      "Epoch 1201/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1058892710.5753 - val_loss: 831548006.5753\n",
      "Epoch 1202/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1017776617.2055 - val_loss: 845448244.1644\n",
      "Epoch 1203/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1029400698.3014 - val_loss: 850144482.6301\n",
      "Epoch 1204/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1029630834.8493 - val_loss: 840649366.5753\n",
      "Epoch 1205/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1030546126.0274 - val_loss: 837481577.2055\n",
      "Epoch 1206/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1056812266.0822 - val_loss: 829171652.8219\n",
      "Epoch 1207/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1032916579.0685 - val_loss: 834122492.4932\n",
      "Epoch 1208/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1024494163.2877 - val_loss: 842760640.0000\n",
      "Epoch 1209/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1072077381.2603 - val_loss: 825415473.9726\n",
      "Epoch 1210/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1021136180.6027 - val_loss: 820521703.0137\n",
      "Epoch 1211/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1037832575.5616 - val_loss: 862236338.1918\n",
      "Epoch 1212/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1016838077.3699 - val_loss: 824096247.2329\n",
      "Epoch 1213/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1015641885.1507 - val_loss: 829335032.1096\n",
      "Epoch 1214/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1022726485.0411 - val_loss: 825877927.8904\n",
      "Epoch 1215/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1012504833.7534 - val_loss: 827397467.6164\n",
      "Epoch 1216/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1019131080.3288 - val_loss: 822560796.0548\n",
      "Epoch 1217/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1035811278.0274 - val_loss: 828778734.2466\n",
      "Epoch 1218/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 1068214164.3836 - val_loss: 818516977.7534\n",
      "Epoch 1219/3350\n",
      "1168/1168 [==============================] - 0s 48us/step - loss: 1018712234.0822 - val_loss: 827845690.5205\n",
      "Epoch 1220/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 1022217148.9315 - val_loss: 831750662.1370\n",
      "Epoch 1221/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1018609348.3836 - val_loss: 843779605.9178\n",
      "Epoch 1222/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1015776378.7397 - val_loss: 837658613.6986\n",
      "Epoch 1223/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1038137087.1233 - val_loss: 840714940.9315\n",
      "Epoch 1224/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1035325486.0274 - val_loss: 826619059.2877\n",
      "Epoch 1225/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1006583059.7260 - val_loss: 827572849.9726\n",
      "Epoch 1226/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1004129272.5479 - val_loss: 837650335.1233\n",
      "Epoch 1227/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1066786096.2192 - val_loss: 811791548.9315\n",
      "Epoch 1228/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1007594009.2055 - val_loss: 836898534.1370\n",
      "Epoch 1229/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 1005842365.8082 - val_loss: 838935645.3699\n",
      "Epoch 1230/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1027768682.0822 - val_loss: 820312851.2877\n",
      "Epoch 1231/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1004221696.8767 - val_loss: 815118456.5479\n",
      "Epoch 1232/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1027469774.0274 - val_loss: 816794836.3836\n",
      "Epoch 1233/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 1024175283.7260 - val_loss: 819103534.4658\n",
      "Epoch 1234/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1007258269.8082 - val_loss: 823340932.8219\n",
      "Epoch 1235/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1029135445.9178 - val_loss: 818577459.7260\n",
      "Epoch 1236/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1024236678.1370 - val_loss: 807839210.9589\n",
      "Epoch 1237/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1002151992.5479 - val_loss: 834849941.4795\n",
      "Epoch 1238/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1016546078.2466 - val_loss: 808897273.8630\n",
      "Epoch 1239/3350\n",
      "1168/1168 [==============================] - 0s 88us/step - loss: 1007638956.2740 - val_loss: 834930128.2192\n",
      "Epoch 1240/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 1008710850.6301 - val_loss: 813805596.7123\n",
      "Epoch 1241/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 998192794.3014 - val_loss: 850642657.7534\n",
      "Epoch 1242/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1017685741.5890 - val_loss: 812623951.7808\n",
      "Epoch 1243/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 1014388205.1507 - val_loss: 832172156.4932\n",
      "Epoch 1244/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1001436552.1096 - val_loss: 807254525.3699\n",
      "Epoch 1245/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 994205846.7945 - val_loss: 827464902.1370\n",
      "Epoch 1246/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1000980138.0822 - val_loss: 856830243.5068\n",
      "Epoch 1247/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1014248105.6438 - val_loss: 806367509.4795\n",
      "Epoch 1248/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1008115547.1781 - val_loss: 810938284.4932\n",
      "Epoch 1249/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 995866772.1644 - val_loss: 871894531.5068\n",
      "Epoch 1250/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 1091107205.6986 - val_loss: 818151081.4247\n",
      "Epoch 1251/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1006651398.5753 - val_loss: 820095302.5753\n",
      "Epoch 1252/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1027032921.4247 - val_loss: 820059102.6849\n",
      "Epoch 1253/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1002024085.9178 - val_loss: 821300122.0822\n",
      "Epoch 1254/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1002870014.0274 - val_loss: 806502567.0137\n",
      "Epoch 1255/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 1011316350.2466 - val_loss: 812508746.7397\n",
      "Epoch 1256/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1007676693.0411 - val_loss: 820332347.1781\n",
      "Epoch 1257/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1038468087.6712 - val_loss: 843201604.3836\n",
      "Epoch 1258/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1031418556.4932 - val_loss: 821551141.0411\n",
      "Epoch 1259/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 1003161447.4521 - val_loss: 808986523.3973\n",
      "Epoch 1260/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 994266185.6438 - val_loss: 821025508.3836\n",
      "Epoch 1261/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 996947465.6438 - val_loss: 824425126.1370\n",
      "Epoch 1262/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 996261625.8630 - val_loss: 803965857.3151\n",
      "Epoch 1263/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 988726276.3836 - val_loss: 815097319.4521\n",
      "Epoch 1264/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1013449486.0274 - val_loss: 856814555.6164\n",
      "Epoch 1265/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 998177479.8904 - val_loss: 820751807.5616\n",
      "Epoch 1266/3350\n",
      "1168/1168 [==============================] - 0s 79us/step - loss: 980471485.8082 - val_loss: 811183655.0137\n",
      "Epoch 1267/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1010312080.6575 - val_loss: 798070825.2055\n",
      "Epoch 1268/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1000943072.4384 - val_loss: 872361847.6712\n",
      "Epoch 1269/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 996604285.8082 - val_loss: 819275783.0137\n",
      "Epoch 1270/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 989990780.0548 - val_loss: 805795310.9041\n",
      "Epoch 1271/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 1021677995.8356 - val_loss: 800673927.4521\n",
      "Epoch 1272/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 980568519.0137 - val_loss: 795136727.6712\n",
      "Epoch 1273/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 993568496.2192 - val_loss: 814441283.0685\n",
      "Epoch 1274/3350\n",
      "1168/1168 [==============================] - 0s 49us/step - loss: 984446154.9589 - val_loss: 823632318.6849\n",
      "Epoch 1275/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 978036254.6849 - val_loss: 815353967.1233\n",
      "Epoch 1276/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 984282916.8219 - val_loss: 806061831.6712\n",
      "Epoch 1277/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 985147505.0959 - val_loss: 803515517.3699\n",
      "Epoch 1278/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 988520417.7534 - val_loss: 792441679.3425\n",
      "Epoch 1279/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 1012202027.8356 - val_loss: 809883932.9315\n",
      "Epoch 1280/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1002362705.9726 - val_loss: 795698673.5342\n",
      "Epoch 1281/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 967782891.3973 - val_loss: 810946465.9726\n",
      "Epoch 1282/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 982816433.0959 - val_loss: 801218318.4658\n",
      "Epoch 1283/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 976943071.5616 - val_loss: 824865780.8219\n",
      "Epoch 1284/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 1034269621.4795 - val_loss: 870962242.6301\n",
      "Epoch 1285/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 996618040.1096 - val_loss: 803561163.8356\n",
      "Epoch 1286/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 977372967.8904 - val_loss: 805646739.5068\n",
      "Epoch 1287/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 974585708.2740 - val_loss: 802095912.7671\n",
      "Epoch 1288/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 971450657.7534 - val_loss: 798898177.7534\n",
      "Epoch 1289/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 979038567.8904 - val_loss: 795223526.3562\n",
      "Epoch 1290/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 969719550.2466 - val_loss: 835536353.7534\n",
      "Epoch 1291/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 980119213.5890 - val_loss: 847347468.7123\n",
      "Epoch 1292/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 977436666.7397 - val_loss: 831587377.9726\n",
      "Epoch 1293/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1005701304.9863 - val_loss: 831252877.1507\n",
      "Epoch 1294/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 995230641.5342 - val_loss: 811886410.9589\n",
      "Epoch 1295/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 969633371.6164 - val_loss: 805001477.2603\n",
      "Epoch 1296/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 1033654076.4932 - val_loss: 791831588.8219\n",
      "Epoch 1297/3350\n",
      "1168/1168 [==============================] - 0s 54us/step - loss: 981930197.0411 - val_loss: 803243011.5068\n",
      "Epoch 1298/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 976123118.0274 - val_loss: 789641762.6301\n",
      "Epoch 1299/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 970788651.3973 - val_loss: 835375580.9315\n",
      "Epoch 1300/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 1007284956.0548 - val_loss: 784513281.3151\n",
      "Epoch 1301/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 967527401.2055 - val_loss: 788887158.7945\n",
      "Epoch 1302/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 963884122.7397 - val_loss: 792990485.9178\n",
      "Epoch 1303/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 973752244.6027 - val_loss: 797895487.3425\n",
      "Epoch 1304/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 964585481.6438 - val_loss: 800499660.2740\n",
      "Epoch 1305/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 960719702.3562 - val_loss: 808391869.3699\n",
      "Epoch 1306/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 981258236.9315 - val_loss: 792870380.2740\n",
      "Epoch 1307/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 964398147.5068 - val_loss: 797666345.6438\n",
      "Epoch 1308/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 965924871.8904 - val_loss: 820180508.0548\n",
      "Epoch 1309/3350\n",
      "1168/1168 [==============================] - 0s 49us/step - loss: 974853650.8493 - val_loss: 780440934.5753\n",
      "Epoch 1310/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 969191326.2466 - val_loss: 789660641.7534\n",
      "Epoch 1311/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 982914218.9589 - val_loss: 814051257.8630\n",
      "Epoch 1312/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 988841972.6027 - val_loss: 819731994.5205\n",
      "Epoch 1313/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 978309745.9726 - val_loss: 784410005.0411\n",
      "Epoch 1314/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 953045411.9452 - val_loss: 811128208.4384\n",
      "Epoch 1315/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 966812298.5205 - val_loss: 791309935.3425\n",
      "Epoch 1316/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 988138530.1918 - val_loss: 790805832.7671\n",
      "Epoch 1317/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 963469047.2329 - val_loss: 783289770.9589\n",
      "Epoch 1318/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 969964752.2192 - val_loss: 775902581.0411\n",
      "Epoch 1319/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 965644165.6986 - val_loss: 782569011.2877\n",
      "Epoch 1320/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 962687487.1233 - val_loss: 787293550.4658\n",
      "Epoch 1321/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 976791106.6301 - val_loss: 795461336.9863\n",
      "Epoch 1322/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 958644382.2466 - val_loss: 787263456.4384\n",
      "Epoch 1323/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 957262223.3425 - val_loss: 795752701.1507\n",
      "Epoch 1324/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 955669014.3562 - val_loss: 784096785.9726\n",
      "Epoch 1325/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 971923740.0548 - val_loss: 787214746.9589\n",
      "Epoch 1326/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 938925133.1507 - val_loss: 839400507.1781\n",
      "Epoch 1327/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 956040217.4247 - val_loss: 783649737.6438\n",
      "Epoch 1328/3350\n",
      "1168/1168 [==============================] - 0s 81us/step - loss: 946702513.5342 - val_loss: 801690431.5616\n",
      "Epoch 1329/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 965941142.3562 - val_loss: 772774008.9863\n",
      "Epoch 1330/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 951711403.8356 - val_loss: 779396028.4932\n",
      "Epoch 1331/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 964329070.4658 - val_loss: 785009991.8904\n",
      "Epoch 1332/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 978056653.8082 - val_loss: 808479953.5342\n",
      "Epoch 1333/3350\n",
      "1168/1168 [==============================] - 0s 79us/step - loss: 970508581.2603 - val_loss: 778136367.3425\n",
      "Epoch 1334/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 957132235.3973 - val_loss: 783353034.0822\n",
      "Epoch 1335/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 946642502.1370 - val_loss: 769985210.7397\n",
      "Epoch 1336/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 952731414.7945 - val_loss: 783460490.9589\n",
      "Epoch 1337/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 948681440.0000 - val_loss: 816980223.1233\n",
      "Epoch 1338/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 967117772.2740 - val_loss: 791626857.2055\n",
      "Epoch 1339/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 960891375.7808 - val_loss: 781048184.1096\n",
      "Epoch 1340/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 944310218.5205 - val_loss: 777512727.6712\n",
      "Epoch 1341/3350\n",
      "1168/1168 [==============================] - 0s 80us/step - loss: 936320592.6575 - val_loss: 832255574.7945\n",
      "Epoch 1342/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 957478801.9726 - val_loss: 772141376.0000\n",
      "Epoch 1343/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 937431777.7534 - val_loss: 776407840.0000\n",
      "Epoch 1344/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 935845910.7945 - val_loss: 770939474.8493\n",
      "Epoch 1345/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 980143875.0685 - val_loss: 859765635.9452\n",
      "Epoch 1346/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 986976556.2740 - val_loss: 839880360.7671\n",
      "Epoch 1347/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 1014917371.1781 - val_loss: 796809555.7260\n",
      "Epoch 1348/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 950963349.4795 - val_loss: 794223167.7808\n",
      "Epoch 1349/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 955037984.8767 - val_loss: 805659729.9726\n",
      "Epoch 1350/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 937208961.7534 - val_loss: 813518563.9452\n",
      "Epoch 1351/3350\n",
      "1168/1168 [==============================] - 0s 88us/step - loss: 963800984.9863 - val_loss: 785676871.8904\n",
      "Epoch 1352/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 928112897.0959 - val_loss: 775616336.0000\n",
      "Epoch 1353/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 938365059.5068 - val_loss: 770484374.5753\n",
      "Epoch 1354/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 951359830.7945 - val_loss: 790025594.3014\n",
      "Epoch 1355/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 940033146.7397 - val_loss: 771710103.0137\n",
      "Epoch 1356/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 948947377.0959 - val_loss: 778509605.6986\n",
      "Epoch 1357/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 965358370.6301 - val_loss: 791550726.1370\n",
      "Epoch 1358/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 935621920.8767 - val_loss: 765787551.1233\n",
      "Epoch 1359/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 947703759.3425 - val_loss: 804616615.4521\n",
      "Epoch 1360/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 964706726.5753 - val_loss: 774385931.3973\n",
      "Epoch 1361/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 935851664.6575 - val_loss: 787624485.2603\n",
      "Epoch 1362/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 965076547.5068 - val_loss: 771776331.3973\n",
      "Epoch 1363/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 933198554.3014 - val_loss: 765797218.1918\n",
      "Epoch 1364/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 926327669.4795 - val_loss: 773478250.0822\n",
      "Epoch 1365/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 976582348.7123 - val_loss: 777075739.6164\n",
      "Epoch 1366/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 942283743.5616 - val_loss: 785894700.2740\n",
      "Epoch 1367/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 937009909.9178 - val_loss: 765590314.7397\n",
      "Epoch 1368/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 945445174.3562 - val_loss: 802766096.6575\n",
      "Epoch 1369/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 959707134.2466 - val_loss: 888302610.4110\n",
      "Epoch 1370/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 973345362.4110 - val_loss: 779333569.3151\n",
      "Epoch 1371/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 941357378.6301 - val_loss: 767634579.7260\n",
      "Epoch 1372/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 933440811.8356 - val_loss: 777260958.6849\n",
      "Epoch 1373/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 929469099.3973 - val_loss: 771817610.0822\n",
      "Epoch 1374/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 944400596.1644 - val_loss: 789165731.9452\n",
      "Epoch 1375/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 924195210.9589 - val_loss: 778860733.3699\n",
      "Epoch 1376/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 945790568.3288 - val_loss: 761608949.2603\n",
      "Epoch 1377/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 919870540.2740 - val_loss: 773706801.0959\n",
      "Epoch 1378/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 957622865.0959 - val_loss: 798426754.6301\n",
      "Epoch 1379/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 940277862.5753 - val_loss: 787165612.9315\n",
      "Epoch 1380/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 933363050.0822 - val_loss: 757342499.0685\n",
      "Epoch 1381/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 928256363.8356 - val_loss: 758303395.2877\n",
      "Epoch 1382/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 936474091.8356 - val_loss: 755278382.4658\n",
      "Epoch 1383/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 914475070.2466 - val_loss: 752582254.0274\n",
      "Epoch 1384/3350\n",
      "1168/1168 [==============================] - 0s 52us/step - loss: 929932432.6575 - val_loss: 799985749.9178\n",
      "Epoch 1385/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 941145340.0548 - val_loss: 758886021.2603\n",
      "Epoch 1386/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 921582856.7671 - val_loss: 772191308.7123\n",
      "Epoch 1387/3350\n",
      "1168/1168 [==============================] - 0s 80us/step - loss: 939174458.7397 - val_loss: 763845641.4247\n",
      "Epoch 1388/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 955521175.2329 - val_loss: 791206567.6712\n",
      "Epoch 1389/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 911543942.5753 - val_loss: 757235237.2603\n",
      "Epoch 1390/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 949812982.3562 - val_loss: 826626336.8767\n",
      "Epoch 1391/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 1028273623.2329 - val_loss: 752252189.3699\n",
      "Epoch 1392/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 946204087.2329 - val_loss: 829482827.8356\n",
      "Epoch 1393/3350\n",
      "1168/1168 [==============================] - 0s 80us/step - loss: 938949001.6438 - val_loss: 779530503.4521\n",
      "Epoch 1394/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 924163080.3288 - val_loss: 753193557.9178\n",
      "Epoch 1395/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 913103283.2877 - val_loss: 785748157.3699\n",
      "Epoch 1396/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 909323040.0000 - val_loss: 820751300.3836\n",
      "Epoch 1397/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 914689210.7397 - val_loss: 755762692.3836\n",
      "Epoch 1398/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 927370084.8219 - val_loss: 787009951.5616\n",
      "Epoch 1399/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 931655325.8082 - val_loss: 761986022.3562\n",
      "Epoch 1400/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 951109245.3699 - val_loss: 829484911.7808\n",
      "Epoch 1401/3350\n",
      "1168/1168 [==============================] - 0s 79us/step - loss: 919633444.8219 - val_loss: 778865775.7808\n",
      "Epoch 1402/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 911322520.9863 - val_loss: 782412142.4658\n",
      "Epoch 1403/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 916260257.3151 - val_loss: 762827699.2877\n",
      "Epoch 1404/3350\n",
      "1168/1168 [==============================] - 0s 79us/step - loss: 933716476.9315 - val_loss: 760221595.3973\n",
      "Epoch 1405/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 909090782.4658 - val_loss: 788637275.1781\n",
      "Epoch 1406/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 925119710.2466 - val_loss: 755868854.3562\n",
      "Epoch 1407/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 905146488.1096 - val_loss: 770262899.5068\n",
      "Epoch 1408/3350\n",
      "1168/1168 [==============================] - 0s 84us/step - loss: 937404671.1233 - val_loss: 751652444.0548\n",
      "Epoch 1409/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 917279598.0274 - val_loss: 822097415.4521\n",
      "Epoch 1410/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 937721935.3425 - val_loss: 755664085.6986\n",
      "Epoch 1411/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 914906347.8356 - val_loss: 750127122.6301\n",
      "Epoch 1412/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 901360721.5342 - val_loss: 758253945.4247\n",
      "Epoch 1413/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 907825187.0685 - val_loss: 750036247.0137\n",
      "Epoch 1414/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 899659256.1096 - val_loss: 755136603.8356\n",
      "Epoch 1415/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 907954790.1370 - val_loss: 749992876.2740\n",
      "Epoch 1416/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 915150771.2877 - val_loss: 764516093.8082\n",
      "Epoch 1417/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 905400704.4384 - val_loss: 745277223.8904\n",
      "Epoch 1418/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 911629458.8493 - val_loss: 756783238.5753\n",
      "Epoch 1419/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 911831484.9315 - val_loss: 764657532.0548\n",
      "Epoch 1420/3350\n",
      "1168/1168 [==============================] - 0s 49us/step - loss: 904502172.0548 - val_loss: 778134025.6438\n",
      "Epoch 1421/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 912224573.8082 - val_loss: 773532970.5205\n",
      "Epoch 1422/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 945981605.6986 - val_loss: 754021248.2192\n",
      "Epoch 1423/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 905829089.3151 - val_loss: 748574826.0822\n",
      "Epoch 1424/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 918535378.4110 - val_loss: 746105276.4932\n",
      "Epoch 1425/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 898789291.8356 - val_loss: 771250982.5753\n",
      "Epoch 1426/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 914224071.8904 - val_loss: 748054905.6438\n",
      "Epoch 1427/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 908789308.4932 - val_loss: 763013683.2877\n",
      "Epoch 1428/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 910371117.1507 - val_loss: 745301312.6575\n",
      "Epoch 1429/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 934777411.5068 - val_loss: 757885106.4110\n",
      "Epoch 1430/3350\n",
      "1168/1168 [==============================] - 0s 53us/step - loss: 899312164.8219 - val_loss: 753897560.9863\n",
      "Epoch 1431/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 896209778.8493 - val_loss: 747360405.4795\n",
      "Epoch 1432/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 892025674.5205 - val_loss: 777374043.3973\n",
      "Epoch 1433/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 936197427.7260 - val_loss: 744870999.2329\n",
      "Epoch 1434/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 898484383.3425 - val_loss: 841568970.5205\n",
      "Epoch 1435/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 949320850.8493 - val_loss: 751333719.6712\n",
      "Epoch 1436/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 882399473.0959 - val_loss: 773937536.4384\n",
      "Epoch 1437/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 900202584.1096 - val_loss: 745781076.3836\n",
      "Epoch 1438/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 889278952.3288 - val_loss: 828428464.6575\n",
      "Epoch 1439/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 922697151.5616 - val_loss: 778735860.1644\n",
      "Epoch 1440/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 895141205.9178 - val_loss: 751700306.8493\n",
      "Epoch 1441/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 882728366.9041 - val_loss: 744809087.7808\n",
      "Epoch 1442/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 894634224.2192 - val_loss: 802685016.5479\n",
      "Epoch 1443/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 904396591.7808 - val_loss: 780657803.8356\n",
      "Epoch 1444/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 884623732.6027 - val_loss: 759565947.6164\n",
      "Epoch 1445/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 907435207.4521 - val_loss: 753799660.7123\n",
      "Epoch 1446/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 907001046.7945 - val_loss: 752696808.7671\n",
      "Epoch 1447/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 880818065.5342 - val_loss: 743324049.9726\n",
      "Epoch 1448/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 882789682.4110 - val_loss: 744048556.7123\n",
      "Epoch 1449/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 886999476.6027 - val_loss: 741101654.1370\n",
      "Epoch 1450/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 888850415.3425 - val_loss: 800464625.0959\n",
      "Epoch 1451/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 899404365.5890 - val_loss: 736291436.4932\n",
      "Epoch 1452/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 882537108.6027 - val_loss: 777915880.3288\n",
      "Epoch 1453/3350\n",
      "1168/1168 [==============================] - 0s 48us/step - loss: 877577561.4247 - val_loss: 737171103.1233\n",
      "Epoch 1454/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 877020291.5068 - val_loss: 751214534.5753\n",
      "Epoch 1455/3350\n",
      "1168/1168 [==============================] - 0s 49us/step - loss: 877418821.2603 - val_loss: 765578986.9589\n",
      "Epoch 1456/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 910341431.6712 - val_loss: 746472174.0274\n",
      "Epoch 1457/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 885801933.5890 - val_loss: 733948737.5342\n",
      "Epoch 1458/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 873434977.7534 - val_loss: 761481608.3288\n",
      "Epoch 1459/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 964747647.1233 - val_loss: 735907010.1918\n",
      "Epoch 1460/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 876868629.4795 - val_loss: 747590219.8356\n",
      "Epoch 1461/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 883379426.1918 - val_loss: 755237628.0548\n",
      "Epoch 1462/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 922215446.7945 - val_loss: 740870506.9589\n",
      "Epoch 1463/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 886691266.6301 - val_loss: 762507864.1096\n",
      "Epoch 1464/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 893268325.2603 - val_loss: 785744644.3836\n",
      "Epoch 1465/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 884346549.0411 - val_loss: 748562624.8767\n",
      "Epoch 1466/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 874816151.6712 - val_loss: 739337033.6438\n",
      "Epoch 1467/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 881904573.3699 - val_loss: 805538961.9726\n",
      "Epoch 1468/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 910133580.7123 - val_loss: 737081696.8767\n",
      "Epoch 1469/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 885898345.2055 - val_loss: 739590577.7534\n",
      "Epoch 1470/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 889523555.5068 - val_loss: 744817677.5890\n",
      "Epoch 1471/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 871625488.2192 - val_loss: 804072770.6301\n",
      "Epoch 1472/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 889604408.1096 - val_loss: 729998828.2740\n",
      "Epoch 1473/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 880492867.9452 - val_loss: 798461450.5205\n",
      "Epoch 1474/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 868119538.8493 - val_loss: 747812888.5479\n",
      "Epoch 1475/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 879747068.9315 - val_loss: 749099865.8630\n",
      "Epoch 1476/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 880366460.0548 - val_loss: 798093652.1644\n",
      "Epoch 1477/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 901815936.8767 - val_loss: 801332183.2329\n",
      "Epoch 1478/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 905418782.9041 - val_loss: 743867640.1096\n",
      "Epoch 1479/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 863219441.0959 - val_loss: 735868281.8630\n",
      "Epoch 1480/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 871850286.0274 - val_loss: 737146896.4384\n",
      "Epoch 1481/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 872805196.7123 - val_loss: 754764196.8219\n",
      "Epoch 1482/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 891369892.3836 - val_loss: 729634350.6849\n",
      "Epoch 1483/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 881689093.6986 - val_loss: 739229470.9041\n",
      "Epoch 1484/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 875244650.5205 - val_loss: 732062518.7945\n",
      "Epoch 1485/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 869983136.4384 - val_loss: 737184656.6575\n",
      "Epoch 1486/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 872257864.7671 - val_loss: 751214713.6438\n",
      "Epoch 1487/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 877102172.0548 - val_loss: 732043548.0548\n",
      "Epoch 1488/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 870500296.7671 - val_loss: 748780050.6301\n",
      "Epoch 1489/3350\n",
      "1168/1168 [==============================] - 0s 47us/step - loss: 881187318.3562 - val_loss: 728698219.3973\n",
      "Epoch 1490/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 864159325.3699 - val_loss: 731834472.7671\n",
      "Epoch 1491/3350\n",
      "1168/1168 [==============================] - 0s 50us/step - loss: 862821319.4521 - val_loss: 730714810.3014\n",
      "Epoch 1492/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 866397778.8493 - val_loss: 741970825.8630\n",
      "Epoch 1493/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 864828489.6438 - val_loss: 736718918.3562\n",
      "Epoch 1494/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 870289255.4521 - val_loss: 725601441.3151\n",
      "Epoch 1495/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 860177590.3562 - val_loss: 734334524.9315\n",
      "Epoch 1496/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 884938170.3014 - val_loss: 752480240.2192\n",
      "Epoch 1497/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 855026944.4384 - val_loss: 730384501.0411\n",
      "Epoch 1498/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 885889339.1781 - val_loss: 723123942.1370\n",
      "Epoch 1499/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 853746799.1233 - val_loss: 723001922.1918\n",
      "Epoch 1500/3350\n",
      "1168/1168 [==============================] - 0s 53us/step - loss: 857880504.1096 - val_loss: 769302040.3288\n",
      "Epoch 1501/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 861032942.4658 - val_loss: 718286964.1644\n",
      "Epoch 1502/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 860072225.3151 - val_loss: 791948965.6986\n",
      "Epoch 1503/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 874690290.8493 - val_loss: 746815044.3836\n",
      "Epoch 1504/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 874646684.0548 - val_loss: 735785964.4932\n",
      "Epoch 1505/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 859674166.3562 - val_loss: 749370393.2055\n",
      "Epoch 1506/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 867086147.0685 - val_loss: 748516306.4110\n",
      "Epoch 1507/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 867045091.9452 - val_loss: 757775991.2329\n",
      "Epoch 1508/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 865871162.7397 - val_loss: 759972270.4658\n",
      "Epoch 1509/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 854507658.9589 - val_loss: 719098626.4110\n",
      "Epoch 1510/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 851000085.0411 - val_loss: 724374848.2192\n",
      "Epoch 1511/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 852328698.7397 - val_loss: 779412931.0685\n",
      "Epoch 1512/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 874815068.0548 - val_loss: 719004836.3836\n",
      "Epoch 1513/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 852906101.0411 - val_loss: 724535761.9726\n",
      "Epoch 1514/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 858311580.4932 - val_loss: 742719391.5616\n",
      "Epoch 1515/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 858069141.2603 - val_loss: 722133528.5479\n",
      "Epoch 1516/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 861082669.1507 - val_loss: 753952368.6575\n",
      "Epoch 1517/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 840715820.7123 - val_loss: 716695718.1370\n",
      "Epoch 1518/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 893281714.6301 - val_loss: 726059360.8767\n",
      "Epoch 1519/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 863753587.7260 - val_loss: 721495120.8767\n",
      "Epoch 1520/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 884869991.4521 - val_loss: 823946151.4521\n",
      "Epoch 1521/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 905380963.9452 - val_loss: 727776220.9315\n",
      "Epoch 1522/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 846050690.6301 - val_loss: 732301477.4795\n",
      "Epoch 1523/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 850222985.2055 - val_loss: 718887772.0548\n",
      "Epoch 1524/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 867099363.9452 - val_loss: 728316470.3562\n",
      "Epoch 1525/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 850016024.5479 - val_loss: 738397145.8630\n",
      "Epoch 1526/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 854521172.6027 - val_loss: 725149423.7808\n",
      "Epoch 1527/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 851405773.1507 - val_loss: 746526096.8767\n",
      "Epoch 1528/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 839093061.2603 - val_loss: 751506445.5890\n",
      "Epoch 1529/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 842989605.6986 - val_loss: 728202696.3288\n",
      "Epoch 1530/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 832310132.8219 - val_loss: 745050022.5753\n",
      "Epoch 1531/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 833271398.1370 - val_loss: 717776629.0411\n",
      "Epoch 1532/3350\n",
      "1168/1168 [==============================] - 0s 83us/step - loss: 837421378.6301 - val_loss: 741516393.2055\n",
      "Epoch 1533/3350\n",
      "1168/1168 [==============================] - 0s 48us/step - loss: 872531262.6849 - val_loss: 727951487.5616\n",
      "Epoch 1534/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 864342949.2603 - val_loss: 727190794.5205\n",
      "Epoch 1535/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 842424290.6301 - val_loss: 720963717.6986\n",
      "Epoch 1536/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 843200041.6438 - val_loss: 721547002.9589\n",
      "Epoch 1537/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 861321854.6849 - val_loss: 802496340.3836\n",
      "Epoch 1538/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 893454464.0000 - val_loss: 717638894.2466\n",
      "Epoch 1539/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 839016487.8904 - val_loss: 720138801.5342\n",
      "Epoch 1540/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 844559568.6575 - val_loss: 712300649.2055\n",
      "Epoch 1541/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 853019492.8219 - val_loss: 719348982.7945\n",
      "Epoch 1542/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 826696606.2466 - val_loss: 724483924.6027\n",
      "Epoch 1543/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 843388454.1370 - val_loss: 727022142.0274\n",
      "Epoch 1544/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 830209850.7397 - val_loss: 714485016.7671\n",
      "Epoch 1545/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 828511999.5616 - val_loss: 718236181.4795\n",
      "Epoch 1546/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 829533492.1644 - val_loss: 737405671.4521\n",
      "Epoch 1547/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 840097767.0137 - val_loss: 709954685.5890\n",
      "Epoch 1548/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 822024751.3425 - val_loss: 710056796.9315\n",
      "Epoch 1549/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 912284994.1918 - val_loss: 701473479.0137\n",
      "Epoch 1550/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 836765159.4521 - val_loss: 748314869.0411\n",
      "Epoch 1551/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 837650077.8082 - val_loss: 749602504.9863\n",
      "Epoch 1552/3350\n",
      "1168/1168 [==============================] - 0s 91us/step - loss: 830464132.3836 - val_loss: 722152543.3425\n",
      "Epoch 1553/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 816088303.7808 - val_loss: 709149529.2055\n",
      "Epoch 1554/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 822725614.0274 - val_loss: 712066740.8219\n",
      "Epoch 1555/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 822253669.6986 - val_loss: 717279194.7397\n",
      "Epoch 1556/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 820559068.0548 - val_loss: 718870463.5616\n",
      "Epoch 1557/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 832319722.0822 - val_loss: 718285155.5068\n",
      "Epoch 1558/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 822972804.3836 - val_loss: 707130584.1096\n",
      "Epoch 1559/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 828079882.5205 - val_loss: 746445931.3973\n",
      "Epoch 1560/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 842593011.7260 - val_loss: 763260453.6986\n",
      "Epoch 1561/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 847959146.0822 - val_loss: 722816179.5068\n",
      "Epoch 1562/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 819040069.2603 - val_loss: 719244527.7808\n",
      "Epoch 1563/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 830235455.1233 - val_loss: 741971224.7671\n",
      "Epoch 1564/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 862344346.3014 - val_loss: 733638306.6301\n",
      "Epoch 1565/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 883112355.9452 - val_loss: 718909982.9041\n",
      "Epoch 1566/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 824738349.5890 - val_loss: 715026532.8219\n",
      "Epoch 1567/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 837416437.4795 - val_loss: 706638859.6164\n",
      "Epoch 1568/3350\n",
      "1168/1168 [==============================] - 0s 48us/step - loss: 816193386.0822 - val_loss: 722889381.2603\n",
      "Epoch 1569/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 804477222.0274 - val_loss: 752296830.6849\n",
      "Epoch 1570/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 836874240.4384 - val_loss: 700541429.6986\n",
      "Epoch 1571/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 811491114.3014 - val_loss: 716005008.0000\n",
      "Epoch 1572/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 855356238.4658 - val_loss: 701131673.8630\n",
      "Epoch 1573/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 840049936.6575 - val_loss: 772255986.8493\n",
      "Epoch 1574/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 823095424.0000 - val_loss: 719778399.5616\n",
      "Epoch 1575/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 809509188.3836 - val_loss: 709239931.1781\n",
      "Epoch 1576/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 823860111.3425 - val_loss: 698454958.9041\n",
      "Epoch 1577/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 830773548.2740 - val_loss: 712076649.2055\n",
      "Epoch 1578/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 809513571.9452 - val_loss: 702453870.9041\n",
      "Epoch 1579/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 826761265.0959 - val_loss: 703597249.9726\n",
      "Epoch 1580/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 811540652.7123 - val_loss: 706253493.9178\n",
      "Epoch 1581/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 807629246.6849 - val_loss: 697088295.2329\n",
      "Epoch 1582/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 813186771.2877 - val_loss: 707988104.5479\n",
      "Epoch 1583/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 825212463.3425 - val_loss: 694989778.8493\n",
      "Epoch 1584/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 811762064.2192 - val_loss: 699478827.1781\n",
      "Epoch 1585/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 815110865.9726 - val_loss: 705205273.6438\n",
      "Epoch 1586/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 812255274.3014 - val_loss: 709691697.5342\n",
      "Epoch 1587/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 804108269.5890 - val_loss: 708506905.4247\n",
      "Epoch 1588/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 807696533.0411 - val_loss: 691429525.0411\n",
      "Epoch 1589/3350\n",
      "1168/1168 [==============================] - 0s 81us/step - loss: 836454995.7260 - val_loss: 690730049.5342\n",
      "Epoch 1590/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 823646052.3836 - val_loss: 791500411.6164\n",
      "Epoch 1591/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 819315120.2192 - val_loss: 724030186.0822\n",
      "Epoch 1592/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 820065415.0137 - val_loss: 709427550.0274\n",
      "Epoch 1593/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 853660459.8356 - val_loss: 700077358.4658\n",
      "Epoch 1594/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 809280989.3699 - val_loss: 694517060.8219\n",
      "Epoch 1595/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 833674134.7945 - val_loss: 721441230.2466\n",
      "Epoch 1596/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 808299771.6164 - val_loss: 697241777.0959\n",
      "Epoch 1597/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 810839490.6301 - val_loss: 693005517.1507\n",
      "Epoch 1598/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 798240527.7808 - val_loss: 695909994.0822\n",
      "Epoch 1599/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 793254172.9315 - val_loss: 696990213.4795\n",
      "Epoch 1600/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 813223306.0822 - val_loss: 695942885.4795\n",
      "Epoch 1601/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 827812830.6849 - val_loss: 693771254.7945\n",
      "Epoch 1602/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 796218857.6438 - val_loss: 695970570.3014\n",
      "Epoch 1603/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 787360826.7397 - val_loss: 690379920.6575\n",
      "Epoch 1604/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 803978162.8493 - val_loss: 726495238.5753\n",
      "Epoch 1605/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 814698408.7671 - val_loss: 684617195.3973\n",
      "Epoch 1606/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 821017934.0274 - val_loss: 821098376.3288\n",
      "Epoch 1607/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 809014796.7123 - val_loss: 767172404.6027\n",
      "Epoch 1608/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 814336992.8767 - val_loss: 686761697.5342\n",
      "Epoch 1609/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 798109549.5890 - val_loss: 698166335.3425\n",
      "Epoch 1610/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 797474443.3973 - val_loss: 753634086.5753\n",
      "Epoch 1611/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 835519201.3151 - val_loss: 698094264.5479\n",
      "Epoch 1612/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 827154244.3836 - val_loss: 679048025.8630\n",
      "Epoch 1613/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 809538430.2466 - val_loss: 702011407.5616\n",
      "Epoch 1614/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 797905036.2740 - val_loss: 700400852.6027\n",
      "Epoch 1615/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 797035923.7260 - val_loss: 749201840.2192\n",
      "Epoch 1616/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 808154064.6575 - val_loss: 712572007.6712\n",
      "Epoch 1617/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 784956512.0000 - val_loss: 714678297.6438\n",
      "Epoch 1618/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 798177076.1644 - val_loss: 684762348.0548\n",
      "Epoch 1619/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 801959939.0685 - val_loss: 686536828.4932\n",
      "Epoch 1620/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 827223050.5205 - val_loss: 684508481.9726\n",
      "Epoch 1621/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 793566461.8082 - val_loss: 684363470.9041\n",
      "Epoch 1622/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 811632495.3425 - val_loss: 677309129.2055\n",
      "Epoch 1623/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 819306734.9041 - val_loss: 685199827.2877\n",
      "Epoch 1624/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 783008563.7260 - val_loss: 680222190.9041\n",
      "Epoch 1625/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 794772826.3014 - val_loss: 697122164.1644\n",
      "Epoch 1626/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 784807824.6575 - val_loss: 679606238.4658\n",
      "Epoch 1627/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 773538110.6849 - val_loss: 687052598.3562\n",
      "Epoch 1628/3350\n",
      "1168/1168 [==============================] - 0s 49us/step - loss: 797879903.5616 - val_loss: 699862863.3425\n",
      "Epoch 1629/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 802574579.2877 - val_loss: 670340762.7397\n",
      "Epoch 1630/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 784058806.7945 - val_loss: 676745348.6027\n",
      "Epoch 1631/3350\n",
      "1168/1168 [==============================] - 0s 56us/step - loss: 798943646.2466 - val_loss: 694886805.2603\n",
      "Epoch 1632/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 773856896.4384 - val_loss: 703643942.7945\n",
      "Epoch 1633/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 776779875.9452 - val_loss: 696144475.1781\n",
      "Epoch 1634/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 784725289.2055 - val_loss: 675540244.8219\n",
      "Epoch 1635/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 780205665.3151 - val_loss: 672013493.9178\n",
      "Epoch 1636/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 784171530.0822 - val_loss: 683565784.1096\n",
      "Epoch 1637/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 776976593.9726 - val_loss: 677488405.4795\n",
      "Epoch 1638/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 808314987.8356 - val_loss: 690251303.2329\n",
      "Epoch 1639/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 775940824.9863 - val_loss: 710537279.1233\n",
      "Epoch 1640/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 784663530.5205 - val_loss: 692967076.6027\n",
      "Epoch 1641/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 778933622.7945 - val_loss: 661167962.3014\n",
      "Epoch 1642/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 773571102.6849 - val_loss: 687698096.0000\n",
      "Epoch 1643/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 770149998.9041 - val_loss: 718887000.1096\n",
      "Epoch 1644/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 785662176.0000 - val_loss: 677525897.6438\n",
      "Epoch 1645/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 775252117.4795 - val_loss: 658706231.2329\n",
      "Epoch 1646/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 753875000.9863 - val_loss: 686215594.9589\n",
      "Epoch 1647/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 789359213.1507 - val_loss: 674835588.1644\n",
      "Epoch 1648/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 787107273.6438 - val_loss: 667592128.0000\n",
      "Epoch 1649/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 793451872.0000 - val_loss: 709456644.8219\n",
      "Epoch 1650/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 777791177.2055 - val_loss: 690842712.1096\n",
      "Epoch 1651/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 793802573.1507 - val_loss: 682657158.1370\n",
      "Epoch 1652/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 790425071.3425 - val_loss: 689528188.4932\n",
      "Epoch 1653/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 770235657.4247 - val_loss: 676567938.8493\n",
      "Epoch 1654/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 763810245.9178 - val_loss: 670469404.9315\n",
      "Epoch 1655/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 772457528.1096 - val_loss: 666696992.0000\n",
      "Epoch 1656/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 918300030.6849 - val_loss: 686471755.3973\n",
      "Epoch 1657/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 766763343.7808 - val_loss: 717982471.0137\n",
      "Epoch 1658/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 797613451.8356 - val_loss: 716437465.6438\n",
      "Epoch 1659/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 776879508.6027 - val_loss: 660314693.6986\n",
      "Epoch 1660/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 759732733.3699 - val_loss: 674455966.0274\n",
      "Epoch 1661/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 776945501.3699 - val_loss: 715927035.1781\n",
      "Epoch 1662/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 773930893.5890 - val_loss: 659970980.6027\n",
      "Epoch 1663/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 783739431.0137 - val_loss: 659811686.1370\n",
      "Epoch 1664/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 767298620.0548 - val_loss: 683852608.2192\n",
      "Epoch 1665/3350\n",
      "1168/1168 [==============================] - 0s 89us/step - loss: 768968705.7534 - val_loss: 671388981.4795\n",
      "Epoch 1666/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 809473587.7260 - val_loss: 723574016.8767\n",
      "Epoch 1667/3350\n",
      "1168/1168 [==============================] - 0s 81us/step - loss: 772639142.1370 - val_loss: 666572533.6986\n",
      "Epoch 1668/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 745697630.2466 - val_loss: 680811200.4384\n",
      "Epoch 1669/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 756801977.8630 - val_loss: 664218464.8767\n",
      "Epoch 1670/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 770477361.0959 - val_loss: 695192035.5068\n",
      "Epoch 1671/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 753337776.2192 - val_loss: 722594062.0274\n",
      "Epoch 1672/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 768135679.1233 - val_loss: 655996163.9452\n",
      "Epoch 1673/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 786845739.8356 - val_loss: 690365266.8493\n",
      "Epoch 1674/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 787033213.8082 - val_loss: 658734541.8082\n",
      "Epoch 1675/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 747272589.1507 - val_loss: 674602866.8493\n",
      "Epoch 1676/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 743460167.4521 - val_loss: 671678513.7534\n",
      "Epoch 1677/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 740276667.6164 - val_loss: 682956485.2603\n",
      "Epoch 1678/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 763146040.9863 - val_loss: 673263587.7260\n",
      "Epoch 1679/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 750631995.6164 - val_loss: 660363744.8767\n",
      "Epoch 1680/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 742318863.7808 - val_loss: 754964087.6712\n",
      "Epoch 1681/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 754744601.4247 - val_loss: 641931079.8904\n",
      "Epoch 1682/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 738112949.4795 - val_loss: 670419287.8904\n",
      "Epoch 1683/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 770004250.3014 - val_loss: 655695505.0959\n",
      "Epoch 1684/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 754044515.9452 - val_loss: 666369761.7534\n",
      "Epoch 1685/3350\n",
      "1168/1168 [==============================] - 0s 47us/step - loss: 738407356.0548 - val_loss: 648487454.2466\n",
      "Epoch 1686/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 736039733.9178 - val_loss: 656605224.7671\n",
      "Epoch 1687/3350\n",
      "1168/1168 [==============================] - 0s 81us/step - loss: 747354018.1918 - val_loss: 665056043.8356\n",
      "Epoch 1688/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 770408138.0822 - val_loss: 685113918.0274\n",
      "Epoch 1689/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 737090763.3973 - val_loss: 655465742.2466\n",
      "Epoch 1690/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 737748812.7123 - val_loss: 664140427.8356\n",
      "Epoch 1691/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 741679083.6164 - val_loss: 653598036.3836\n",
      "Epoch 1692/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 742614999.6712 - val_loss: 651899314.4110\n",
      "Epoch 1693/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 748506808.5479 - val_loss: 652726394.7397\n",
      "Epoch 1694/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 777548412.4932 - val_loss: 670797343.7808\n",
      "Epoch 1695/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 741713518.0274 - val_loss: 654461524.6027\n",
      "Epoch 1696/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 768419711.1233 - val_loss: 661089088.2192\n",
      "Epoch 1697/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 750933989.6986 - val_loss: 657722759.4521\n",
      "Epoch 1698/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 739365324.7123 - val_loss: 664188064.2192\n",
      "Epoch 1699/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 734557373.8082 - val_loss: 659094804.1644\n",
      "Epoch 1700/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 728548372.6027 - val_loss: 664014497.3151\n",
      "Epoch 1701/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 729566939.1781 - val_loss: 651903674.7397\n",
      "Epoch 1702/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 733020879.7808 - val_loss: 658199965.3699\n",
      "Epoch 1703/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 726537196.7123 - val_loss: 653715866.9589\n",
      "Epoch 1704/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 735750200.1096 - val_loss: 665833456.2192\n",
      "Epoch 1705/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 753203052.7123 - val_loss: 648647767.2329\n",
      "Epoch 1706/3350\n",
      "1168/1168 [==============================] - 0s 81us/step - loss: 747468464.2192 - val_loss: 634607570.6301\n",
      "Epoch 1707/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 729021041.5342 - val_loss: 647032953.2055\n",
      "Epoch 1708/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 758245880.1096 - val_loss: 652507506.1918\n",
      "Epoch 1709/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 739027943.4521 - val_loss: 654718325.2603\n",
      "Epoch 1710/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 747168464.6575 - val_loss: 651249377.9726\n",
      "Epoch 1711/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 784511647.1233 - val_loss: 680991940.8219\n",
      "Epoch 1712/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 752479024.6575 - val_loss: 660386113.5342\n",
      "Epoch 1713/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 723817052.9315 - val_loss: 678523358.9041\n",
      "Epoch 1714/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 724731655.4521 - val_loss: 648053388.7123\n",
      "Epoch 1715/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 819342120.3288 - val_loss: 709836426.0822\n",
      "Epoch 1716/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 777740715.8356 - val_loss: 654726254.0274\n",
      "Epoch 1717/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 728098318.4658 - val_loss: 656730143.1233\n",
      "Epoch 1718/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 721291470.9041 - val_loss: 651550574.4658\n",
      "Epoch 1719/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 743708783.3425 - val_loss: 642223770.5205\n",
      "Epoch 1720/3350\n",
      "1168/1168 [==============================] - 0s 52us/step - loss: 736065176.1096 - val_loss: 641559263.3425\n",
      "Epoch 1721/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 798102119.4521 - val_loss: 672783374.0274\n",
      "Epoch 1722/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 733863594.0822 - val_loss: 659401229.1507\n",
      "Epoch 1723/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 719740168.7671 - val_loss: 651949270.7945\n",
      "Epoch 1724/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 725800721.9726 - val_loss: 642634138.7397\n",
      "Epoch 1725/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 724531677.3699 - val_loss: 644802323.7260\n",
      "Epoch 1726/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 720224626.8493 - val_loss: 645943179.3973\n",
      "Epoch 1727/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 718300880.2192 - val_loss: 694999249.7534\n",
      "Epoch 1728/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 744694859.8356 - val_loss: 659977330.4110\n",
      "Epoch 1729/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 732483620.8219 - val_loss: 640881530.0822\n",
      "Epoch 1730/3350\n",
      "1168/1168 [==============================] - 0s 47us/step - loss: 714694673.3151 - val_loss: 647836904.1096\n",
      "Epoch 1731/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 719826339.0685 - val_loss: 699451605.0411\n",
      "Epoch 1732/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 719617207.2329 - val_loss: 649086381.8082\n",
      "Epoch 1733/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 718310538.0822 - val_loss: 644328643.0685\n",
      "Epoch 1734/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 714094075.1781 - val_loss: 658662108.7123\n",
      "Epoch 1735/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 702085813.9178 - val_loss: 647776612.6027\n",
      "Epoch 1736/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 724474384.2192 - val_loss: 668201613.5890\n",
      "Epoch 1737/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 716017357.1507 - val_loss: 654636639.1233\n",
      "Epoch 1738/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 706809057.7534 - val_loss: 713847413.0411\n",
      "Epoch 1739/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 721049036.2740 - val_loss: 651299961.4247\n",
      "Epoch 1740/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 729852142.4658 - val_loss: 648908399.3425\n",
      "Epoch 1741/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 712167630.9041 - val_loss: 687176163.0685\n",
      "Epoch 1742/3350\n",
      "1168/1168 [==============================] - 0s 53us/step - loss: 732156228.8219 - val_loss: 639018140.3836\n",
      "Epoch 1743/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 705105429.9178 - val_loss: 643830801.3151\n",
      "Epoch 1744/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 708725999.3425 - val_loss: 658796793.4247\n",
      "Epoch 1745/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 710970641.9726 - val_loss: 646031112.5479\n",
      "Epoch 1746/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 720164313.8630 - val_loss: 659908591.7808\n",
      "Epoch 1747/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 716166231.6712 - val_loss: 647907569.9726\n",
      "Epoch 1748/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 725778665.2055 - val_loss: 681920498.6301\n",
      "Epoch 1749/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 724925842.4110 - val_loss: 671450611.5068\n",
      "Epoch 1750/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 695833605.6986 - val_loss: 666546243.0685\n",
      "Epoch 1751/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 718955904.4384 - val_loss: 691679371.8356\n",
      "Epoch 1752/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 739450100.1644 - val_loss: 693952377.8630\n",
      "Epoch 1753/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 712457011.7260 - val_loss: 639218594.6301\n",
      "Epoch 1754/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 697890293.9178 - val_loss: 640611137.5342\n",
      "Epoch 1755/3350\n",
      "1168/1168 [==============================] - 0s 49us/step - loss: 706778525.8082 - val_loss: 660986280.3288\n",
      "Epoch 1756/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 708353315.0685 - val_loss: 649855354.7397\n",
      "Epoch 1757/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 714661120.8767 - val_loss: 656149404.9315\n",
      "Epoch 1758/3350\n",
      "1168/1168 [==============================] - 0s 48us/step - loss: 702125242.3014 - val_loss: 651166190.9041\n",
      "Epoch 1759/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 710762336.4384 - val_loss: 654720980.3836\n",
      "Epoch 1760/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 714409287.4521 - val_loss: 667889103.3425\n",
      "Epoch 1761/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 733517490.8493 - val_loss: 643742050.1918\n",
      "Epoch 1762/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 689561386.9589 - val_loss: 657205797.0411\n",
      "Epoch 1763/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 688990795.6164 - val_loss: 681191979.1781\n",
      "Epoch 1764/3350\n",
      "1168/1168 [==============================] - 0s 50us/step - loss: 724625635.0685 - val_loss: 644104946.4110\n",
      "Epoch 1765/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 707812732.9315 - val_loss: 659463021.8082\n",
      "Epoch 1766/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 700002000.6575 - val_loss: 668142837.9178\n",
      "Epoch 1767/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 708715905.3151 - val_loss: 740779368.7671\n",
      "Epoch 1768/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 698076702.0274 - val_loss: 648447071.0137\n",
      "Epoch 1769/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 713837644.2740 - val_loss: 704181886.2466\n",
      "Epoch 1770/3350\n",
      "1168/1168 [==============================] - 0s 83us/step - loss: 712897035.3973 - val_loss: 652286812.2740\n",
      "Epoch 1771/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 710659636.8219 - val_loss: 653055464.7671\n",
      "Epoch 1772/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 693932487.0137 - val_loss: 656262192.8767\n",
      "Epoch 1773/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 699819391.5616 - val_loss: 659188149.5890\n",
      "Epoch 1774/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 693530932.1644 - val_loss: 653337687.6712\n",
      "Epoch 1775/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 700153419.3973 - val_loss: 702638568.5479\n",
      "Epoch 1776/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 696343356.4932 - val_loss: 657604915.0685\n",
      "Epoch 1777/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 717835747.0685 - val_loss: 669742041.4247\n",
      "Epoch 1778/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 712138536.7671 - val_loss: 671328141.5890\n",
      "Epoch 1779/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 681988085.4795 - val_loss: 666542627.9452\n",
      "Epoch 1780/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 684723988.6027 - val_loss: 685549206.5753\n",
      "Epoch 1781/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 700971573.9178 - val_loss: 656434921.6438\n",
      "Epoch 1782/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 681264362.0822 - val_loss: 685162440.9863\n",
      "Epoch 1783/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 683547024.2192 - val_loss: 648637559.2329\n",
      "Epoch 1784/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 681184745.6438 - val_loss: 684768380.9315\n",
      "Epoch 1785/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 689953446.1370 - val_loss: 720689526.2466\n",
      "Epoch 1786/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 766990344.7671 - val_loss: 680691866.0822\n",
      "Epoch 1787/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 689333788.4932 - val_loss: 649663518.4658\n",
      "Epoch 1788/3350\n",
      "1168/1168 [==============================] - 0s 94us/step - loss: 680379246.6849 - val_loss: 738668151.2329\n",
      "Epoch 1789/3350\n",
      "1168/1168 [==============================] - 0s 52us/step - loss: 695770291.7260 - val_loss: 682437125.9178\n",
      "Epoch 1790/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 688335635.7260 - val_loss: 676966132.0548\n",
      "Epoch 1791/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 699575633.7534 - val_loss: 691412537.4247\n",
      "Epoch 1792/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 682694542.4658 - val_loss: 714037608.3288\n",
      "Epoch 1793/3350\n",
      "1168/1168 [==============================] - 0s 53us/step - loss: 688930734.9041 - val_loss: 679602882.6301\n",
      "Epoch 1794/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 686194815.5616 - val_loss: 684565702.1370\n",
      "Epoch 1795/3350\n",
      "1168/1168 [==============================] - 0s 86us/step - loss: 698731609.8630 - val_loss: 652434564.1644\n",
      "Epoch 1796/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 690856839.6712 - val_loss: 695497335.8904\n",
      "Epoch 1797/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 689805806.0274 - val_loss: 653950173.5890\n",
      "Epoch 1798/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 685229892.8219 - val_loss: 673764367.7808\n",
      "Epoch 1799/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 681001974.3562 - val_loss: 653911063.0137\n",
      "Epoch 1800/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 674385183.5616 - val_loss: 677793815.0137\n",
      "Epoch 1801/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 674063524.8219 - val_loss: 670256986.3014\n",
      "Epoch 1802/3350\n",
      "1168/1168 [==============================] - 0s 48us/step - loss: 675388549.2603 - val_loss: 734315372.7123\n",
      "Epoch 1803/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 681118319.7808 - val_loss: 674741178.9589\n",
      "Epoch 1804/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 677808345.4247 - val_loss: 669753958.1370\n",
      "Epoch 1805/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 674880651.8356 - val_loss: 669034636.7123\n",
      "Epoch 1806/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 708453872.2192 - val_loss: 692145572.8219\n",
      "Epoch 1807/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 693651507.9452 - val_loss: 684352368.0000\n",
      "Epoch 1808/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 675405487.3425 - val_loss: 669036421.4795\n",
      "Epoch 1809/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 693495331.0685 - val_loss: 649374366.9041\n",
      "Epoch 1810/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 674272555.8356 - val_loss: 719646479.3425\n",
      "Epoch 1811/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 679558426.7397 - val_loss: 667130228.1644\n",
      "Epoch 1812/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 686921851.6164 - val_loss: 669830676.6027\n",
      "Epoch 1813/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 711147013.2603 - val_loss: 665111231.5616\n",
      "Epoch 1814/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 729459594.9589 - val_loss: 660520564.4932\n",
      "Epoch 1815/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 675601172.6027 - val_loss: 653456302.0274\n",
      "Epoch 1816/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 691544937.6438 - val_loss: 694841989.4795\n",
      "Epoch 1817/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 672155831.2329 - val_loss: 674309895.2329\n",
      "Epoch 1818/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 711683310.4658 - val_loss: 723888157.3699\n",
      "Epoch 1819/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 680618463.3425 - val_loss: 670745897.8630\n",
      "Epoch 1820/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 673142730.0822 - val_loss: 654407788.2740\n",
      "Epoch 1821/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 668275343.7808 - val_loss: 748435592.3288\n",
      "Epoch 1822/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 694585740.7123 - val_loss: 699345645.5890\n",
      "Epoch 1823/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 671323238.1370 - val_loss: 663166632.1096\n",
      "Epoch 1824/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 654487603.7260 - val_loss: 669249868.7123\n",
      "Epoch 1825/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 663321773.5890 - val_loss: 668244896.4384\n",
      "Epoch 1826/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 662353576.3288 - val_loss: 665027606.5753\n",
      "Epoch 1827/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 661996553.6438 - val_loss: 699041075.7260\n",
      "Epoch 1828/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 664106563.0685 - val_loss: 680490193.9726\n",
      "Epoch 1829/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 652178868.6027 - val_loss: 711842314.7397\n",
      "Epoch 1830/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 654744795.1781 - val_loss: 672272948.3836\n",
      "Epoch 1831/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 652444120.1096 - val_loss: 663730795.6164\n",
      "Epoch 1832/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 649564529.9726 - val_loss: 669046238.5753\n",
      "Epoch 1833/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 677397372.0548 - val_loss: 727990499.9452\n",
      "Epoch 1834/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 664495958.3562 - val_loss: 704224819.0685\n",
      "Epoch 1835/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 677095376.2192 - val_loss: 677907165.8082\n",
      "Epoch 1836/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 702439865.4247 - val_loss: 713928332.2740\n",
      "Epoch 1837/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 663001405.8082 - val_loss: 662381497.2055\n",
      "Epoch 1838/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 658061680.6575 - val_loss: 676980561.5342\n",
      "Epoch 1839/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 678472073.6438 - val_loss: 685248299.1781\n",
      "Epoch 1840/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 674123242.0822 - val_loss: 687689170.6301\n",
      "Epoch 1841/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 674307536.2192 - val_loss: 677288729.8630\n",
      "Epoch 1842/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 642363008.8767 - val_loss: 674677258.5205\n",
      "Epoch 1843/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 685808542.2466 - val_loss: 677202791.6712\n",
      "Epoch 1844/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 694686909.3699 - val_loss: 659871484.4932\n",
      "Epoch 1845/3350\n",
      "1168/1168 [==============================] - 0s 49us/step - loss: 668594458.3014 - val_loss: 694525328.1096\n",
      "Epoch 1846/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 657608022.3562 - val_loss: 707051754.9589\n",
      "Epoch 1847/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 656324293.2603 - val_loss: 662432688.4384\n",
      "Epoch 1848/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 656840465.5342 - val_loss: 677679248.8767\n",
      "Epoch 1849/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 683007875.0685 - val_loss: 690106634.9589\n",
      "Epoch 1850/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 681845082.1918 - val_loss: 652214068.8219\n",
      "Epoch 1851/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 683275146.5205 - val_loss: 808014910.2466\n",
      "Epoch 1852/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 665983492.8219 - val_loss: 673798233.6438\n",
      "Epoch 1853/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 654786479.7808 - val_loss: 669099699.8356\n",
      "Epoch 1854/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 648714038.3562 - val_loss: 674851330.3014\n",
      "Epoch 1855/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 667434088.1096 - val_loss: 673798488.5479\n",
      "Epoch 1856/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 667672540.0548 - val_loss: 668923237.2603\n",
      "Epoch 1857/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 638024026.3014 - val_loss: 702962638.9041\n",
      "Epoch 1858/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 662456115.2877 - val_loss: 705767732.6027\n",
      "Epoch 1859/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 647623146.0822 - val_loss: 686587134.9041\n",
      "Epoch 1860/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 649107863.2329 - val_loss: 691258245.2603\n",
      "Epoch 1861/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 668807870.2466 - val_loss: 695035842.4110\n",
      "Epoch 1862/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 636111040.8767 - val_loss: 689012593.7534\n",
      "Epoch 1863/3350\n",
      "1168/1168 [==============================] - 0s 100us/step - loss: 650873336.9863 - val_loss: 665267336.1096\n",
      "Epoch 1864/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 645493312.0000 - val_loss: 684018698.3014\n",
      "Epoch 1865/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 671416213.4795 - val_loss: 691096344.3288\n",
      "Epoch 1866/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 668375807.1233 - val_loss: 688850761.8630\n",
      "Epoch 1867/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 655653959.0137 - val_loss: 704866110.0274\n",
      "Epoch 1868/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 642909111.6712 - val_loss: 710062769.5342\n",
      "Epoch 1869/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 644800319.5616 - val_loss: 662121468.2740\n",
      "Epoch 1870/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 665142328.9863 - val_loss: 754610646.3562\n",
      "Epoch 1871/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 651292881.5342 - val_loss: 678985303.6712\n",
      "Epoch 1872/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 658301821.3699 - val_loss: 672795928.1096\n",
      "Epoch 1873/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 650452846.0274 - val_loss: 679338708.6027\n",
      "Epoch 1874/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 635150333.8082 - val_loss: 665468664.7671\n",
      "Epoch 1875/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 635131317.9178 - val_loss: 681362720.8767\n",
      "Epoch 1876/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 631448301.5890 - val_loss: 698318423.2329\n",
      "Epoch 1877/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 687205517.5890 - val_loss: 684801737.2055\n",
      "Epoch 1878/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 655039397.6986 - val_loss: 717729710.9041\n",
      "Epoch 1879/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 653449249.3151 - val_loss: 661979085.8082\n",
      "Epoch 1880/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 626060658.4110 - val_loss: 680781123.7260\n",
      "Epoch 1881/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 635507917.1507 - val_loss: 654530357.2603\n",
      "Epoch 1882/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 632828088.5479 - val_loss: 669384376.3288\n",
      "Epoch 1883/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 641360986.7397 - val_loss: 671911941.6986\n",
      "Epoch 1884/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 660016619.3973 - val_loss: 650191754.9589\n",
      "Epoch 1885/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 644297939.7260 - val_loss: 668013901.5890\n",
      "Epoch 1886/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 652474794.5205 - val_loss: 682833854.2466\n",
      "Epoch 1887/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 643935729.7534 - val_loss: 737596490.0822\n",
      "Epoch 1888/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 636045487.5616 - val_loss: 665969886.9041\n",
      "Epoch 1889/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 646514857.6438 - val_loss: 724811129.8630\n",
      "Epoch 1890/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 682330374.1370 - val_loss: 706763899.8356\n",
      "Epoch 1891/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 671164356.3836 - val_loss: 680376281.4247\n",
      "Epoch 1892/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 634053611.8356 - val_loss: 672568668.4932\n",
      "Epoch 1893/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 625989810.4110 - val_loss: 752192305.0959\n",
      "Epoch 1894/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 734352816.6575 - val_loss: 692209815.8904\n",
      "Epoch 1895/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 640601171.2877 - val_loss: 680779456.8767\n",
      "Epoch 1896/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 627133479.8904 - val_loss: 653915392.8767\n",
      "Epoch 1897/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 633051417.4247 - val_loss: 654394869.9178\n",
      "Epoch 1898/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 625327211.8356 - val_loss: 651840650.3014\n",
      "Epoch 1899/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 624235479.6712 - val_loss: 681889895.2329\n",
      "Epoch 1900/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 623754286.4658 - val_loss: 662030512.0000\n",
      "Epoch 1901/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 638640605.8082 - val_loss: 748141135.7808\n",
      "Epoch 1902/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 630294456.9863 - val_loss: 695466602.8493\n",
      "Epoch 1903/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 641955444.1644 - val_loss: 677800096.2192\n",
      "Epoch 1904/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 632223746.6301 - val_loss: 649425648.2192\n",
      "Epoch 1905/3350\n",
      "1168/1168 [==============================] - 0s 49us/step - loss: 617403913.7534 - val_loss: 675881009.7534\n",
      "Epoch 1906/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 621937578.7397 - val_loss: 678927240.7671\n",
      "Epoch 1907/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 634576700.9315 - val_loss: 653329223.8904\n",
      "Epoch 1908/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 623422290.4110 - val_loss: 640622996.3836\n",
      "Epoch 1909/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 659389794.8493 - val_loss: 727473486.0274\n",
      "Epoch 1910/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 646093816.1096 - val_loss: 656577304.9863\n",
      "Epoch 1911/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 639765723.3973 - val_loss: 675415298.8493\n",
      "Epoch 1912/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 637251229.3699 - val_loss: 678681511.6712\n",
      "Epoch 1913/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 621798212.8219 - val_loss: 741233079.2329\n",
      "Epoch 1914/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 703764707.9452 - val_loss: 658824479.3425\n",
      "Epoch 1915/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 620492326.3562 - val_loss: 681444019.7260\n",
      "Epoch 1916/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 628817954.1918 - val_loss: 659461013.9178\n",
      "Epoch 1917/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 658892420.8219 - val_loss: 686849027.2877\n",
      "Epoch 1918/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 620513708.2740 - val_loss: 664014418.6301\n",
      "Epoch 1919/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 616612547.5068 - val_loss: 653411797.5890\n",
      "Epoch 1920/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 625864615.4521 - val_loss: 667226368.2192\n",
      "Epoch 1921/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 623267967.5616 - val_loss: 647056496.8767\n",
      "Epoch 1922/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 613931448.1096 - val_loss: 650418185.6438\n",
      "Epoch 1923/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 624360249.8630 - val_loss: 645436476.8219\n",
      "Epoch 1924/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 616785602.6301 - val_loss: 731663921.5342\n",
      "Epoch 1925/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 629251443.2877 - val_loss: 689412997.0411\n",
      "Epoch 1926/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 622844849.9726 - val_loss: 662061977.2055\n",
      "Epoch 1927/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 626773584.6575 - val_loss: 688564011.3973\n",
      "Epoch 1928/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 604852528.2192 - val_loss: 660770626.3014\n",
      "Epoch 1929/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 622432554.9589 - val_loss: 745958003.7260\n",
      "Epoch 1930/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 620177724.4932 - val_loss: 626271132.1644\n",
      "Epoch 1931/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 616346790.1370 - val_loss: 755617687.2329\n",
      "Epoch 1932/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 627450736.6575 - val_loss: 657632881.3151\n",
      "Epoch 1933/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 618509509.2603 - val_loss: 640317038.4658\n",
      "Epoch 1934/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 619226136.5479 - val_loss: 645015198.6849\n",
      "Epoch 1935/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 636110049.3151 - val_loss: 635149310.6849\n",
      "Epoch 1936/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 622252513.3151 - val_loss: 635855579.3973\n",
      "Epoch 1937/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 620171238.1370 - val_loss: 653455676.7123\n",
      "Epoch 1938/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 623292723.7260 - val_loss: 651901759.1233\n",
      "Epoch 1939/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 626840005.6986 - val_loss: 659758775.6712\n",
      "Epoch 1940/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 604370301.3699 - val_loss: 678635310.4658\n",
      "Epoch 1941/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 607073213.3699 - val_loss: 660308814.9041\n",
      "Epoch 1942/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 617071790.2466 - val_loss: 651697207.3425\n",
      "Epoch 1943/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 631116288.4384 - val_loss: 675462583.4521\n",
      "Epoch 1944/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 608510673.0959 - val_loss: 652217870.2466\n",
      "Epoch 1945/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 603626328.1096 - val_loss: 647406458.5205\n",
      "Epoch 1946/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 598318803.2877 - val_loss: 682619705.2055\n",
      "Epoch 1947/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 608631169.3151 - val_loss: 635251815.3425\n",
      "Epoch 1948/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 603501745.0959 - val_loss: 716710553.4247\n",
      "Epoch 1949/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 645666369.7534 - val_loss: 648886286.9041\n",
      "Epoch 1950/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 610189254.5753 - val_loss: 672934894.9041\n",
      "Epoch 1951/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 604381019.1781 - val_loss: 691412567.4521\n",
      "Epoch 1952/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 607153831.8904 - val_loss: 627749388.7123\n",
      "Epoch 1953/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 625665957.6986 - val_loss: 660524199.0137\n",
      "Epoch 1954/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 619615117.1507 - val_loss: 639438001.0959\n",
      "Epoch 1955/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 660845832.3288 - val_loss: 642888446.4658\n",
      "Epoch 1956/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 604479534.4658 - val_loss: 633068013.8082\n",
      "Epoch 1957/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 605807577.8630 - val_loss: 654380625.9726\n",
      "Epoch 1958/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 623189749.9178 - val_loss: 656405687.8904\n",
      "Epoch 1959/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 611274973.3699 - val_loss: 625071395.0685\n",
      "Epoch 1960/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 605370782.6849 - val_loss: 621642982.7945\n",
      "Epoch 1961/3350\n",
      "1168/1168 [==============================] - 0s 51us/step - loss: 610761404.0548 - val_loss: 657037513.4247\n",
      "Epoch 1962/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 594765495.4521 - val_loss: 669306087.2329\n",
      "Epoch 1963/3350\n",
      "1168/1168 [==============================] - 0s 53us/step - loss: 612811544.1096 - val_loss: 618818942.6849\n",
      "Epoch 1964/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 625642589.8082 - val_loss: 678255352.1096\n",
      "Epoch 1965/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 593821530.3014 - val_loss: 639828131.9452\n",
      "Epoch 1966/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 610971359.1233 - val_loss: 632659215.3425\n",
      "Epoch 1967/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 598946303.5616 - val_loss: 636459056.4384\n",
      "Epoch 1968/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 594419114.0822 - val_loss: 670504166.3562\n",
      "Epoch 1969/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 622134721.7534 - val_loss: 635704000.6575\n",
      "Epoch 1970/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 601113104.6575 - val_loss: 665031796.6027\n",
      "Epoch 1971/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 590565523.7260 - val_loss: 643345351.1233\n",
      "Epoch 1972/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 609889682.8493 - val_loss: 630172010.9589\n",
      "Epoch 1973/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 589186132.6027 - val_loss: 676325038.0274\n",
      "Epoch 1974/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 596528945.5342 - val_loss: 646455514.5205\n",
      "Epoch 1975/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 612602141.8082 - val_loss: 631517231.3425\n",
      "Epoch 1976/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 596577456.2192 - val_loss: 696239303.2329\n",
      "Epoch 1977/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 597739921.0959 - val_loss: 616196601.8630\n",
      "Epoch 1978/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 607216777.6438 - val_loss: 621770679.5616\n",
      "Epoch 1979/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 600736391.8904 - val_loss: 647757146.3014\n",
      "Epoch 1980/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 601287534.9041 - val_loss: 657072496.4384\n",
      "Epoch 1981/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 599906036.6027 - val_loss: 636669662.4658\n",
      "Epoch 1982/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 620232957.8082 - val_loss: 635129531.7260\n",
      "Epoch 1983/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 585368809.6438 - val_loss: 619909947.6164\n",
      "Epoch 1984/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 592751756.2740 - val_loss: 627035097.2055\n",
      "Epoch 1985/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 579536797.8082 - val_loss: 629341660.2740\n",
      "Epoch 1986/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 590705284.6027 - val_loss: 617421630.2466\n",
      "Epoch 1987/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 610912920.1096 - val_loss: 635745725.8082\n",
      "Epoch 1988/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 598791580.9315 - val_loss: 639933130.0822\n",
      "Epoch 1989/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 583997999.3425 - val_loss: 634259951.5616\n",
      "Epoch 1990/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 574463071.1233 - val_loss: 599494431.0137\n",
      "Epoch 1991/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 602790905.8630 - val_loss: 625413954.4110\n",
      "Epoch 1992/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 617819502.9041 - val_loss: 671185867.6164\n",
      "Epoch 1993/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 614947160.9863 - val_loss: 746254277.2603\n",
      "Epoch 1994/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 579887429.6986 - val_loss: 628706907.6164\n",
      "Epoch 1995/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 596841772.4932 - val_loss: 677436288.2192\n",
      "Epoch 1996/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 585688093.3699 - val_loss: 607688115.7260\n",
      "Epoch 1997/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 625580604.4932 - val_loss: 616210440.5479\n",
      "Epoch 1998/3350\n",
      "1168/1168 [==============================] - 0s 55us/step - loss: 577505680.4384 - val_loss: 642564782.6849\n",
      "Epoch 1999/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 592224059.6164 - val_loss: 664366332.7123\n",
      "Epoch 2000/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 592911886.0274 - val_loss: 616078726.3562\n",
      "Epoch 2001/3350\n",
      "1168/1168 [==============================] - 0s 50us/step - loss: 590578437.6986 - val_loss: 620536061.5890\n",
      "Epoch 2002/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 583066668.2740 - val_loss: 631876786.6301\n",
      "Epoch 2003/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 587541713.7534 - val_loss: 608257819.6164\n",
      "Epoch 2004/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 577914926.0274 - val_loss: 614220653.3699\n",
      "Epoch 2005/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 590612650.9589 - val_loss: 599220157.1507\n",
      "Epoch 2006/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 581354684.4932 - val_loss: 615231905.9726\n",
      "Epoch 2007/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 571485574.5753 - val_loss: 626673925.9178\n",
      "Epoch 2008/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 582151936.4384 - val_loss: 615429697.3151\n",
      "Epoch 2009/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 576026984.3288 - val_loss: 615381298.1918\n",
      "Epoch 2010/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 605038908.9315 - val_loss: 658086574.4658\n",
      "Epoch 2011/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 605232373.0411 - val_loss: 607631378.1918\n",
      "Epoch 2012/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 580982635.3973 - val_loss: 599915898.0822\n",
      "Epoch 2013/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 610258852.8219 - val_loss: 672157052.4932\n",
      "Epoch 2014/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 584241605.6986 - val_loss: 622400646.2466\n",
      "Epoch 2015/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 569301336.1096 - val_loss: 605688584.9863\n",
      "Epoch 2016/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 579445831.8904 - val_loss: 599510908.0548\n",
      "Epoch 2017/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 571811379.7260 - val_loss: 611722157.8082\n",
      "Epoch 2018/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 566403997.8082 - val_loss: 622352456.5479\n",
      "Epoch 2019/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 585481616.2192 - val_loss: 600627940.2740\n",
      "Epoch 2020/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 570327716.8219 - val_loss: 636929113.2055\n",
      "Epoch 2021/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 601099542.7945 - val_loss: 594154275.0685\n",
      "Epoch 2022/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 578919957.0411 - val_loss: 638926835.9452\n",
      "Epoch 2023/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 565076238.4658 - val_loss: 602291656.7671\n",
      "Epoch 2024/3350\n",
      "1168/1168 [==============================] - 0s 82us/step - loss: 570902252.7123 - val_loss: 616107251.7260\n",
      "Epoch 2025/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 580164748.7123 - val_loss: 616074721.3151\n",
      "Epoch 2026/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 559038930.4110 - val_loss: 643841218.1918\n",
      "Epoch 2027/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 564652535.6712 - val_loss: 663417830.7945\n",
      "Epoch 2028/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 570857640.7671 - val_loss: 607626097.9726\n",
      "Epoch 2029/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 593180512.4384 - val_loss: 629505682.6301\n",
      "Epoch 2030/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 571059429.0411 - val_loss: 597011301.0411\n",
      "Epoch 2031/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 560079226.3014 - val_loss: 590080310.1370\n",
      "Epoch 2032/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 567890713.6438 - val_loss: 597743012.6027\n",
      "Epoch 2033/3350\n",
      "1168/1168 [==============================] - 0s 49us/step - loss: 580530523.6164 - val_loss: 611932842.9589\n",
      "Epoch 2034/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 561781962.0822 - val_loss: 674338470.1370\n",
      "Epoch 2035/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 576621936.6575 - val_loss: 593278272.6575\n",
      "Epoch 2036/3350\n",
      "1168/1168 [==============================] - 0s 91us/step - loss: 585852374.7945 - val_loss: 591276877.1507\n",
      "Epoch 2037/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 561760444.0548 - val_loss: 606499235.9452\n",
      "Epoch 2038/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 553287495.8904 - val_loss: 602994078.2466\n",
      "Epoch 2039/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 567048534.3562 - val_loss: 633440206.6849\n",
      "Epoch 2040/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 591972593.9726 - val_loss: 599065723.6164\n",
      "Epoch 2041/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 602577041.9726 - val_loss: 597422116.0548\n",
      "Epoch 2042/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 581731494.1370 - val_loss: 580199150.0274\n",
      "Epoch 2043/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 566559663.7808 - val_loss: 625510484.6027\n",
      "Epoch 2044/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 561506105.6438 - val_loss: 591502513.0959\n",
      "Epoch 2045/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 565236305.5342 - val_loss: 594818128.6575\n",
      "Epoch 2046/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 597921821.8082 - val_loss: 636015708.2740\n",
      "Epoch 2047/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 579172437.9178 - val_loss: 594064827.9452\n",
      "Epoch 2048/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 581178734.0274 - val_loss: 619408876.2740\n",
      "Epoch 2049/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 567725102.9041 - val_loss: 612005446.2466\n",
      "Epoch 2050/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 583254584.3288 - val_loss: 589940688.8767\n",
      "Epoch 2051/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 557734864.0000 - val_loss: 602300896.4384\n",
      "Epoch 2052/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 558512192.2192 - val_loss: 596422295.4521\n",
      "Epoch 2053/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 563399724.7123 - val_loss: 646763773.3699\n",
      "Epoch 2054/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 565992305.9726 - val_loss: 599623801.7534\n",
      "Epoch 2055/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 613264013.1507 - val_loss: 568094437.0411\n",
      "Epoch 2056/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 562365313.7534 - val_loss: 589740220.2740\n",
      "Epoch 2057/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 561857385.2055 - val_loss: 593302266.5205\n",
      "Epoch 2058/3350\n",
      "1168/1168 [==============================] - 0s 89us/step - loss: 561848142.4658 - val_loss: 582570994.4110\n",
      "Epoch 2059/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 560474952.7671 - val_loss: 600373412.8219\n",
      "Epoch 2060/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 560907647.5616 - val_loss: 588377977.6438\n",
      "Epoch 2061/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 546913299.5068 - val_loss: 598942498.7397\n",
      "Epoch 2062/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 556674805.4795 - val_loss: 587744806.7945\n",
      "Epoch 2063/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 548372586.5205 - val_loss: 587621730.1918\n",
      "Epoch 2064/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 547515510.3562 - val_loss: 607486989.3699\n",
      "Epoch 2065/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 563781308.4932 - val_loss: 618237132.9315\n",
      "Epoch 2066/3350\n",
      "1168/1168 [==============================] - 0s 79us/step - loss: 557669281.3151 - val_loss: 575557904.3288\n",
      "Epoch 2067/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 556895255.2329 - val_loss: 584222158.2466\n",
      "Epoch 2068/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 564502247.4521 - val_loss: 585496096.0000\n",
      "Epoch 2069/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 557050564.8219 - val_loss: 597435945.6438\n",
      "Epoch 2070/3350\n",
      "1168/1168 [==============================] - 0s 80us/step - loss: 570631710.6849 - val_loss: 575074497.2055\n",
      "Epoch 2071/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 561556222.6849 - val_loss: 608385652.1644\n",
      "Epoch 2072/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 561754772.1644 - val_loss: 609283327.1233\n",
      "Epoch 2073/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 545461731.5068 - val_loss: 578773196.3836\n",
      "Epoch 2074/3350\n",
      "1168/1168 [==============================] - 0s 96us/step - loss: 554484455.8904 - val_loss: 621379989.4795\n",
      "Epoch 2075/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 549161895.0137 - val_loss: 571966062.9041\n",
      "Epoch 2076/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 568495670.3562 - val_loss: 689075239.0137\n",
      "Epoch 2077/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 570312785.5342 - val_loss: 612695504.0000\n",
      "Epoch 2078/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 549693514.9589 - val_loss: 597192158.9041\n",
      "Epoch 2079/3350\n",
      "1168/1168 [==============================] - 0s 81us/step - loss: 537727573.9178 - val_loss: 593332436.3836\n",
      "Epoch 2080/3350\n",
      "1168/1168 [==============================] - 0s 79us/step - loss: 570330342.5753 - val_loss: 721838876.9315\n",
      "Epoch 2081/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 568402142.6849 - val_loss: 567741217.9726\n",
      "Epoch 2082/3350\n",
      "1168/1168 [==============================] - 0s 81us/step - loss: 558475624.9863 - val_loss: 587182693.6986\n",
      "Epoch 2083/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 583559493.2603 - val_loss: 601011004.7123\n",
      "Epoch 2084/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 591652800.4384 - val_loss: 632589940.8219\n",
      "Epoch 2085/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 548526503.0137 - val_loss: 590998490.3014\n",
      "Epoch 2086/3350\n",
      "1168/1168 [==============================] - 0s 86us/step - loss: 557180566.1370 - val_loss: 578941477.0411\n",
      "Epoch 2087/3350\n",
      "1168/1168 [==============================] - 0s 94us/step - loss: 543947102.6849 - val_loss: 583225682.7397\n",
      "Epoch 2088/3350\n",
      "1168/1168 [==============================] - 0s 95us/step - loss: 549712317.5890 - val_loss: 596601104.2192\n",
      "Epoch 2089/3350\n",
      "1168/1168 [==============================] - 0s 95us/step - loss: 546051472.0000 - val_loss: 574883398.5753\n",
      "Epoch 2090/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 544725014.7945 - val_loss: 578024816.0000\n",
      "Epoch 2091/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 535578012.0548 - val_loss: 611093263.7808\n",
      "Epoch 2092/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 573555390.2466 - val_loss: 600086072.1096\n",
      "Epoch 2093/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 580584606.6849 - val_loss: 597912102.3562\n",
      "Epoch 2094/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 551202820.3836 - val_loss: 586123970.4110\n",
      "Epoch 2095/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 545386042.5205 - val_loss: 595402463.5616\n",
      "Epoch 2096/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 546825315.5068 - val_loss: 633002664.1096\n",
      "Epoch 2097/3350\n",
      "1168/1168 [==============================] - 0s 80us/step - loss: 585053991.0137 - val_loss: 644370989.5890\n",
      "Epoch 2098/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 578413549.1507 - val_loss: 601038985.6438\n",
      "Epoch 2099/3350\n",
      "1168/1168 [==============================] - 0s 80us/step - loss: 538982056.3288 - val_loss: 589461794.8493\n",
      "Epoch 2100/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 573492095.5616 - val_loss: 587511263.0137\n",
      "Epoch 2101/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 552805727.1233 - val_loss: 603261690.6301\n",
      "Epoch 2102/3350\n",
      "1168/1168 [==============================] - 0s 83us/step - loss: 547844055.0137 - val_loss: 607167051.1781\n",
      "Epoch 2103/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 536006561.7534 - val_loss: 592249512.5479\n",
      "Epoch 2104/3350\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 534630279.0137 - val_loss: 622241749.6986\n",
      "Epoch 2105/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 549507632.2192 - val_loss: 608648375.2329\n",
      "Epoch 2106/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 561815568.6575 - val_loss: 603941318.9041\n",
      "Epoch 2107/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 540651248.2192 - val_loss: 649061282.1918\n",
      "Epoch 2108/3350\n",
      "1168/1168 [==============================] - 0s 81us/step - loss: 555331893.4795 - val_loss: 578492682.3014\n",
      "Epoch 2109/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 536282361.8630 - val_loss: 607290124.7123\n",
      "Epoch 2110/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 544310105.8630 - val_loss: 630119977.9726\n",
      "Epoch 2111/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 561106704.0000 - val_loss: 621500026.9589\n",
      "Epoch 2112/3350\n",
      "1168/1168 [==============================] - 0s 86us/step - loss: 546377300.6027 - val_loss: 591336752.0000\n",
      "Epoch 2113/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 541904148.1644 - val_loss: 629380226.4110\n",
      "Epoch 2114/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 535392475.6164 - val_loss: 610205686.3562\n",
      "Epoch 2115/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 559087650.1918 - val_loss: 576584153.2055\n",
      "Epoch 2116/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 543053521.9726 - val_loss: 657856178.6301\n",
      "Epoch 2117/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 558350712.5479 - val_loss: 605866725.6986\n",
      "Epoch 2118/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 544151245.1507 - val_loss: 573672720.9863\n",
      "Epoch 2119/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 552518795.8356 - val_loss: 588460084.1644\n",
      "Epoch 2120/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 539992149.9178 - val_loss: 592117659.5068\n",
      "Epoch 2121/3350\n",
      "1168/1168 [==============================] - 0s 89us/step - loss: 531912156.7123 - val_loss: 581832015.3425\n",
      "Epoch 2122/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 537080134.5753 - val_loss: 573697929.3151\n",
      "Epoch 2123/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 556266044.0548 - val_loss: 596235759.7808\n",
      "Epoch 2124/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 580605319.6712 - val_loss: 645189914.0822\n",
      "Epoch 2125/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 541424079.7808 - val_loss: 603394403.5068\n",
      "Epoch 2126/3350\n",
      "1168/1168 [==============================] - 0s 55us/step - loss: 534867033.8630 - val_loss: 594444116.8219\n",
      "Epoch 2127/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 551120833.3151 - val_loss: 604055065.6438\n",
      "Epoch 2128/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 547753322.5205 - val_loss: 607490999.2329\n",
      "Epoch 2129/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 567877244.0548 - val_loss: 585818601.7534\n",
      "Epoch 2130/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 561487655.4521 - val_loss: 580053965.9178\n",
      "Epoch 2131/3350\n",
      "1168/1168 [==============================] - 0s 81us/step - loss: 542450884.1644 - val_loss: 580532042.5205\n",
      "Epoch 2132/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 544014373.2603 - val_loss: 574447680.8767\n",
      "Epoch 2133/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 533383003.6164 - val_loss: 587717529.2055\n",
      "Epoch 2134/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 537360252.4932 - val_loss: 605291238.1370\n",
      "Epoch 2135/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 539433506.1918 - val_loss: 587237644.0548\n",
      "Epoch 2136/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 536509322.5205 - val_loss: 592686750.9041\n",
      "Epoch 2137/3350\n",
      "1168/1168 [==============================] - 0s 84us/step - loss: 526053203.0685 - val_loss: 594186228.3836\n",
      "Epoch 2138/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 531420392.7671 - val_loss: 589446186.6301\n",
      "Epoch 2139/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 533837188.6027 - val_loss: 588743632.6575\n",
      "Epoch 2140/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 525245283.9452 - val_loss: 622537052.4932\n",
      "Epoch 2141/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 535810488.5479 - val_loss: 600462982.6849\n",
      "Epoch 2142/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 529427108.8219 - val_loss: 561381189.2603\n",
      "Epoch 2143/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 521023961.4247 - val_loss: 570281328.3288\n",
      "Epoch 2144/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 547709269.4795 - val_loss: 630457573.9178\n",
      "Epoch 2145/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 527885063.4521 - val_loss: 585773002.9589\n",
      "Epoch 2146/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 535783760.2192 - val_loss: 584175124.9315\n",
      "Epoch 2147/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 559846981.6986 - val_loss: 573385571.5068\n",
      "Epoch 2148/3350\n",
      "1168/1168 [==============================] - 0s 84us/step - loss: 531539059.2877 - val_loss: 587332123.8356\n",
      "Epoch 2149/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 576327390.6849 - val_loss: 767576879.7808\n",
      "Epoch 2150/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 574017435.6164 - val_loss: 601751965.6986\n",
      "Epoch 2151/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 540578606.0274 - val_loss: 604756538.0822\n",
      "Epoch 2152/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 524850228.8219 - val_loss: 588399811.2877\n",
      "Epoch 2153/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 530670699.3973 - val_loss: 576848993.3151\n",
      "Epoch 2154/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 525001232.8767 - val_loss: 569855072.9863\n",
      "Epoch 2155/3350\n",
      "1168/1168 [==============================] - 0s 90us/step - loss: 526311802.3014 - val_loss: 570985760.4384\n",
      "Epoch 2156/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 526251964.0548 - val_loss: 575935152.4384\n",
      "Epoch 2157/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 530897915.6164 - val_loss: 626656053.2603\n",
      "Epoch 2158/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 545081080.9863 - val_loss: 577578280.9863\n",
      "Epoch 2159/3350\n",
      "1168/1168 [==============================] - 0s 84us/step - loss: 524013087.1233 - val_loss: 582512574.6849\n",
      "Epoch 2160/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 535414200.1096 - val_loss: 598524391.0137\n",
      "Epoch 2161/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 518415946.9589 - val_loss: 607320601.6438\n",
      "Epoch 2162/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 535347708.7123 - val_loss: 583081344.5479\n",
      "Epoch 2163/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 521512620.2740 - val_loss: 623501134.0274\n",
      "Epoch 2164/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 527519700.1644 - val_loss: 576456359.1233\n",
      "Epoch 2165/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 541899520.0000 - val_loss: 642346410.9589\n",
      "Epoch 2166/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 521157072.0000 - val_loss: 574605068.1644\n",
      "Epoch 2167/3350\n",
      "1168/1168 [==============================] - 0s 83us/step - loss: 520287034.7397 - val_loss: 591613003.8356\n",
      "Epoch 2168/3350\n",
      "1168/1168 [==============================] - 0s 79us/step - loss: 514009184.2192 - val_loss: 667071717.6986\n",
      "Epoch 2169/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 538404853.9178 - val_loss: 616778643.7260\n",
      "Epoch 2170/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 532545424.2192 - val_loss: 633576810.3014\n",
      "Epoch 2171/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 549786393.4247 - val_loss: 582647747.0685\n",
      "Epoch 2172/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 526171630.4658 - val_loss: 627111662.0274\n",
      "Epoch 2173/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 521785472.0000 - val_loss: 587768910.0274\n",
      "Epoch 2174/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 517672167.0137 - val_loss: 574546490.1918\n",
      "Epoch 2175/3350\n",
      "1168/1168 [==============================] - 0s 87us/step - loss: 528425546.9589 - val_loss: 593561670.1370\n",
      "Epoch 2176/3350\n",
      "1168/1168 [==============================] - 0s 83us/step - loss: 525500713.2055 - val_loss: 602398224.0000\n",
      "Epoch 2177/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 514854189.5890 - val_loss: 600217166.0274\n",
      "Epoch 2178/3350\n",
      "1168/1168 [==============================] - 0s 81us/step - loss: 523634274.1918 - val_loss: 577915317.9178\n",
      "Epoch 2179/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 543621434.3014 - val_loss: 605004189.3699\n",
      "Epoch 2180/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 515158346.5205 - val_loss: 610934529.3151\n",
      "Epoch 2181/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 544456344.1096 - val_loss: 578059022.7945\n",
      "Epoch 2182/3350\n",
      "1168/1168 [==============================] - 0s 81us/step - loss: 525966822.5753 - val_loss: 595154190.0274\n",
      "Epoch 2183/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 518758535.2329 - val_loss: 601474320.2192\n",
      "Epoch 2184/3350\n",
      "1168/1168 [==============================] - 0s 80us/step - loss: 533815459.5068 - val_loss: 607628924.2740\n",
      "Epoch 2185/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 544976704.4384 - val_loss: 634766279.3425\n",
      "Epoch 2186/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 527965519.7808 - val_loss: 589924287.5616\n",
      "Epoch 2187/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 523573657.8630 - val_loss: 591381631.1233\n",
      "Epoch 2188/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 534249070.9041 - val_loss: 591501784.1096\n",
      "Epoch 2189/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 519935632.2192 - val_loss: 624020032.1096\n",
      "Epoch 2190/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 516684437.0411 - val_loss: 593488730.0822\n",
      "Epoch 2191/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 523677119.1233 - val_loss: 598924997.5342\n",
      "Epoch 2192/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 529657352.3288 - val_loss: 663633677.1507\n",
      "Epoch 2193/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 515809192.3288 - val_loss: 610946804.6027\n",
      "Epoch 2194/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 509023846.1370 - val_loss: 576228651.8356\n",
      "Epoch 2195/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 530169016.9863 - val_loss: 623333937.9726\n",
      "Epoch 2196/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 548243214.0274 - val_loss: 570019336.2192\n",
      "Epoch 2197/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 521377006.4658 - val_loss: 586997047.2329\n",
      "Epoch 2198/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 513430853.0411 - val_loss: 607126813.8082\n",
      "Epoch 2199/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 519147519.1233 - val_loss: 587813522.0822\n",
      "Epoch 2200/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 509052323.0685 - val_loss: 599931478.5753\n",
      "Epoch 2201/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 526685686.7945 - val_loss: 611350214.5753\n",
      "Epoch 2202/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 509138243.5068 - val_loss: 590783874.0822\n",
      "Epoch 2203/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 510858947.9452 - val_loss: 659110589.1507\n",
      "Epoch 2204/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 542086634.9589 - val_loss: 592880132.8219\n",
      "Epoch 2205/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 509386051.0685 - val_loss: 586092546.9589\n",
      "Epoch 2206/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 513747301.9178 - val_loss: 580381565.9178\n",
      "Epoch 2207/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 513542170.7397 - val_loss: 579679001.2055\n",
      "Epoch 2208/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 514281372.4932 - val_loss: 650939238.1370\n",
      "Epoch 2209/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 528452115.7260 - val_loss: 588229388.4932\n",
      "Epoch 2210/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 525099253.9178 - val_loss: 626035675.3973\n",
      "Epoch 2211/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 510317624.5479 - val_loss: 633937580.0548\n",
      "Epoch 2212/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 523179779.5068 - val_loss: 577825167.8904\n",
      "Epoch 2213/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 509222940.0548 - val_loss: 599790089.8630\n",
      "Epoch 2214/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 522700197.2603 - val_loss: 581317089.7534\n",
      "Epoch 2215/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 515773419.9452 - val_loss: 589063915.6164\n",
      "Epoch 2216/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 509533425.9726 - val_loss: 594242733.4795\n",
      "Epoch 2217/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 504973261.6986 - val_loss: 610148377.6438\n",
      "Epoch 2218/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 519654591.1233 - val_loss: 777368671.1233\n",
      "Epoch 2219/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 538885784.3288 - val_loss: 587174348.4932\n",
      "Epoch 2220/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 502635136.0000 - val_loss: 688643527.8904\n",
      "Epoch 2221/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 578411814.5753 - val_loss: 600693866.0822\n",
      "Epoch 2222/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 528944334.0274 - val_loss: 598189786.5205\n",
      "Epoch 2223/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 536592573.5890 - val_loss: 572874132.4932\n",
      "Epoch 2224/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 510219370.9589 - val_loss: 646918576.2192\n",
      "Epoch 2225/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 509707391.3425 - val_loss: 595731799.8904\n",
      "Epoch 2226/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 520919458.1918 - val_loss: 618034713.8630\n",
      "Epoch 2227/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 533466995.7260 - val_loss: 590273363.7260\n",
      "Epoch 2228/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 539616023.6712 - val_loss: 578448451.5068\n",
      "Epoch 2229/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 515009725.8082 - val_loss: 573350083.6164\n",
      "Epoch 2230/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 517637582.0274 - val_loss: 571703966.3562\n",
      "Epoch 2231/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 526926624.0000 - val_loss: 587693099.8356\n",
      "Epoch 2232/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 512326941.8082 - val_loss: 618266012.6027\n",
      "Epoch 2233/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 517803591.8904 - val_loss: 623671866.0822\n",
      "Epoch 2234/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 531325006.0274 - val_loss: 594069295.1233\n",
      "Epoch 2235/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 510469897.2055 - val_loss: 692039560.3288\n",
      "Epoch 2236/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 517009026.6301 - val_loss: 586724428.2740\n",
      "Epoch 2237/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 520103571.7260 - val_loss: 626654141.8082\n",
      "Epoch 2238/3350\n",
      "1168/1168 [==============================] - 0s 53us/step - loss: 506324153.6438 - val_loss: 571385875.2877\n",
      "Epoch 2239/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 549067328.6575 - val_loss: 642431485.3699\n",
      "Epoch 2240/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 519329833.5342 - val_loss: 671497913.0959\n",
      "Epoch 2241/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 575584924.0548 - val_loss: 605433888.4384\n",
      "Epoch 2242/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 514812071.0137 - val_loss: 679123740.0548\n",
      "Epoch 2243/3350\n",
      "1168/1168 [==============================] - 0s 80us/step - loss: 524739352.9863 - val_loss: 588277418.9589\n",
      "Epoch 2244/3350\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 526921986.6301 - val_loss: 582741788.8219\n",
      "Epoch 2245/3350\n",
      "1168/1168 [==============================] - 0s 94us/step - loss: 509023332.3836 - val_loss: 621783202.8493\n",
      "Epoch 2246/3350\n",
      "1168/1168 [==============================] - 0s 80us/step - loss: 533100326.1370 - val_loss: 755878735.7808\n",
      "Epoch 2247/3350\n",
      "1168/1168 [==============================] - 0s 83us/step - loss: 568921941.0411 - val_loss: 591078735.0137\n",
      "Epoch 2248/3350\n",
      "1168/1168 [==============================] - 0s 98us/step - loss: 516486942.6849 - val_loss: 566434279.6712\n",
      "Epoch 2249/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 509931689.2055 - val_loss: 654580508.0548\n",
      "Epoch 2250/3350\n",
      "1168/1168 [==============================] - 0s 84us/step - loss: 511803376.2192 - val_loss: 587236099.5068\n",
      "Epoch 2251/3350\n",
      "1168/1168 [==============================] - 0s 98us/step - loss: 497698087.4521 - val_loss: 574274635.8356\n",
      "Epoch 2252/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 522909303.2329 - val_loss: 581073949.3699\n",
      "Epoch 2253/3350\n",
      "1168/1168 [==============================] - 0s 82us/step - loss: 525819839.7808 - val_loss: 599593149.1507\n",
      "Epoch 2254/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 504470851.9452 - val_loss: 613238127.3425\n",
      "Epoch 2255/3350\n",
      "1168/1168 [==============================] - 0s 55us/step - loss: 527336191.1233 - val_loss: 593867252.1644\n",
      "Epoch 2256/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 492839945.2055 - val_loss: 593330652.2740\n",
      "Epoch 2257/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 493784288.8767 - val_loss: 591275300.9315\n",
      "Epoch 2258/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 498567742.2466 - val_loss: 611824536.1096\n",
      "Epoch 2259/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 518224290.6301 - val_loss: 601376578.2466\n",
      "Epoch 2260/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 520306800.0000 - val_loss: 612044777.6438\n",
      "Epoch 2261/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 532297041.0959 - val_loss: 564634568.2192\n",
      "Epoch 2262/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 503443527.0137 - val_loss: 593366839.4521\n",
      "Epoch 2263/3350\n",
      "1168/1168 [==============================] - 0s 92us/step - loss: 492292496.6575 - val_loss: 584795622.9041\n",
      "Epoch 2264/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 512789943.2329 - val_loss: 645147833.6438\n",
      "Epoch 2265/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 517108413.5890 - val_loss: 619818581.0411\n",
      "Epoch 2266/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 497178772.1644 - val_loss: 580587061.4795\n",
      "Epoch 2267/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 509434009.2055 - val_loss: 568647481.8630\n",
      "Epoch 2268/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 488181618.4110 - val_loss: 587841545.3151\n",
      "Epoch 2269/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 518880179.2877 - val_loss: 674316814.6849\n",
      "Epoch 2270/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 519754574.9041 - val_loss: 579754034.7397\n",
      "Epoch 2271/3350\n",
      "1168/1168 [==============================] - 0s 84us/step - loss: 511193792.8767 - val_loss: 634165344.6575\n",
      "Epoch 2272/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 513054231.6712 - val_loss: 592302510.7945\n",
      "Epoch 2273/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 497011676.2740 - val_loss: 598827222.7945\n",
      "Epoch 2274/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 506155600.8767 - val_loss: 608054151.0137\n",
      "Epoch 2275/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 500820809.6438 - val_loss: 594456935.0137\n",
      "Epoch 2276/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 493886858.0822 - val_loss: 591688604.0548\n",
      "Epoch 2277/3350\n",
      "1168/1168 [==============================] - 0s 55us/step - loss: 491459488.0000 - val_loss: 604959454.6849\n",
      "Epoch 2278/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 519848608.4384 - val_loss: 612802433.3151\n",
      "Epoch 2279/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 506289387.8356 - val_loss: 576353628.9315\n",
      "Epoch 2280/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 492116472.5479 - val_loss: 583658387.9452\n",
      "Epoch 2281/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 490792152.9863 - val_loss: 590598198.5753\n",
      "Epoch 2282/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 498425876.6027 - val_loss: 611565517.3699\n",
      "Epoch 2283/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 497682825.6438 - val_loss: 600044014.2466\n",
      "Epoch 2284/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 508360651.3973 - val_loss: 582221387.3973\n",
      "Epoch 2285/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 500782419.0685 - val_loss: 597652136.5479\n",
      "Epoch 2286/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 499882695.0137 - val_loss: 593700525.6986\n",
      "Epoch 2287/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 510086659.9452 - val_loss: 585013316.1644\n",
      "Epoch 2288/3350\n",
      "1168/1168 [==============================] - 0s 82us/step - loss: 513664532.1644 - val_loss: 600436647.8904\n",
      "Epoch 2289/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 580466588.0548 - val_loss: 574958951.4521\n",
      "Epoch 2290/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 516348351.7808 - val_loss: 612239091.9452\n",
      "Epoch 2291/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 491660962.6301 - val_loss: 601521571.5068\n",
      "Epoch 2292/3350\n",
      "1168/1168 [==============================] - 0s 83us/step - loss: 505366464.2192 - val_loss: 577426187.8356\n",
      "Epoch 2293/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 491643025.0959 - val_loss: 561809068.7123\n",
      "Epoch 2294/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 502023977.2055 - val_loss: 576881113.8630\n",
      "Epoch 2295/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 492825998.6849 - val_loss: 643588004.6027\n",
      "Epoch 2296/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 482477419.8356 - val_loss: 617346163.7260\n",
      "Epoch 2297/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 513052154.0822 - val_loss: 588214421.4795\n",
      "Epoch 2298/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 484820193.3151 - val_loss: 571475969.5342\n",
      "Epoch 2299/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 485427344.8767 - val_loss: 584894707.8356\n",
      "Epoch 2300/3350\n",
      "1168/1168 [==============================] - 0s 54us/step - loss: 492558763.1781 - val_loss: 600275642.4110\n",
      "Epoch 2301/3350\n",
      "1168/1168 [==============================] - 0s 85us/step - loss: 485067093.9178 - val_loss: 600811268.6027\n",
      "Epoch 2302/3350\n",
      "1168/1168 [==============================] - 0s 80us/step - loss: 513042126.4658 - val_loss: 591623705.2055\n",
      "Epoch 2303/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 487302475.6164 - val_loss: 589742720.3288\n",
      "Epoch 2304/3350\n",
      "1168/1168 [==============================] - 0s 79us/step - loss: 499113628.9315 - val_loss: 579296512.0000\n",
      "Epoch 2305/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 491924377.4247 - val_loss: 602029786.4658\n",
      "Epoch 2306/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 489303945.2055 - val_loss: 577900422.9041\n",
      "Epoch 2307/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 481902517.4795 - val_loss: 615639666.4110\n",
      "Epoch 2308/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 493601603.0685 - val_loss: 609263844.0548\n",
      "Epoch 2309/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 480288272.6575 - val_loss: 598108202.9589\n",
      "Epoch 2310/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 478871617.7534 - val_loss: 589742115.2877\n",
      "Epoch 2311/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 484461081.4247 - val_loss: 575520384.3288\n",
      "Epoch 2312/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 489663270.5753 - val_loss: 627870390.7945\n",
      "Epoch 2313/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 488116125.3699 - val_loss: 583649801.5342\n",
      "Epoch 2314/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 479676877.5890 - val_loss: 574494814.6849\n",
      "Epoch 2315/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 500198309.6986 - val_loss: 579944372.3836\n",
      "Epoch 2316/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 498172506.3014 - val_loss: 584622756.4932\n",
      "Epoch 2317/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 490064520.3288 - val_loss: 672698598.5753\n",
      "Epoch 2318/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 493021015.2329 - val_loss: 593232513.4247\n",
      "Epoch 2319/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 513021564.4932 - val_loss: 584038910.0274\n",
      "Epoch 2320/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 498953496.1096 - val_loss: 611285665.2055\n",
      "Epoch 2321/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 478868785.0959 - val_loss: 614199234.4110\n",
      "Epoch 2322/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 490285824.4384 - val_loss: 613758877.3699\n",
      "Epoch 2323/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 502364442.3014 - val_loss: 590842785.5342\n",
      "Epoch 2324/3350\n",
      "1168/1168 [==============================] - 0s 79us/step - loss: 508530710.7945 - val_loss: 630117532.6575\n",
      "Epoch 2325/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 490396737.3151 - val_loss: 585133958.6849\n",
      "Epoch 2326/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 481858975.1233 - val_loss: 611494897.5342\n",
      "Epoch 2327/3350\n",
      "1168/1168 [==============================] - 0s 81us/step - loss: 517772181.9178 - val_loss: 601338438.9041\n",
      "Epoch 2328/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 511334586.0822 - val_loss: 618641176.0000\n",
      "Epoch 2329/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 479744520.3288 - val_loss: 586247426.9589\n",
      "Epoch 2330/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 483550306.6301 - val_loss: 580954560.8767\n",
      "Epoch 2331/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 485775002.7397 - val_loss: 583851716.1644\n",
      "Epoch 2332/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 488788621.5890 - val_loss: 587578161.0959\n",
      "Epoch 2333/3350\n",
      "1168/1168 [==============================] - 0s 54us/step - loss: 485605505.9726 - val_loss: 598319303.4521\n",
      "Epoch 2334/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 488688631.2329 - val_loss: 605951131.2877\n",
      "Epoch 2335/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 480235674.7397 - val_loss: 599180393.2055\n",
      "Epoch 2336/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 494837660.9315 - val_loss: 579400749.6438\n",
      "Epoch 2337/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 509419707.1781 - val_loss: 591668485.9178\n",
      "Epoch 2338/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 474984185.4247 - val_loss: 597165475.6164\n",
      "Epoch 2339/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 500801938.8493 - val_loss: 566344319.8904\n",
      "Epoch 2340/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 486987832.5479 - val_loss: 619520327.0137\n",
      "Epoch 2341/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 495141730.4110 - val_loss: 600131277.2603\n",
      "Epoch 2342/3350\n",
      "1168/1168 [==============================] - 0s 48us/step - loss: 500783399.2329 - val_loss: 610596642.1918\n",
      "Epoch 2343/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 487737603.6164 - val_loss: 605180130.1918\n",
      "Epoch 2344/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 484220712.3288 - val_loss: 620833272.7671\n",
      "Epoch 2345/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 493373745.0959 - val_loss: 614540929.3151\n",
      "Epoch 2346/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 495924770.8493 - val_loss: 571877932.9315\n",
      "Epoch 2347/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 481993235.0685 - val_loss: 594558131.9452\n",
      "Epoch 2348/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 474527901.8082 - val_loss: 587739308.1644\n",
      "Epoch 2349/3350\n",
      "1168/1168 [==============================] - 0s 82us/step - loss: 487023050.9589 - val_loss: 692607486.2466\n",
      "Epoch 2350/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 545569725.8082 - val_loss: 604621480.3288\n",
      "Epoch 2351/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 527128345.8630 - val_loss: 613102630.3562\n",
      "Epoch 2352/3350\n",
      "1168/1168 [==============================] - 0s 80us/step - loss: 506408056.9863 - val_loss: 574270859.3973\n",
      "Epoch 2353/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 490850790.5753 - val_loss: 591287616.2192\n",
      "Epoch 2354/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 493739536.2192 - val_loss: 593446137.6438\n",
      "Epoch 2355/3350\n",
      "1168/1168 [==============================] - 0s 84us/step - loss: 484604816.2192 - val_loss: 595377771.0685\n",
      "Epoch 2356/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 474410808.5479 - val_loss: 619857025.0411\n",
      "Epoch 2357/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 560457098.5205 - val_loss: 668896212.1644\n",
      "Epoch 2358/3350\n",
      "1168/1168 [==============================] - 0s 82us/step - loss: 515435127.2329 - val_loss: 586049537.0959\n",
      "Epoch 2359/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 474488448.8767 - val_loss: 637826909.6438\n",
      "Epoch 2360/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 538189842.4110 - val_loss: 585136570.9589\n",
      "Epoch 2361/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 471615235.5068 - val_loss: 609316285.5890\n",
      "Epoch 2362/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 472290435.5068 - val_loss: 575573453.5890\n",
      "Epoch 2363/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 472005897.6438 - val_loss: 587019803.3973\n",
      "Epoch 2364/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 497751246.4658 - val_loss: 584778792.3288\n",
      "Epoch 2365/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 484679606.1370 - val_loss: 585979556.3836\n",
      "Epoch 2366/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 468130843.1781 - val_loss: 596110345.8630\n",
      "Epoch 2367/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 484011511.6712 - val_loss: 609805446.1370\n",
      "Epoch 2368/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 497605592.3288 - val_loss: 581090416.6575\n",
      "Epoch 2369/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 488081662.9041 - val_loss: 593206999.1233\n",
      "Epoch 2370/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 510341677.1507 - val_loss: 577025424.9863\n",
      "Epoch 2371/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 479572174.4658 - val_loss: 585142885.2603\n",
      "Epoch 2372/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 565931767.6712 - val_loss: 618225397.6986\n",
      "Epoch 2373/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 476975793.0959 - val_loss: 596071693.1507\n",
      "Epoch 2374/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 485406128.2192 - val_loss: 596525503.2877\n",
      "Epoch 2375/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 497171033.8630 - val_loss: 621455959.4521\n",
      "Epoch 2376/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 466798716.9315 - val_loss: 579208720.7671\n",
      "Epoch 2377/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 465250471.0137 - val_loss: 612221791.1233\n",
      "Epoch 2378/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 491597182.2466 - val_loss: 591543147.3973\n",
      "Epoch 2379/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 469176309.4795 - val_loss: 623869134.9041\n",
      "Epoch 2380/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 482440432.2192 - val_loss: 576329686.1370\n",
      "Epoch 2381/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 482187749.2603 - val_loss: 620774277.6986\n",
      "Epoch 2382/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 470609184.8767 - val_loss: 585233201.0959\n",
      "Epoch 2383/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 463923900.9315 - val_loss: 583301200.1096\n",
      "Epoch 2384/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 470703352.3288 - val_loss: 671549810.4110\n",
      "Epoch 2385/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 504553134.4658 - val_loss: 591394951.2329\n",
      "Epoch 2386/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 478527927.2329 - val_loss: 573866855.6712\n",
      "Epoch 2387/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 466314634.9589 - val_loss: 596963467.8356\n",
      "Epoch 2388/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 466842694.7945 - val_loss: 577207673.2055\n",
      "Epoch 2389/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 469314831.3425 - val_loss: 563872179.3973\n",
      "Epoch 2390/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 500278342.5753 - val_loss: 606953265.9726\n",
      "Epoch 2391/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 456896706.1918 - val_loss: 584093506.6301\n",
      "Epoch 2392/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 464823576.5479 - val_loss: 601511421.3699\n",
      "Epoch 2393/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 468132688.4384 - val_loss: 580339031.5616\n",
      "Epoch 2394/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 472638501.2603 - val_loss: 579557561.3151\n",
      "Epoch 2395/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 466927636.1644 - val_loss: 573463150.6849\n",
      "Epoch 2396/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 470271838.4658 - val_loss: 582118372.4932\n",
      "Epoch 2397/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 472140405.0411 - val_loss: 587498359.7808\n",
      "Epoch 2398/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 464274438.1370 - val_loss: 614059464.5479\n",
      "Epoch 2399/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 476563626.5205 - val_loss: 593115928.5479\n",
      "Epoch 2400/3350\n",
      "1168/1168 [==============================] - 0s 48us/step - loss: 466578188.7123 - val_loss: 595971651.2877\n",
      "Epoch 2401/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 477437038.9041 - val_loss: 631031729.9726\n",
      "Epoch 2402/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 471254852.3836 - val_loss: 657600682.5205\n",
      "Epoch 2403/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 494220801.5342 - val_loss: 775659403.6164\n",
      "Epoch 2404/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 517822446.9041 - val_loss: 609658001.7534\n",
      "Epoch 2405/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 483047216.8767 - val_loss: 600226831.8904\n",
      "Epoch 2406/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 494315317.0411 - val_loss: 583442963.7808\n",
      "Epoch 2407/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 479847054.9041 - val_loss: 594194730.9041\n",
      "Epoch 2408/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 488090593.7534 - val_loss: 585095133.1507\n",
      "Epoch 2409/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 486984014.0274 - val_loss: 600745181.1507\n",
      "Epoch 2410/3350\n",
      "1168/1168 [==============================] - 0s 46us/step - loss: 480348196.8219 - val_loss: 574555364.3836\n",
      "Epoch 2411/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 482090512.6575 - val_loss: 614041194.7397\n",
      "Epoch 2412/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 544260555.3973 - val_loss: 613758454.5753\n",
      "Epoch 2413/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 480877664.0000 - val_loss: 603803490.6301\n",
      "Epoch 2414/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 476606683.1781 - val_loss: 585148314.5205\n",
      "Epoch 2415/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 508398124.2740 - val_loss: 624586751.1233\n",
      "Epoch 2416/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 471983026.8493 - val_loss: 589417573.5890\n",
      "Epoch 2417/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 476582552.3288 - val_loss: 627949396.3836\n",
      "Epoch 2418/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 463589227.8356 - val_loss: 603553000.1096\n",
      "Epoch 2419/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 467876413.3699 - val_loss: 578476118.5753\n",
      "Epoch 2420/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 463020129.6438 - val_loss: 591830605.8082\n",
      "Epoch 2421/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 472100844.7123 - val_loss: 600596442.7397\n",
      "Epoch 2422/3350\n",
      "1168/1168 [==============================] - 0s 50us/step - loss: 457234284.2740 - val_loss: 607153552.0000\n",
      "Epoch 2423/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 472888464.6575 - val_loss: 573709125.9178\n",
      "Epoch 2424/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 506799734.7945 - val_loss: 572204102.5753\n",
      "Epoch 2425/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 480161576.3288 - val_loss: 634022656.8767\n",
      "Epoch 2426/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 460795923.2877 - val_loss: 579807220.6027\n",
      "Epoch 2427/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 457156573.8082 - val_loss: 595443506.6301\n",
      "Epoch 2428/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 480175918.2466 - val_loss: 584908468.6027\n",
      "Epoch 2429/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 465944308.6027 - val_loss: 584686440.3288\n",
      "Epoch 2430/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 472414734.6849 - val_loss: 601694719.1233\n",
      "Epoch 2431/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 461933283.9452 - val_loss: 577728570.8493\n",
      "Epoch 2432/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 488123360.8767 - val_loss: 586965656.3288\n",
      "Epoch 2433/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 474382250.3014 - val_loss: 575704343.0137\n",
      "Epoch 2434/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 451401623.6712 - val_loss: 576552298.3014\n",
      "Epoch 2435/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 459742203.6164 - val_loss: 569595602.9589\n",
      "Epoch 2436/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 499540154.5205 - val_loss: 615991727.7808\n",
      "Epoch 2437/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 476809557.0411 - val_loss: 587148139.8356\n",
      "Epoch 2438/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 464890177.9726 - val_loss: 588061120.2192\n",
      "Epoch 2439/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 469488472.3288 - val_loss: 588850660.4932\n",
      "Epoch 2440/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 459443783.4521 - val_loss: 622158809.4247\n",
      "Epoch 2441/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 467744814.9041 - val_loss: 595380721.3151\n",
      "Epoch 2442/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 475640325.2603 - val_loss: 637398974.4658\n",
      "Epoch 2443/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 469297374.2466 - val_loss: 586742164.8219\n",
      "Epoch 2444/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 459555889.0959 - val_loss: 634906272.0000\n",
      "Epoch 2445/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 488338144.8767 - val_loss: 596665336.3288\n",
      "Epoch 2446/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 501328217.8630 - val_loss: 633970168.5479\n",
      "Epoch 2447/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 466765351.8904 - val_loss: 589783553.0959\n",
      "Epoch 2448/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 456680642.1918 - val_loss: 588298122.6301\n",
      "Epoch 2449/3350\n",
      "1168/1168 [==============================] - 0s 52us/step - loss: 488973367.2329 - val_loss: 602398181.6986\n",
      "Epoch 2450/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 462770373.2603 - val_loss: 582520166.1370\n",
      "Epoch 2451/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 458097481.4247 - val_loss: 671145122.1918\n",
      "Epoch 2452/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 484808171.3973 - val_loss: 586086513.0959\n",
      "Epoch 2453/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 470297251.5068 - val_loss: 610584503.0137\n",
      "Epoch 2454/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 479147201.7534 - val_loss: 597984540.4932\n",
      "Epoch 2455/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 482337460.3836 - val_loss: 592397292.8219\n",
      "Epoch 2456/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 480723518.6849 - val_loss: 608008405.3699\n",
      "Epoch 2457/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 446066169.8630 - val_loss: 634329301.2603\n",
      "Epoch 2458/3350\n",
      "1168/1168 [==============================] - 0s 56us/step - loss: 501905051.1781 - val_loss: 590164140.8767\n",
      "Epoch 2459/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 475953908.1644 - val_loss: 596574246.7945\n",
      "Epoch 2460/3350\n",
      "1168/1168 [==============================] - 0s 55us/step - loss: 467923110.1370 - val_loss: 576943090.1918\n",
      "Epoch 2461/3350\n",
      "1168/1168 [==============================] - 0s 55us/step - loss: 455741899.8356 - val_loss: 579805284.1644\n",
      "Epoch 2462/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 463530003.7260 - val_loss: 561954950.3562\n",
      "Epoch 2463/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 455675633.9726 - val_loss: 648317996.7123\n",
      "Epoch 2464/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 459726267.6164 - val_loss: 597223656.9863\n",
      "Epoch 2465/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 462787921.0959 - val_loss: 592931171.7260\n",
      "Epoch 2466/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 458094883.0685 - val_loss: 552066970.7397\n",
      "Epoch 2467/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 459294081.3151 - val_loss: 573401697.7534\n",
      "Epoch 2468/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 459475984.2192 - val_loss: 570888704.5479\n",
      "Epoch 2469/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 455641164.2740 - val_loss: 586003610.3014\n",
      "Epoch 2470/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 459812177.9726 - val_loss: 591408195.2877\n",
      "Epoch 2471/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 466802474.0822 - val_loss: 579303688.3288\n",
      "Epoch 2472/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 454163103.5616 - val_loss: 598566530.8493\n",
      "Epoch 2473/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 480980736.4384 - val_loss: 572998959.5616\n",
      "Epoch 2474/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 455913208.9863 - val_loss: 571773186.4110\n",
      "Epoch 2475/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 499432266.9589 - val_loss: 549401437.6986\n",
      "Epoch 2476/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 452724719.1233 - val_loss: 590165255.2329\n",
      "Epoch 2477/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 458394499.9452 - val_loss: 574592014.2466\n",
      "Epoch 2478/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 455404682.5205 - val_loss: 576895218.0822\n",
      "Epoch 2479/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 447460561.5342 - val_loss: 574662718.1370\n",
      "Epoch 2480/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 458098280.3288 - val_loss: 586542135.2329\n",
      "Epoch 2481/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 458473631.5616 - val_loss: 593668923.5068\n",
      "Epoch 2482/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 451319797.2603 - val_loss: 594903255.6712\n",
      "Epoch 2483/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 454199372.6027 - val_loss: 591560537.6438\n",
      "Epoch 2484/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 446418025.2055 - val_loss: 588666467.9452\n",
      "Epoch 2485/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 452551619.9452 - val_loss: 599340429.8082\n",
      "Epoch 2486/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 468704878.9041 - val_loss: 583214753.5342\n",
      "Epoch 2487/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 462766037.9178 - val_loss: 569902241.4247\n",
      "Epoch 2488/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 457678780.4932 - val_loss: 625966465.5342\n",
      "Epoch 2489/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 449220436.6027 - val_loss: 586072119.6712\n",
      "Epoch 2490/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 471406678.3562 - val_loss: 674787070.2466\n",
      "Epoch 2491/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 480032222.6849 - val_loss: 567270062.7945\n",
      "Epoch 2492/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 454543467.3973 - val_loss: 628714226.8493\n",
      "Epoch 2493/3350\n",
      "1168/1168 [==============================] - 0s 53us/step - loss: 468297661.3699 - val_loss: 568073475.8356\n",
      "Epoch 2494/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 463651261.8082 - val_loss: 572791388.3836\n",
      "Epoch 2495/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 462015860.6027 - val_loss: 590702669.5890\n",
      "Epoch 2496/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 471500679.8904 - val_loss: 586265259.3973\n",
      "Epoch 2497/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 486047616.0000 - val_loss: 569713481.9178\n",
      "Epoch 2498/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 445641050.3014 - val_loss: 608732156.7123\n",
      "Epoch 2499/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 485510022.3562 - val_loss: 586202068.1644\n",
      "Epoch 2500/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 455357984.4384 - val_loss: 590027600.4384\n",
      "Epoch 2501/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 453373031.0137 - val_loss: 579184212.8219\n",
      "Epoch 2502/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 454319134.2466 - val_loss: 564124305.5342\n",
      "Epoch 2503/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 452788632.5479 - val_loss: 589456512.4384\n",
      "Epoch 2504/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 465757027.0685 - val_loss: 582706756.6027\n",
      "Epoch 2505/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 458157509.6986 - val_loss: 595378006.1370\n",
      "Epoch 2506/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 456493839.7808 - val_loss: 590227029.2603\n",
      "Epoch 2507/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 453981982.2466 - val_loss: 582505806.4658\n",
      "Epoch 2508/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 445067448.5479 - val_loss: 601952732.9315\n",
      "Epoch 2509/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 469004234.0822 - val_loss: 548157031.8904\n",
      "Epoch 2510/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 449779779.5068 - val_loss: 590377353.4247\n",
      "Epoch 2511/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 441117634.1918 - val_loss: 586008532.8219\n",
      "Epoch 2512/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 461790467.5068 - val_loss: 551384474.1918\n",
      "Epoch 2513/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 462664199.4521 - val_loss: 591692068.3836\n",
      "Epoch 2514/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 453574228.6027 - val_loss: 557270613.2603\n",
      "Epoch 2515/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 447283212.4932 - val_loss: 578166500.8219\n",
      "Epoch 2516/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 475235178.9589 - val_loss: 677395262.6849\n",
      "Epoch 2517/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 473711601.0959 - val_loss: 600873218.1918\n",
      "Epoch 2518/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 448979136.2192 - val_loss: 617649925.6986\n",
      "Epoch 2519/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 451147687.8904 - val_loss: 587501998.2466\n",
      "Epoch 2520/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 454592242.8493 - val_loss: 575165084.1644\n",
      "Epoch 2521/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 440761148.0548 - val_loss: 574191177.2055\n",
      "Epoch 2522/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 449421607.4521 - val_loss: 576559164.1644\n",
      "Epoch 2523/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 446366563.0685 - val_loss: 567144515.3973\n",
      "Epoch 2524/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 441215646.6849 - val_loss: 587374963.7260\n",
      "Epoch 2525/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 449467459.0685 - val_loss: 587140251.8356\n",
      "Epoch 2526/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 447366786.1918 - val_loss: 564368549.0411\n",
      "Epoch 2527/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 452018050.6301 - val_loss: 596591670.5753\n",
      "Epoch 2528/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 452141287.4521 - val_loss: 566647656.3288\n",
      "Epoch 2529/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 443091725.5890 - val_loss: 564365354.5205\n",
      "Epoch 2530/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 461292584.3288 - val_loss: 552998871.4521\n",
      "Epoch 2531/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 461335354.7397 - val_loss: 659439674.3014\n",
      "Epoch 2532/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 443458401.7534 - val_loss: 586542681.9726\n",
      "Epoch 2533/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 500470192.6575 - val_loss: 579408533.4247\n",
      "Epoch 2534/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 473877634.6301 - val_loss: 619799193.8630\n",
      "Epoch 2535/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 461338926.6849 - val_loss: 573658528.4384\n",
      "Epoch 2536/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 493241957.6986 - val_loss: 573048449.8630\n",
      "Epoch 2537/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 459349314.6301 - val_loss: 570549445.4795\n",
      "Epoch 2538/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 438679132.9315 - val_loss: 650887065.4247\n",
      "Epoch 2539/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 502699526.5753 - val_loss: 543647701.6986\n",
      "Epoch 2540/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 478814933.0411 - val_loss: 621920261.2603\n",
      "Epoch 2541/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 451001061.2603 - val_loss: 583024400.6575\n",
      "Epoch 2542/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 456589550.0274 - val_loss: 577663712.2192\n",
      "Epoch 2543/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 460517005.1507 - val_loss: 572479608.0000\n",
      "Epoch 2544/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 452527239.4521 - val_loss: 586629384.1096\n",
      "Epoch 2545/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 436365874.8493 - val_loss: 598357988.6027\n",
      "Epoch 2546/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 445525258.0822 - val_loss: 582676912.8767\n",
      "Epoch 2547/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 458529958.1370 - val_loss: 590486244.1644\n",
      "Epoch 2548/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 441637251.0685 - val_loss: 602511129.4247\n",
      "Epoch 2549/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 433289306.3014 - val_loss: 580136973.9178\n",
      "Epoch 2550/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 437939130.7397 - val_loss: 580234507.3973\n",
      "Epoch 2551/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 452651555.5068 - val_loss: 562406825.6438\n",
      "Epoch 2552/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 441903561.8630 - val_loss: 570432449.2055\n",
      "Epoch 2553/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 433317370.7397 - val_loss: 576795296.4384\n",
      "Epoch 2554/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 430361859.9452 - val_loss: 568035353.2055\n",
      "Epoch 2555/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 438993863.0137 - val_loss: 597906880.6575\n",
      "Epoch 2556/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 481767002.9589 - val_loss: 585402978.6301\n",
      "Epoch 2557/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 444420638.2466 - val_loss: 577367874.1918\n",
      "Epoch 2558/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 435921738.0822 - val_loss: 611330691.2877\n",
      "Epoch 2559/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 446985211.1781 - val_loss: 568085371.7260\n",
      "Epoch 2560/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 440659019.3973 - val_loss: 564974610.3014\n",
      "Epoch 2561/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 435225493.6986 - val_loss: 568839553.6438\n",
      "Epoch 2562/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 436913935.3425 - val_loss: 571199260.8219\n",
      "Epoch 2563/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 441345443.0685 - val_loss: 661520650.0822\n",
      "Epoch 2564/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 479755416.1096 - val_loss: 589575268.6027\n",
      "Epoch 2565/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 441928723.5068 - val_loss: 575467688.1644\n",
      "Epoch 2566/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 444517588.1644 - val_loss: 584390963.7260\n",
      "Epoch 2567/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 485558837.9178 - val_loss: 570635801.6438\n",
      "Epoch 2568/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 459057115.1781 - val_loss: 590230703.1233\n",
      "Epoch 2569/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 467171673.4247 - val_loss: 603643810.1918\n",
      "Epoch 2570/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 451235319.2329 - val_loss: 586111246.6849\n",
      "Epoch 2571/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 443347928.9863 - val_loss: 553403906.7397\n",
      "Epoch 2572/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 436277635.2877 - val_loss: 586830322.1918\n",
      "Epoch 2573/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 437149823.5616 - val_loss: 592610861.4795\n",
      "Epoch 2574/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 437904797.3699 - val_loss: 577588843.3425\n",
      "Epoch 2575/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 438953845.4795 - val_loss: 572465574.3562\n",
      "Epoch 2576/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 428152319.3425 - val_loss: 581104680.3288\n",
      "Epoch 2577/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 439480796.2740 - val_loss: 574552001.3151\n",
      "Epoch 2578/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 437375027.2877 - val_loss: 563770013.8082\n",
      "Epoch 2579/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 442216937.6438 - val_loss: 571951793.5342\n",
      "Epoch 2580/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 459578236.0548 - val_loss: 560662928.1096\n",
      "Epoch 2581/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 432585657.8630 - val_loss: 564758789.8082\n",
      "Epoch 2582/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 441024615.4521 - val_loss: 588059269.0411\n",
      "Epoch 2583/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 440367027.8356 - val_loss: 571004500.4932\n",
      "Epoch 2584/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 456921973.6986 - val_loss: 556550515.7260\n",
      "Epoch 2585/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 463920035.9452 - val_loss: 565716356.4932\n",
      "Epoch 2586/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 437679750.5753 - val_loss: 565242385.8630\n",
      "Epoch 2587/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 456780239.7808 - val_loss: 599899357.3699\n",
      "Epoch 2588/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 461196300.2740 - val_loss: 600397017.9726\n",
      "Epoch 2589/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 456543876.8219 - val_loss: 626739597.2055\n",
      "Epoch 2590/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 439858785.7534 - val_loss: 598481000.5479\n",
      "Epoch 2591/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 444540356.8219 - val_loss: 609910257.7534\n",
      "Epoch 2592/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 426095992.5479 - val_loss: 576499029.7534\n",
      "Epoch 2593/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 438905859.5068 - val_loss: 598782413.5890\n",
      "Epoch 2594/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 440027654.1370 - val_loss: 602486743.8904\n",
      "Epoch 2595/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 434464931.9452 - val_loss: 567621779.1781\n",
      "Epoch 2596/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 428274461.5890 - val_loss: 628221148.9315\n",
      "Epoch 2597/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 458631005.8082 - val_loss: 596137372.2740\n",
      "Epoch 2598/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 434147520.0000 - val_loss: 578489302.7945\n",
      "Epoch 2599/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 428813007.7808 - val_loss: 566887054.3562\n",
      "Epoch 2600/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 430078718.2466 - val_loss: 565015606.1370\n",
      "Epoch 2601/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 440383254.7945 - val_loss: 563477611.5068\n",
      "Epoch 2602/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 465463335.8904 - val_loss: 567633928.1096\n",
      "Epoch 2603/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 432763207.6712 - val_loss: 574853920.1096\n",
      "Epoch 2604/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 429459200.8767 - val_loss: 577580986.3014\n",
      "Epoch 2605/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 435624948.1644 - val_loss: 570243251.5068\n",
      "Epoch 2606/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 463216742.1370 - val_loss: 564649069.0959\n",
      "Epoch 2607/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 433441852.9315 - val_loss: 601906494.9041\n",
      "Epoch 2608/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 433720184.1096 - val_loss: 565618039.6712\n",
      "Epoch 2609/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 445550238.2466 - val_loss: 572051619.6164\n",
      "Epoch 2610/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 448892719.7808 - val_loss: 585383488.6575\n",
      "Epoch 2611/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 480268644.8219 - val_loss: 542876331.7260\n",
      "Epoch 2612/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 466309452.7123 - val_loss: 574393571.7260\n",
      "Epoch 2613/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 450386173.8082 - val_loss: 569783345.5342\n",
      "Epoch 2614/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 481019609.4247 - val_loss: 580599085.5890\n",
      "Epoch 2615/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 452583104.4384 - val_loss: 559410225.9726\n",
      "Epoch 2616/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 432992407.6712 - val_loss: 585767159.2329\n",
      "Epoch 2617/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 427350534.7945 - val_loss: 596322126.2466\n",
      "Epoch 2618/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 446318584.3288 - val_loss: 567528656.7671\n",
      "Epoch 2619/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 434180008.7671 - val_loss: 598357950.2466\n",
      "Epoch 2620/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 436924193.8630 - val_loss: 586184417.7534\n",
      "Epoch 2621/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 432633926.5753 - val_loss: 576502931.1781\n",
      "Epoch 2622/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 430759944.7671 - val_loss: 579647748.3836\n",
      "Epoch 2623/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 443606378.5205 - val_loss: 565501017.8630\n",
      "Epoch 2624/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 471822439.6712 - val_loss: 590774890.9589\n",
      "Epoch 2625/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 439499863.6712 - val_loss: 576965797.8630\n",
      "Epoch 2626/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 436783216.8767 - val_loss: 567748906.1918\n",
      "Epoch 2627/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 432189632.6575 - val_loss: 562352958.4658\n",
      "Epoch 2628/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 429662257.0959 - val_loss: 590357227.9452\n",
      "Epoch 2629/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 451406061.5890 - val_loss: 563017777.5342\n",
      "Epoch 2630/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 423603877.2603 - val_loss: 567947878.0274\n",
      "Epoch 2631/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 420077463.8904 - val_loss: 581507776.2192\n",
      "Epoch 2632/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 449563264.8767 - val_loss: 569503751.4521\n",
      "Epoch 2633/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 462260592.6575 - val_loss: 656678139.8356\n",
      "Epoch 2634/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 456278459.6164 - val_loss: 620989336.7671\n",
      "Epoch 2635/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 450560581.0411 - val_loss: 562924157.6986\n",
      "Epoch 2636/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 429538447.1233 - val_loss: 584539623.4521\n",
      "Epoch 2637/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 453844632.5479 - val_loss: 560418290.8493\n",
      "Epoch 2638/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 437363479.8904 - val_loss: 611909736.5479\n",
      "Epoch 2639/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 429743377.5342 - val_loss: 586535219.0685\n",
      "Epoch 2640/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 436032252.4932 - val_loss: 558943692.1644\n",
      "Epoch 2641/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 423447785.6438 - val_loss: 591498457.7534\n",
      "Epoch 2642/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 438243252.1644 - val_loss: 567725191.2329\n",
      "Epoch 2643/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 470968318.6849 - val_loss: 597824363.3973\n",
      "Epoch 2644/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 440521031.8904 - val_loss: 573748522.8493\n",
      "Epoch 2645/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 425189664.0000 - val_loss: 574296511.6712\n",
      "Epoch 2646/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 424132522.0822 - val_loss: 595357835.1233\n",
      "Epoch 2647/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 427030566.4658 - val_loss: 582137373.2603\n",
      "Epoch 2648/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 426699856.0000 - val_loss: 599446116.1644\n",
      "Epoch 2649/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 442064859.8356 - val_loss: 572946411.9452\n",
      "Epoch 2650/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 449785818.9589 - val_loss: 601561199.4521\n",
      "Epoch 2651/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 432984505.8630 - val_loss: 580165661.3699\n",
      "Epoch 2652/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 423201105.9726 - val_loss: 585649695.3425\n",
      "Epoch 2653/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 432671627.6164 - val_loss: 560873874.7397\n",
      "Epoch 2654/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 436222129.5342 - val_loss: 633492655.3425\n",
      "Epoch 2655/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 440685779.7260 - val_loss: 617787658.3014\n",
      "Epoch 2656/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 474727001.8630 - val_loss: 666540222.2466\n",
      "Epoch 2657/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 460705963.6164 - val_loss: 574212231.6712\n",
      "Epoch 2658/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 443076149.0411 - val_loss: 556944567.5616\n",
      "Epoch 2659/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 431530520.5479 - val_loss: 572410247.4521\n",
      "Epoch 2660/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 427665758.4658 - val_loss: 580964098.4110\n",
      "Epoch 2661/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 433896073.2055 - val_loss: 576359313.5890\n",
      "Epoch 2662/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 459091592.3288 - val_loss: 561055733.9178\n",
      "Epoch 2663/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 434033538.1918 - val_loss: 550442131.9452\n",
      "Epoch 2664/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 414691703.6712 - val_loss: 590356820.3836\n",
      "Epoch 2665/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 433320667.3973 - val_loss: 570513782.7945\n",
      "Epoch 2666/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 432203515.1781 - val_loss: 574337586.6301\n",
      "Epoch 2667/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 436925903.3425 - val_loss: 634942372.8219\n",
      "Epoch 2668/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 506885524.6027 - val_loss: 668735912.9863\n",
      "Epoch 2669/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 420665367.0137 - val_loss: 581099129.6438\n",
      "Epoch 2670/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 424363055.7808 - val_loss: 579962072.3288\n",
      "Epoch 2671/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 421720557.5890 - val_loss: 583790523.8356\n",
      "Epoch 2672/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 421418708.6027 - val_loss: 604351766.1370\n",
      "Epoch 2673/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 420191201.9726 - val_loss: 578467029.3699\n",
      "Epoch 2674/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 434406595.0685 - val_loss: 631904101.9178\n",
      "Epoch 2675/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 435135237.9178 - val_loss: 582354481.6438\n",
      "Epoch 2676/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 440180129.3151 - val_loss: 617504751.5068\n",
      "Epoch 2677/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 441572795.1781 - val_loss: 618591556.3836\n",
      "Epoch 2678/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 424267488.4384 - val_loss: 579757178.5205\n",
      "Epoch 2679/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 433452598.3562 - val_loss: 579636691.1781\n",
      "Epoch 2680/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 415396924.9315 - val_loss: 608272179.5068\n",
      "Epoch 2681/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 425923982.0274 - val_loss: 562718837.1507\n",
      "Epoch 2682/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 425985381.2603 - val_loss: 598963749.2603\n",
      "Epoch 2683/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 419461288.5479 - val_loss: 577037758.6849\n",
      "Epoch 2684/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 436752750.4658 - val_loss: 561346553.0959\n",
      "Epoch 2685/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 419490467.9452 - val_loss: 588692338.1918\n",
      "Epoch 2686/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 423847901.8082 - val_loss: 587912046.2466\n",
      "Epoch 2687/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 424873862.1370 - val_loss: 587216320.7671\n",
      "Epoch 2688/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 440243628.7123 - val_loss: 583674306.1918\n",
      "Epoch 2689/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 438790659.0685 - val_loss: 574693829.0411\n",
      "Epoch 2690/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 439470867.8356 - val_loss: 579194695.2329\n",
      "Epoch 2691/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 424002985.6438 - val_loss: 562245286.3562\n",
      "Epoch 2692/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 438453338.5205 - val_loss: 596319498.3014\n",
      "Epoch 2693/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 416065224.3288 - val_loss: 603335193.8630\n",
      "Epoch 2694/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 418604905.6438 - val_loss: 603298334.4658\n",
      "Epoch 2695/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 436995869.8082 - val_loss: 584280243.7260\n",
      "Epoch 2696/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 419021846.7945 - val_loss: 574285923.6164\n",
      "Epoch 2697/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 423859523.0685 - val_loss: 569100690.3014\n",
      "Epoch 2698/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 415410101.4795 - val_loss: 590845653.8630\n",
      "Epoch 2699/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 504789072.6575 - val_loss: 651284141.1507\n",
      "Epoch 2700/3350\n",
      "1168/1168 [==============================] - 0s 86us/step - loss: 428942442.3014 - val_loss: 590337130.5205\n",
      "Epoch 2701/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 415930032.2192 - val_loss: 581647296.7671\n",
      "Epoch 2702/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 420808924.7123 - val_loss: 598198293.0411\n",
      "Epoch 2703/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 419849631.5616 - val_loss: 560101166.4658\n",
      "Epoch 2704/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 421464425.2055 - val_loss: 623049459.9452\n",
      "Epoch 2705/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 415689349.0411 - val_loss: 566903363.7260\n",
      "Epoch 2706/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 417873159.8904 - val_loss: 588916125.6986\n",
      "Epoch 2707/3350\n",
      "1168/1168 [==============================] - 0s 47us/step - loss: 417232793.4247 - val_loss: 638616373.0137\n",
      "Epoch 2708/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 431708049.9726 - val_loss: 594110773.5890\n",
      "Epoch 2709/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 422784396.2740 - val_loss: 600290811.5890\n",
      "Epoch 2710/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 446652943.5616 - val_loss: 557442097.8630\n",
      "Epoch 2711/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 418297560.1096 - val_loss: 578840782.9041\n",
      "Epoch 2712/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 436371725.3699 - val_loss: 584207559.5068\n",
      "Epoch 2713/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 419329959.8904 - val_loss: 587546947.0685\n",
      "Epoch 2714/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 411942848.6575 - val_loss: 595351951.7808\n",
      "Epoch 2715/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 429628544.8767 - val_loss: 629064742.7945\n",
      "Epoch 2716/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 429595168.0000 - val_loss: 579976658.4658\n",
      "Epoch 2717/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 427406357.6986 - val_loss: 627240984.5479\n",
      "Epoch 2718/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 421512110.0274 - val_loss: 577874362.3014\n",
      "Epoch 2719/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 429571500.7123 - val_loss: 598362902.7945\n",
      "Epoch 2720/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 438788130.1918 - val_loss: 567652250.8493\n",
      "Epoch 2721/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 419575291.3973 - val_loss: 556700820.9315\n",
      "Epoch 2722/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 425280049.9726 - val_loss: 581895669.1507\n",
      "Epoch 2723/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 444402384.6575 - val_loss: 563977614.6849\n",
      "Epoch 2724/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 421365821.3699 - val_loss: 582862555.8356\n",
      "Epoch 2725/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 434143939.5068 - val_loss: 617573834.5205\n",
      "Epoch 2726/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 421193093.6986 - val_loss: 547826046.9041\n",
      "Epoch 2727/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 429574222.0274 - val_loss: 559608442.6301\n",
      "Epoch 2728/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 415578247.0137 - val_loss: 592828452.7671\n",
      "Epoch 2729/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 413414502.1370 - val_loss: 586443649.5342\n",
      "Epoch 2730/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 426744763.1781 - val_loss: 625785770.9589\n",
      "Epoch 2731/3350\n",
      "1168/1168 [==============================] - 0s 86us/step - loss: 418361219.5068 - val_loss: 569070280.9863\n",
      "Epoch 2732/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 413525342.6849 - val_loss: 573030628.1644\n",
      "Epoch 2733/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 406690290.1918 - val_loss: 617362968.5479\n",
      "Epoch 2734/3350\n",
      "1168/1168 [==============================] - 0s 84us/step - loss: 413286604.7123 - val_loss: 563835797.4795\n",
      "Epoch 2735/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 424823306.0822 - val_loss: 568902325.3699\n",
      "Epoch 2736/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 410084083.9452 - val_loss: 599181280.0000\n",
      "Epoch 2737/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 447932862.2466 - val_loss: 576280472.5479\n",
      "Epoch 2738/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 433173734.1370 - val_loss: 569728692.7123\n",
      "Epoch 2739/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 435762163.7260 - val_loss: 618554790.3288\n",
      "Epoch 2740/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 475248680.3288 - val_loss: 572779285.8082\n",
      "Epoch 2741/3350\n",
      "1168/1168 [==============================] - 0s 80us/step - loss: 418326212.1644 - val_loss: 565984006.4658\n",
      "Epoch 2742/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 457841601.5342 - val_loss: 554234566.5205\n",
      "Epoch 2743/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 415801670.7945 - val_loss: 598815696.8219\n",
      "Epoch 2744/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 424092524.7123 - val_loss: 611480310.8767\n",
      "Epoch 2745/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 412610319.3425 - val_loss: 598715344.7671\n",
      "Epoch 2746/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 412292993.3151 - val_loss: 599730366.5753\n",
      "Epoch 2747/3350\n",
      "1168/1168 [==============================] - 0s 94us/step - loss: 403694478.4658 - val_loss: 609677934.3288\n",
      "Epoch 2748/3350\n",
      "1168/1168 [==============================] - 0s 81us/step - loss: 410394316.7123 - val_loss: 590397897.4795\n",
      "Epoch 2749/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 422647902.2466 - val_loss: 607585331.5068\n",
      "Epoch 2750/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 418157239.4521 - val_loss: 588819327.7808\n",
      "Epoch 2751/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 409850866.8493 - val_loss: 584114619.1781\n",
      "Epoch 2752/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 403292794.7397 - val_loss: 563330491.0685\n",
      "Epoch 2753/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 413702036.6027 - val_loss: 588705969.6438\n",
      "Epoch 2754/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 414218478.0274 - val_loss: 581531893.4795\n",
      "Epoch 2755/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 403715569.9726 - val_loss: 559038089.9726\n",
      "Epoch 2756/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 426718913.0959 - val_loss: 561539148.6027\n",
      "Epoch 2757/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 427590740.6027 - val_loss: 609513215.3425\n",
      "Epoch 2758/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 405643655.6712 - val_loss: 592936016.0000\n",
      "Epoch 2759/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 416819970.1918 - val_loss: 566360272.9863\n",
      "Epoch 2760/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 405684916.6027 - val_loss: 580600027.5068\n",
      "Epoch 2761/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 405397782.3562 - val_loss: 566965987.1781\n",
      "Epoch 2762/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 420756740.8219 - val_loss: 595719328.1096\n",
      "Epoch 2763/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 430232732.9315 - val_loss: 563336666.4110\n",
      "Epoch 2764/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 413067589.2603 - val_loss: 596840342.6849\n",
      "Epoch 2765/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 413164919.4521 - val_loss: 588298070.7945\n",
      "Epoch 2766/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 414980956.9315 - val_loss: 565447915.5068\n",
      "Epoch 2767/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 428439242.3014 - val_loss: 572128414.7397\n",
      "Epoch 2768/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 462349742.4658 - val_loss: 590323638.8493\n",
      "Epoch 2769/3350\n",
      "1168/1168 [==============================] - 0s 81us/step - loss: 474914453.4795 - val_loss: 633161220.8219\n",
      "Epoch 2770/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 448120008.7671 - val_loss: 604307340.7123\n",
      "Epoch 2771/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 409676099.7260 - val_loss: 584687096.4384\n",
      "Epoch 2772/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 414961564.9315 - val_loss: 561393643.6164\n",
      "Epoch 2773/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 417504468.1644 - val_loss: 585684088.8219\n",
      "Epoch 2774/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 400251635.5068 - val_loss: 632416561.9726\n",
      "Epoch 2775/3350\n",
      "1168/1168 [==============================] - 0s 57us/step - loss: 411135919.1233 - val_loss: 586003961.3151\n",
      "Epoch 2776/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 411937309.8082 - val_loss: 565015394.4110\n",
      "Epoch 2777/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 406585006.4658 - val_loss: 580089008.1096\n",
      "Epoch 2778/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 399748834.8493 - val_loss: 585344530.0822\n",
      "Epoch 2779/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 415333697.6438 - val_loss: 585237095.4521\n",
      "Epoch 2780/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 408218717.3699 - val_loss: 575377700.9315\n",
      "Epoch 2781/3350\n",
      "1168/1168 [==============================] - 0s 83us/step - loss: 403527042.6301 - val_loss: 573893281.0411\n",
      "Epoch 2782/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 405316852.6027 - val_loss: 631350177.5342\n",
      "Epoch 2783/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 418537754.3014 - val_loss: 591960291.5068\n",
      "Epoch 2784/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 419355658.9589 - val_loss: 577228694.7945\n",
      "Epoch 2785/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 435679318.3562 - val_loss: 593168789.5890\n",
      "Epoch 2786/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 442733889.7534 - val_loss: 665855527.0137\n",
      "Epoch 2787/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 406302932.6027 - val_loss: 568664406.3562\n",
      "Epoch 2788/3350\n",
      "1168/1168 [==============================] - 0s 56us/step - loss: 427107249.9726 - val_loss: 711395868.4932\n",
      "Epoch 2789/3350\n",
      "1168/1168 [==============================] - 0s 82us/step - loss: 415090142.6849 - val_loss: 561369530.0822\n",
      "Epoch 2790/3350\n",
      "1168/1168 [==============================] - 0s 83us/step - loss: 410215504.2192 - val_loss: 581320809.7534\n",
      "Epoch 2791/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 416622297.4247 - val_loss: 576060007.7534\n",
      "Epoch 2792/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 422408656.6575 - val_loss: 572789893.1507\n",
      "Epoch 2793/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 407100427.6164 - val_loss: 607014944.0000\n",
      "Epoch 2794/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 411662433.3151 - val_loss: 632582284.0548\n",
      "Epoch 2795/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 406174351.7808 - val_loss: 629471465.3151\n",
      "Epoch 2796/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 415425705.6438 - val_loss: 622580553.4247\n",
      "Epoch 2797/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 405918565.2603 - val_loss: 593995424.5479\n",
      "Epoch 2798/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 413380816.6575 - val_loss: 666365263.1233\n",
      "Epoch 2799/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 412214908.4932 - val_loss: 597981942.4110\n",
      "Epoch 2800/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 401164606.4658 - val_loss: 576679443.5068\n",
      "Epoch 2801/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 402757895.4521 - val_loss: 611706981.1507\n",
      "Epoch 2802/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 405312304.6575 - val_loss: 602099734.8493\n",
      "Epoch 2803/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 428351579.1781 - val_loss: 652402651.8904\n",
      "Epoch 2804/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 447443791.3425 - val_loss: 653864134.5753\n",
      "Epoch 2805/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 419766505.4247 - val_loss: 582584833.2603\n",
      "Epoch 2806/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 441475105.3151 - val_loss: 664850510.7945\n",
      "Epoch 2807/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 405161809.5342 - val_loss: 591636321.3699\n",
      "Epoch 2808/3350\n",
      "1168/1168 [==============================] - 0s 51us/step - loss: 402614571.3973 - val_loss: 606179412.8219\n",
      "Epoch 2809/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 414534584.1096 - val_loss: 554938508.8219\n",
      "Epoch 2810/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 434750532.8219 - val_loss: 603820002.8493\n",
      "Epoch 2811/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 408131492.8219 - val_loss: 587201877.2603\n",
      "Epoch 2812/3350\n",
      "1168/1168 [==============================] - 0s 83us/step - loss: 410075360.4384 - val_loss: 610639019.6164\n",
      "Epoch 2813/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 409491771.6164 - val_loss: 593740637.9178\n",
      "Epoch 2814/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 400257021.8082 - val_loss: 600598785.2055\n",
      "Epoch 2815/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 426878955.3973 - val_loss: 575894962.6027\n",
      "Epoch 2816/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 406157229.5890 - val_loss: 605959422.6849\n",
      "Epoch 2817/3350\n",
      "1168/1168 [==============================] - 0s 52us/step - loss: 404640927.1233 - val_loss: 648577125.2603\n",
      "Epoch 2818/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 430113159.0137 - val_loss: 605328511.6712\n",
      "Epoch 2819/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 401782631.8904 - val_loss: 611270921.2055\n",
      "Epoch 2820/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 409259406.9041 - val_loss: 608237529.4247\n",
      "Epoch 2821/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 417561219.7260 - val_loss: 602095512.1096\n",
      "Epoch 2822/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 412544623.1233 - val_loss: 699968140.6575\n",
      "Epoch 2823/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 443837643.6164 - val_loss: 590667975.2329\n",
      "Epoch 2824/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 411134975.1233 - val_loss: 612881892.4384\n",
      "Epoch 2825/3350\n",
      "1168/1168 [==============================] - 0s 81us/step - loss: 413833484.0548 - val_loss: 589871250.5205\n",
      "Epoch 2826/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 427032296.7671 - val_loss: 591332002.0822\n",
      "Epoch 2827/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 392838605.2603 - val_loss: 612577868.2192\n",
      "Epoch 2828/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 417439801.8630 - val_loss: 594511668.2192\n",
      "Epoch 2829/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 395534243.7260 - val_loss: 606072270.1370\n",
      "Epoch 2830/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 401794233.4247 - val_loss: 606204714.8493\n",
      "Epoch 2831/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 398990504.7671 - val_loss: 592963762.1370\n",
      "Epoch 2832/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 404588052.3836 - val_loss: 629719906.8493\n",
      "Epoch 2833/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 421821301.0411 - val_loss: 629177567.8082\n",
      "Epoch 2834/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 425018609.9726 - val_loss: 604196209.8630\n",
      "Epoch 2835/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 413107992.5479 - val_loss: 588957563.2877\n",
      "Epoch 2836/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 397105660.0548 - val_loss: 605331146.8493\n",
      "Epoch 2837/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 395983883.6164 - val_loss: 589122409.4247\n",
      "Epoch 2838/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 421834392.9863 - val_loss: 588160601.2055\n",
      "Epoch 2839/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 399532346.5205 - val_loss: 574765097.4247\n",
      "Epoch 2840/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 402619295.1233 - val_loss: 585684092.2740\n",
      "Epoch 2841/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 415425824.8767 - val_loss: 596771345.9178\n",
      "Epoch 2842/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 398025344.4384 - val_loss: 587456395.9452\n",
      "Epoch 2843/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 402946453.2603 - val_loss: 623514851.0685\n",
      "Epoch 2844/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 426889600.4384 - val_loss: 620704483.7808\n",
      "Epoch 2845/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 521812693.0411 - val_loss: 614361800.9863\n",
      "Epoch 2846/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 545493580.2740 - val_loss: 569145753.0959\n",
      "Epoch 2847/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 430018387.0685 - val_loss: 589882333.0411\n",
      "Epoch 2848/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 428502222.0274 - val_loss: 614695134.9041\n",
      "Epoch 2849/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 401482825.6438 - val_loss: 619046544.8767\n",
      "Epoch 2850/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 406720877.3699 - val_loss: 589061409.7534\n",
      "Epoch 2851/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 395325596.0548 - val_loss: 559373494.9041\n",
      "Epoch 2852/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 410281603.9452 - val_loss: 616150938.3014\n",
      "Epoch 2853/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 394651764.6027 - val_loss: 609634825.8630\n",
      "Epoch 2854/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 402070196.1644 - val_loss: 605229047.0685\n",
      "Epoch 2855/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 407995716.8219 - val_loss: 593784771.1781\n",
      "Epoch 2856/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 405253226.0822 - val_loss: 593539939.8356\n",
      "Epoch 2857/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 394626739.7260 - val_loss: 611444095.5068\n",
      "Epoch 2858/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 397820148.6027 - val_loss: 593956941.5890\n",
      "Epoch 2859/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 396693394.4110 - val_loss: 597676325.9726\n",
      "Epoch 2860/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 430299551.0137 - val_loss: 586312406.9041\n",
      "Epoch 2861/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 399605394.8493 - val_loss: 621024584.9863\n",
      "Epoch 2862/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 401374612.8219 - val_loss: 587478352.9315\n",
      "Epoch 2863/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 398754895.7808 - val_loss: 603558059.1781\n",
      "Epoch 2864/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 393692258.1918 - val_loss: 619555284.1096\n",
      "Epoch 2865/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 415409242.9589 - val_loss: 581004391.3425\n",
      "Epoch 2866/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 406336756.6027 - val_loss: 570133251.7808\n",
      "Epoch 2867/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 395617851.6164 - val_loss: 622983068.9315\n",
      "Epoch 2868/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 404390874.9589 - val_loss: 599660178.7397\n",
      "Epoch 2869/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 391806611.2877 - val_loss: 621403606.4658\n",
      "Epoch 2870/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 394118065.5342 - val_loss: 602211503.1781\n",
      "Epoch 2871/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 410637627.1781 - val_loss: 593791092.0000\n",
      "Epoch 2872/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 393004506.3014 - val_loss: 591430125.3699\n",
      "Epoch 2873/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 389131013.2603 - val_loss: 610390589.3699\n",
      "Epoch 2874/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 389433820.0548 - val_loss: 597592370.1918\n",
      "Epoch 2875/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 410599862.7945 - val_loss: 628854635.2877\n",
      "Epoch 2876/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 413677642.0822 - val_loss: 637447477.0685\n",
      "Epoch 2877/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 440683762.4110 - val_loss: 681062907.4795\n",
      "Epoch 2878/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 415130467.0685 - val_loss: 616251174.9178\n",
      "Epoch 2879/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 400734897.3151 - val_loss: 594562370.4658\n",
      "Epoch 2880/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 402111850.5205 - val_loss: 599042110.7397\n",
      "Epoch 2881/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 395120370.8493 - val_loss: 629582819.9452\n",
      "Epoch 2882/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 389719427.0685 - val_loss: 616266054.5753\n",
      "Epoch 2883/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 396837283.9452 - val_loss: 633688923.1781\n",
      "Epoch 2884/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 428183395.9452 - val_loss: 597264488.4384\n",
      "Epoch 2885/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 385709264.6575 - val_loss: 611891724.1644\n",
      "Epoch 2886/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 389452931.2877 - val_loss: 595662078.2466\n",
      "Epoch 2887/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 389045592.1096 - val_loss: 610560698.5205\n",
      "Epoch 2888/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 391964505.2055 - val_loss: 639718740.3836\n",
      "Epoch 2889/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 408069244.9315 - val_loss: 679950338.4110\n",
      "Epoch 2890/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 400339022.4658 - val_loss: 631362024.1370\n",
      "Epoch 2891/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 400524755.7260 - val_loss: 601122386.4110\n",
      "Epoch 2892/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 390036121.8630 - val_loss: 634704407.0137\n",
      "Epoch 2893/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 398262552.9863 - val_loss: 630513611.8356\n",
      "Epoch 2894/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 396309315.2877 - val_loss: 646797741.1507\n",
      "Epoch 2895/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 390522990.4658 - val_loss: 616806713.7534\n",
      "Epoch 2896/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 393441390.2466 - val_loss: 580168179.8356\n",
      "Epoch 2897/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 415963564.2740 - val_loss: 602973582.7945\n",
      "Epoch 2898/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 392256126.2466 - val_loss: 607271222.5753\n",
      "Epoch 2899/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 406759619.9452 - val_loss: 620594839.6712\n",
      "Epoch 2900/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 425699658.9589 - val_loss: 608706055.0137\n",
      "Epoch 2901/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 398897813.9178 - val_loss: 615677805.6438\n",
      "Epoch 2902/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 451144911.7808 - val_loss: 597664276.6575\n",
      "Epoch 2903/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 479320833.7534 - val_loss: 744245731.5068\n",
      "Epoch 2904/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 493482527.1233 - val_loss: 621769251.0685\n",
      "Epoch 2905/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 447365637.6986 - val_loss: 611611900.8219\n",
      "Epoch 2906/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 400056119.0137 - val_loss: 616891693.1507\n",
      "Epoch 2907/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 391941579.8356 - val_loss: 585490799.8356\n",
      "Epoch 2908/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 417363341.1507 - val_loss: 605166864.5479\n",
      "Epoch 2909/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 393482074.9589 - val_loss: 607271887.5616\n",
      "Epoch 2910/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 384360466.4110 - val_loss: 604204632.9315\n",
      "Epoch 2911/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 424978344.1096 - val_loss: 596491656.1096\n",
      "Epoch 2912/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 395968880.8767 - val_loss: 699844425.8630\n",
      "Epoch 2913/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 393777500.7123 - val_loss: 662491820.8219\n",
      "Epoch 2914/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 397386129.9726 - val_loss: 653795360.0000\n",
      "Epoch 2915/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 400440984.5479 - val_loss: 618844025.8630\n",
      "Epoch 2916/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 387867908.3836 - val_loss: 609816516.8219\n",
      "Epoch 2917/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 394967463.8904 - val_loss: 660398422.7945\n",
      "Epoch 2918/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 390451541.9178 - val_loss: 591537801.8630\n",
      "Epoch 2919/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 386643302.5753 - val_loss: 622328329.1507\n",
      "Epoch 2920/3350\n",
      "1168/1168 [==============================] - 0s 49us/step - loss: 407838698.0822 - val_loss: 592272338.9589\n",
      "Epoch 2921/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 384748457.4247 - val_loss: 611170467.0685\n",
      "Epoch 2922/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 386278575.7808 - val_loss: 627132461.2740\n",
      "Epoch 2923/3350\n",
      "1168/1168 [==============================] - 0s 51us/step - loss: 407649080.5479 - val_loss: 615047240.8493\n",
      "Epoch 2924/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 407275847.0137 - val_loss: 610102122.1918\n",
      "Epoch 2925/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 401288377.8630 - val_loss: 596516299.7808\n",
      "Epoch 2926/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 389993989.0411 - val_loss: 590007978.5205\n",
      "Epoch 2927/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 401503547.6164 - val_loss: 597094601.3425\n",
      "Epoch 2928/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 382592992.0000 - val_loss: 602696144.1096\n",
      "Epoch 2929/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 391582428.0548 - val_loss: 621276342.7671\n",
      "Epoch 2930/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 384884561.9726 - val_loss: 626560425.4247\n",
      "Epoch 2931/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 428485236.1644 - val_loss: 660015956.3699\n",
      "Epoch 2932/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 404952735.5616 - val_loss: 593348449.9726\n",
      "Epoch 2933/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 382149097.2055 - val_loss: 642610853.8082\n",
      "Epoch 2934/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 402745788.9315 - val_loss: 612576493.5342\n",
      "Epoch 2935/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 399898425.2055 - val_loss: 695047696.6575\n",
      "Epoch 2936/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 382882178.4110 - val_loss: 633960817.2055\n",
      "Epoch 2937/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 390075245.6986 - val_loss: 603848171.9452\n",
      "Epoch 2938/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 384534597.9178 - val_loss: 608051522.2192\n",
      "Epoch 2939/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 390124525.1507 - val_loss: 608821084.1644\n",
      "Epoch 2940/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 388455795.1781 - val_loss: 660173120.4384\n",
      "Epoch 2941/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 426270755.5068 - val_loss: 651061409.0959\n",
      "Epoch 2942/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 441978635.8356 - val_loss: 604579199.7808\n",
      "Epoch 2943/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 422397866.3014 - val_loss: 605116704.0411\n",
      "Epoch 2944/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 400831374.0274 - val_loss: 611685955.1781\n",
      "Epoch 2945/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 378663755.8356 - val_loss: 625163791.3425\n",
      "Epoch 2946/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 382511131.1781 - val_loss: 665887568.3493\n",
      "Epoch 2947/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 403792201.6438 - val_loss: 628274432.4384\n",
      "Epoch 2948/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 395144802.6301 - val_loss: 622487936.4384\n",
      "Epoch 2949/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 387295641.8630 - val_loss: 617563923.0685\n",
      "Epoch 2950/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 381033751.2329 - val_loss: 617610371.2877\n",
      "Epoch 2951/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 389609228.4932 - val_loss: 596902412.2740\n",
      "Epoch 2952/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 388569460.1644 - val_loss: 614673949.5890\n",
      "Epoch 2953/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 383185791.1233 - val_loss: 609467217.6438\n",
      "Epoch 2954/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 388402976.8767 - val_loss: 606539322.6849\n",
      "Epoch 2955/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 408760556.7123 - val_loss: 640517499.3973\n",
      "Epoch 2956/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 385831945.7534 - val_loss: 594942849.9726\n",
      "Epoch 2957/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 381401857.3151 - val_loss: 626314859.0685\n",
      "Epoch 2958/3350\n",
      "1168/1168 [==============================] - 0s 50us/step - loss: 386261011.2877 - val_loss: 609957910.2466\n",
      "Epoch 2959/3350\n",
      "1168/1168 [==============================] - 0s 83us/step - loss: 410058371.0685 - val_loss: 753885383.8904\n",
      "Epoch 2960/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 429548978.4110 - val_loss: 594388246.7397\n",
      "Epoch 2961/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 376614463.1233 - val_loss: 605636238.2466\n",
      "Epoch 2962/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 416440515.5068 - val_loss: 618382259.4521\n",
      "Epoch 2963/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 384216588.9315 - val_loss: 616034455.4521\n",
      "Epoch 2964/3350\n",
      "1168/1168 [==============================] - 0s 80us/step - loss: 382266168.9863 - val_loss: 622426846.6849\n",
      "Epoch 2965/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 377707406.9041 - val_loss: 639215007.3151\n",
      "Epoch 2966/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 420871552.8767 - val_loss: 592809810.9589\n",
      "Epoch 2967/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 386630143.5616 - val_loss: 636070846.6849\n",
      "Epoch 2968/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 378158114.1918 - val_loss: 613042380.6027\n",
      "Epoch 2969/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 388047627.8356 - val_loss: 690046828.7123\n",
      "Epoch 2970/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 392790047.1233 - val_loss: 610344336.3288\n",
      "Epoch 2971/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 380563361.5342 - val_loss: 597765275.8356\n",
      "Epoch 2972/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 379608552.3288 - val_loss: 614263181.7260\n",
      "Epoch 2973/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 401114825.6438 - val_loss: 620038120.6575\n",
      "Epoch 2974/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 387580151.2329 - val_loss: 622908456.1096\n",
      "Epoch 2975/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 383641399.6712 - val_loss: 636845983.6712\n",
      "Epoch 2976/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 386618899.7260 - val_loss: 608391494.3014\n",
      "Epoch 2977/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 382079294.9041 - val_loss: 616154866.9589\n",
      "Epoch 2978/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 381487874.1918 - val_loss: 625611100.8767\n",
      "Epoch 2979/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 378141899.3973 - val_loss: 617383890.8493\n",
      "Epoch 2980/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 410025043.7260 - val_loss: 700014220.9315\n",
      "Epoch 2981/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 408411452.0548 - val_loss: 685470998.3562\n",
      "Epoch 2982/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 410366087.4521 - val_loss: 638113785.4110\n",
      "Epoch 2983/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 435736204.2740 - val_loss: 586297513.2055\n",
      "Epoch 2984/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 377385249.3151 - val_loss: 621833074.3014\n",
      "Epoch 2985/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 376071644.2740 - val_loss: 614846279.3425\n",
      "Epoch 2986/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 373332683.1781 - val_loss: 603643048.0000\n",
      "Epoch 2987/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 372383073.7534 - val_loss: 618109215.2329\n",
      "Epoch 2988/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 385369446.5753 - val_loss: 611246632.9041\n",
      "Epoch 2989/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 394514507.1781 - val_loss: 608418565.0959\n",
      "Epoch 2990/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 385012646.6849 - val_loss: 621784126.9041\n",
      "Epoch 2991/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 383791900.4932 - val_loss: 653906059.8356\n",
      "Epoch 2992/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 381766003.5068 - val_loss: 606249309.1507\n",
      "Epoch 2993/3350\n",
      "1168/1168 [==============================] - 0s 48us/step - loss: 380024215.2329 - val_loss: 639965827.5068\n",
      "Epoch 2994/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 389271911.8904 - val_loss: 632051909.1507\n",
      "Epoch 2995/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 375560878.0274 - val_loss: 620513015.0137\n",
      "Epoch 2996/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 379446387.7260 - val_loss: 613974071.2329\n",
      "Epoch 2997/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 373648781.3699 - val_loss: 639356944.4384\n",
      "Epoch 2998/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 379874049.0959 - val_loss: 601926264.8767\n",
      "Epoch 2999/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 395199805.3699 - val_loss: 646244885.2603\n",
      "Epoch 3000/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 394703180.7123 - val_loss: 643222876.2740\n",
      "Epoch 3001/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 409341701.3699 - val_loss: 584280291.3973\n",
      "Epoch 3002/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 372689731.0685 - val_loss: 594148516.8767\n",
      "Epoch 3003/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 390879675.6164 - val_loss: 612901158.3562\n",
      "Epoch 3004/3350\n",
      "1168/1168 [==============================] - 0s 48us/step - loss: 382400765.3699 - val_loss: 577066190.5753\n",
      "Epoch 3005/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 392085483.8356 - val_loss: 617072319.4521\n",
      "Epoch 3006/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 391468839.8904 - val_loss: 583877758.4658\n",
      "Epoch 3007/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 419942888.9863 - val_loss: 606715596.6027\n",
      "Epoch 3008/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 388930579.7260 - val_loss: 600557322.4658\n",
      "Epoch 3009/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 377465240.9863 - val_loss: 592422257.9178\n",
      "Epoch 3010/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 384150739.5068 - val_loss: 585507126.1918\n",
      "Epoch 3011/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 382213899.8356 - val_loss: 618357693.9726\n",
      "Epoch 3012/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 366614109.3699 - val_loss: 612782004.9863\n",
      "Epoch 3013/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 397893162.0822 - val_loss: 584601364.1096\n",
      "Epoch 3014/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 368592038.3562 - val_loss: 619129543.2740\n",
      "Epoch 3015/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 388853703.4521 - val_loss: 590538857.3699\n",
      "Epoch 3016/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 378135500.2740 - val_loss: 594222562.3014\n",
      "Epoch 3017/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 382034759.0137 - val_loss: 607755834.4658\n",
      "Epoch 3018/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 390673735.4521 - val_loss: 622150761.0000\n",
      "Epoch 3019/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 386487551.1233 - val_loss: 586910668.7123\n",
      "Epoch 3020/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 453987363.0685 - val_loss: 645403143.4521\n",
      "Epoch 3021/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 392975746.1918 - val_loss: 605750431.1233\n",
      "Epoch 3022/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 373100216.9863 - val_loss: 588781044.5205\n",
      "Epoch 3023/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 384869093.2603 - val_loss: 588449790.8493\n",
      "Epoch 3024/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 390687951.3425 - val_loss: 597402791.1233\n",
      "Epoch 3025/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 384275165.8082 - val_loss: 581632883.2877\n",
      "Epoch 3026/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 380107524.3836 - val_loss: 596890010.3014\n",
      "Epoch 3027/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 384243055.3425 - val_loss: 613063669.4795\n",
      "Epoch 3028/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 387997340.7123 - val_loss: 561178279.6164\n",
      "Epoch 3029/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 392629561.6438 - val_loss: 607085108.3836\n",
      "Epoch 3030/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 386078544.2192 - val_loss: 608580436.4932\n",
      "Epoch 3031/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 374334285.9178 - val_loss: 599074979.2877\n",
      "Epoch 3032/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 384465406.9041 - val_loss: 612332748.6027\n",
      "Epoch 3033/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 378695194.7397 - val_loss: 616807771.1370\n",
      "Epoch 3034/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 384368380.9315 - val_loss: 608832907.8356\n",
      "Epoch 3035/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 375129882.3014 - val_loss: 586849771.0137\n",
      "Epoch 3036/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 375129369.8630 - val_loss: 583299356.6027\n",
      "Epoch 3037/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 369024848.8767 - val_loss: 647016305.3151\n",
      "Epoch 3038/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 384293337.4247 - val_loss: 559742476.3562\n",
      "Epoch 3039/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 370528117.9178 - val_loss: 575207775.1233\n",
      "Epoch 3040/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 372752562.8493 - val_loss: 580993595.0685\n",
      "Epoch 3041/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 368671516.0548 - val_loss: 580657040.3288\n",
      "Epoch 3042/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 393626826.9589 - val_loss: 628218230.7397\n",
      "Epoch 3043/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 373173087.5616 - val_loss: 566527848.1096\n",
      "Epoch 3044/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 392163233.0959 - val_loss: 578375901.3151\n",
      "Epoch 3045/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 397956152.1096 - val_loss: 573218750.3562\n",
      "Epoch 3046/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 386718725.2603 - val_loss: 579631206.0822\n",
      "Epoch 3047/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 378277159.8904 - val_loss: 604100920.7671\n",
      "Epoch 3048/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 379936356.3836 - val_loss: 646602370.1233\n",
      "Epoch 3049/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 389505246.6849 - val_loss: 571994739.7260\n",
      "Epoch 3050/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 369610992.2192 - val_loss: 565608471.6712\n",
      "Epoch 3051/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 378639252.6027 - val_loss: 607492574.7808\n",
      "Epoch 3052/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 380132220.2740 - val_loss: 588690438.2466\n",
      "Epoch 3053/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 380253712.2192 - val_loss: 561906921.2055\n",
      "Epoch 3054/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 405834666.5205 - val_loss: 555340386.9452\n",
      "Epoch 3055/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 373691443.0685 - val_loss: 586356477.3767\n",
      "Epoch 3056/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 392913041.0959 - val_loss: 576606870.4110\n",
      "Epoch 3057/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 367615755.3973 - val_loss: 577502584.4384\n",
      "Epoch 3058/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 374738112.6575 - val_loss: 562898594.4110\n",
      "Epoch 3059/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 371205689.6438 - val_loss: 596814289.6438\n",
      "Epoch 3060/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 372249425.0959 - val_loss: 605747748.1644\n",
      "Epoch 3061/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 384490507.3973 - val_loss: 547213562.4384\n",
      "Epoch 3062/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 373009671.2329 - val_loss: 593527259.6164\n",
      "Epoch 3063/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 389468901.4795 - val_loss: 584848075.8356\n",
      "Epoch 3064/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 394543698.9589 - val_loss: 570993059.0959\n",
      "Epoch 3065/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 391382069.2603 - val_loss: 565853502.9041\n",
      "Epoch 3066/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 367056516.8219 - val_loss: 628679096.7671\n",
      "Epoch 3067/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 372420271.1233 - val_loss: 594310555.6438\n",
      "Epoch 3068/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 388558121.4247 - val_loss: 566730747.4521\n",
      "Epoch 3069/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 400989508.8219 - val_loss: 653643574.3562\n",
      "Epoch 3070/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 391167345.9726 - val_loss: 573959228.7123\n",
      "Epoch 3071/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 400310105.4247 - val_loss: 580597838.9589\n",
      "Epoch 3072/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 383345178.7397 - val_loss: 580209737.9726\n",
      "Epoch 3073/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 368803055.3425 - val_loss: 570087584.3836\n",
      "Epoch 3074/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 372581925.4795 - val_loss: 574108233.6712\n",
      "Epoch 3075/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 375658728.5479 - val_loss: 564515787.1781\n",
      "Epoch 3076/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 377611675.6164 - val_loss: 568843002.4110\n",
      "Epoch 3077/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 380475015.6712 - val_loss: 578992624.0000\n",
      "Epoch 3078/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 374098333.1507 - val_loss: 567481578.6301\n",
      "Epoch 3079/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 367209844.3836 - val_loss: 569376184.0000\n",
      "Epoch 3080/3350\n",
      "1168/1168 [==============================] - 0s 79us/step - loss: 373338001.4247 - val_loss: 576328697.4795\n",
      "Epoch 3081/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 448935170.1918 - val_loss: 560094160.5479\n",
      "Epoch 3082/3350\n",
      "1168/1168 [==============================] - 0s 87us/step - loss: 372229694.6849 - val_loss: 580692347.1781\n",
      "Epoch 3083/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 396224174.9041 - val_loss: 572292395.0685\n",
      "Epoch 3084/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 374675866.3014 - val_loss: 573319359.1233\n",
      "Epoch 3085/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 373939484.0548 - val_loss: 582506050.9589\n",
      "Epoch 3086/3350\n",
      "1168/1168 [==============================] - 0s 100us/step - loss: 361470582.1370 - val_loss: 618824832.1986\n",
      "Epoch 3087/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 394293516.9315 - val_loss: 574738503.5616\n",
      "Epoch 3088/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 367419196.0548 - val_loss: 582057551.0137\n",
      "Epoch 3089/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 390924765.1507 - val_loss: 548376591.7808\n",
      "Epoch 3090/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 403716020.9315 - val_loss: 551188294.2466\n",
      "Epoch 3091/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 372740858.7397 - val_loss: 590807978.9589\n",
      "Epoch 3092/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 367084670.2466 - val_loss: 569372269.8082\n",
      "Epoch 3093/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 397644305.9726 - val_loss: 648006538.3014\n",
      "Epoch 3094/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 388063966.2466 - val_loss: 574517437.5890\n",
      "Epoch 3095/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 386102331.1781 - val_loss: 568997186.7397\n",
      "Epoch 3096/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 362603083.3973 - val_loss: 586132147.8356\n",
      "Epoch 3097/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 358118389.9178 - val_loss: 581508372.3836\n",
      "Epoch 3098/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 428690771.7260 - val_loss: 604182414.0411\n",
      "Epoch 3099/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 399887868.0548 - val_loss: 589087609.3356\n",
      "Epoch 3100/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 364278674.8493 - val_loss: 585298848.4384\n",
      "Epoch 3101/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 368753781.0411 - val_loss: 543015887.6712\n",
      "Epoch 3102/3350\n",
      "1168/1168 [==============================] - 0s 79us/step - loss: 370778278.1370 - val_loss: 563714072.1096\n",
      "Epoch 3103/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 373986187.8356 - val_loss: 616822594.4589\n",
      "Epoch 3104/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 385316654.4658 - val_loss: 576354916.8767\n",
      "Epoch 3105/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 366416136.9863 - val_loss: 569272550.1918\n",
      "Epoch 3106/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 379249096.1096 - val_loss: 576439213.5342\n",
      "Epoch 3107/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 385384715.8356 - val_loss: 613883861.4795\n",
      "Epoch 3108/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 381135374.9041 - val_loss: 570538872.1096\n",
      "Epoch 3109/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 358298963.2877 - val_loss: 560013547.7260\n",
      "Epoch 3110/3350\n",
      "1168/1168 [==============================] - 0s 88us/step - loss: 369241721.5342 - val_loss: 560093988.6575\n",
      "Epoch 3111/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 376865689.8630 - val_loss: 566468641.9178\n",
      "Epoch 3112/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 390481196.2740 - val_loss: 536913594.7945\n",
      "Epoch 3113/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 380996603.6164 - val_loss: 572790651.4795\n",
      "Epoch 3114/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 370037338.3014 - val_loss: 546659551.3973\n",
      "Epoch 3115/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 368812851.2877 - val_loss: 542977729.7534\n",
      "Epoch 3116/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 414167363.9452 - val_loss: 597872967.8904\n",
      "Epoch 3117/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 372883013.2603 - val_loss: 597505470.3562\n",
      "Epoch 3118/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 376093249.3151 - val_loss: 548642353.2603\n",
      "Epoch 3119/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 371427162.3014 - val_loss: 597065709.8082\n",
      "Epoch 3120/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 362932666.3014 - val_loss: 562226649.4247\n",
      "Epoch 3121/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 360310718.0274 - val_loss: 564125046.5205\n",
      "Epoch 3122/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 371100177.9726 - val_loss: 557739009.4795\n",
      "Epoch 3123/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 368559118.0274 - val_loss: 558636189.2603\n",
      "Epoch 3124/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 376690466.6301 - val_loss: 539097339.7260\n",
      "Epoch 3125/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 437766706.0822 - val_loss: 574774745.5890\n",
      "Epoch 3126/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 375867222.7945 - val_loss: 607464750.7945\n",
      "Epoch 3127/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 362649607.0137 - val_loss: 585826237.9178\n",
      "Epoch 3128/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 365784978.6301 - val_loss: 618393862.1370\n",
      "Epoch 3129/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 384361798.1370 - val_loss: 569372236.1096\n",
      "Epoch 3130/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 378763192.7671 - val_loss: 562171268.0548\n",
      "Epoch 3131/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 363202435.0685 - val_loss: 571974696.0000\n",
      "Epoch 3132/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 365939405.8082 - val_loss: 555028584.0000\n",
      "Epoch 3133/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 365832774.7945 - val_loss: 584586605.0411\n",
      "Epoch 3134/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 356477566.6849 - val_loss: 582322619.4521\n",
      "Epoch 3135/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 391554116.6027 - val_loss: 590358106.1575\n",
      "Epoch 3136/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 379385251.3973 - val_loss: 560301072.0000\n",
      "Epoch 3137/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 365427631.3425 - val_loss: 586123972.1096\n",
      "Epoch 3138/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 375824353.7534 - val_loss: 546922347.7260\n",
      "Epoch 3139/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 361426424.1096 - val_loss: 553193193.2055\n",
      "Epoch 3140/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 366104881.5342 - val_loss: 555773664.7123\n",
      "Epoch 3141/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 369793856.0000 - val_loss: 580934387.0685\n",
      "Epoch 3142/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 363575627.8356 - val_loss: 566853059.6164\n",
      "Epoch 3143/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 392411075.7260 - val_loss: 587893966.0274\n",
      "Epoch 3144/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 361311555.7260 - val_loss: 557961212.1096\n",
      "Epoch 3145/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 361274461.4795 - val_loss: 587053214.0274\n",
      "Epoch 3146/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 378464693.9178 - val_loss: 589993903.4521\n",
      "Epoch 3147/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 355624396.4932 - val_loss: 565272128.7671\n",
      "Epoch 3148/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 370777496.3288 - val_loss: 553060210.7397\n",
      "Epoch 3149/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 361810948.1644 - val_loss: 572107724.8356\n",
      "Epoch 3150/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 353606184.3288 - val_loss: 556803738.6575\n",
      "Epoch 3151/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 367997834.5205 - val_loss: 559117651.5068\n",
      "Epoch 3152/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 363145776.6575 - val_loss: 555753751.2603\n",
      "Epoch 3153/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 370403251.7260 - val_loss: 564147954.9315\n",
      "Epoch 3154/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 367793762.1918 - val_loss: 564739729.4247\n",
      "Epoch 3155/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 356436002.4110 - val_loss: 551109909.3151\n",
      "Epoch 3156/3350\n",
      "1168/1168 [==============================] - 0s 55us/step - loss: 368462576.2192 - val_loss: 558145261.3151\n",
      "Epoch 3157/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 355119375.5616 - val_loss: 555067765.1507\n",
      "Epoch 3158/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 360346244.3836 - val_loss: 578110303.0137\n",
      "Epoch 3159/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 394900599.4521 - val_loss: 660243663.5685\n",
      "Epoch 3160/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 418379849.2055 - val_loss: 606694471.9452\n",
      "Epoch 3161/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 368778465.3151 - val_loss: 537976807.3425\n",
      "Epoch 3162/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 355085439.7808 - val_loss: 568739439.2877\n",
      "Epoch 3163/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 367859265.7534 - val_loss: 567086450.7397\n",
      "Epoch 3164/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 366881587.5068 - val_loss: 558544431.8904\n",
      "Epoch 3165/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 355775168.6575 - val_loss: 584639803.2740\n",
      "Epoch 3166/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 354845905.5342 - val_loss: 565479180.0548\n",
      "Epoch 3167/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 365628009.6438 - val_loss: 585794683.1507\n",
      "Epoch 3168/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 368307292.4932 - val_loss: 599132067.6164\n",
      "Epoch 3169/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 367050672.2192 - val_loss: 537946783.7808\n",
      "Epoch 3170/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 373332936.5479 - val_loss: 536695439.3425\n",
      "Epoch 3171/3350\n",
      "1168/1168 [==============================] - 0s 80us/step - loss: 370246984.3288 - val_loss: 564849338.7397\n",
      "Epoch 3172/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 380032486.1370 - val_loss: 569969473.5890\n",
      "Epoch 3173/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 359929513.2055 - val_loss: 556121088.6575\n",
      "Epoch 3174/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 360157417.6438 - val_loss: 576955768.4384\n",
      "Epoch 3175/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 366763188.8219 - val_loss: 553181456.5753\n",
      "Epoch 3176/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 366527865.8630 - val_loss: 588926717.4795\n",
      "Epoch 3177/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 355165961.4247 - val_loss: 553375742.4110\n",
      "Epoch 3178/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 366524602.5205 - val_loss: 578702824.3836\n",
      "Epoch 3179/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 355591910.1370 - val_loss: 566107675.1781\n",
      "Epoch 3180/3350\n",
      "1168/1168 [==============================] - 0s 103us/step - loss: 360924213.0411 - val_loss: 537481857.6438\n",
      "Epoch 3181/3350\n",
      "1168/1168 [==============================] - 0s 82us/step - loss: 370087925.9178 - val_loss: 557189063.4521\n",
      "Epoch 3182/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 356774808.9863 - val_loss: 555591900.0000\n",
      "Epoch 3183/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 381861945.6438 - val_loss: 617733039.7808\n",
      "Epoch 3184/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 366280910.9041 - val_loss: 550817073.9178\n",
      "Epoch 3185/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 364948982.3562 - val_loss: 635060172.0548\n",
      "Epoch 3186/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 356578101.0411 - val_loss: 557252280.7671\n",
      "Epoch 3187/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 362191766.7945 - val_loss: 587600196.1644\n",
      "Epoch 3188/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 383503211.3973 - val_loss: 562806783.9452\n",
      "Epoch 3189/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 358777154.6301 - val_loss: 571317847.3014\n",
      "Epoch 3190/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 373447534.0274 - val_loss: 541457089.4795\n",
      "Epoch 3191/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 362170003.2877 - val_loss: 545984092.3836\n",
      "Epoch 3192/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 358924462.4658 - val_loss: 544811346.4110\n",
      "Epoch 3193/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 350065020.0548 - val_loss: 581653188.8219\n",
      "Epoch 3194/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 356709159.8904 - val_loss: 580266642.1370\n",
      "Epoch 3195/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 370278133.0411 - val_loss: 586235291.7260\n",
      "Epoch 3196/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 370713480.7671 - val_loss: 613411068.9315\n",
      "Epoch 3197/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 386532668.0548 - val_loss: 590122764.7123\n",
      "Epoch 3198/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 381405904.2192 - val_loss: 556599523.7260\n",
      "Epoch 3199/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 358792846.6849 - val_loss: 555936517.0411\n",
      "Epoch 3200/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 374450368.8767 - val_loss: 580337239.2329\n",
      "Epoch 3201/3350\n",
      "1168/1168 [==============================] - 0s 85us/step - loss: 372498980.8219 - val_loss: 537232596.1644\n",
      "Epoch 3202/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 369133026.1918 - val_loss: 532460757.3699\n",
      "Epoch 3203/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 375699864.9863 - val_loss: 613269858.7397\n",
      "Epoch 3204/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 375740098.6301 - val_loss: 588275922.9589\n",
      "Epoch 3205/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 356210305.3151 - val_loss: 559989011.8904\n",
      "Epoch 3206/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 354601529.8630 - val_loss: 573699410.5753\n",
      "Epoch 3207/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 355512691.9452 - val_loss: 570537561.9726\n",
      "Epoch 3208/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 368832883.0685 - val_loss: 562721511.6164\n",
      "Epoch 3209/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 365596757.0411 - val_loss: 575797864.4384\n",
      "Epoch 3210/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 360381034.5205 - val_loss: 562128083.1781\n",
      "Epoch 3211/3350\n",
      "1168/1168 [==============================] - 0s 91us/step - loss: 349595118.9041 - val_loss: 566538042.9589\n",
      "Epoch 3212/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 370150474.5205 - val_loss: 629823478.3562\n",
      "Epoch 3213/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 367735892.1644 - val_loss: 548710823.3425\n",
      "Epoch 3214/3350\n",
      "1168/1168 [==============================] - 0s 75us/step - loss: 357736297.2055 - val_loss: 572888152.4658\n",
      "Epoch 3215/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 352683133.8082 - val_loss: 547417942.1370\n",
      "Epoch 3216/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 385520220.4932 - val_loss: 584927307.2329\n",
      "Epoch 3217/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 358124897.5342 - val_loss: 572365796.5479\n",
      "Epoch 3218/3350\n",
      "1168/1168 [==============================] - 0s 80us/step - loss: 354147237.2603 - val_loss: 577534141.6438\n",
      "Epoch 3219/3350\n",
      "1168/1168 [==============================] - 0s 77us/step - loss: 364411543.6712 - val_loss: 573478295.1233\n",
      "Epoch 3220/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 377400255.7808 - val_loss: 582965636.3836\n",
      "Epoch 3221/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 408015385.2055 - val_loss: 563247265.2603\n",
      "Epoch 3222/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 354810520.5479 - val_loss: 558320938.7397\n",
      "Epoch 3223/3350\n",
      "1168/1168 [==============================] - 0s 76us/step - loss: 353225684.6027 - val_loss: 567615990.4110\n",
      "Epoch 3224/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 355845404.0548 - val_loss: 570025435.5068\n",
      "Epoch 3225/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 359015807.1233 - val_loss: 569586197.8356\n",
      "Epoch 3226/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 363166160.4384 - val_loss: 543030729.2877\n",
      "Epoch 3227/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 359639627.8356 - val_loss: 642384373.4795\n",
      "Epoch 3228/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 385385476.8219 - val_loss: 565560547.0685\n",
      "Epoch 3229/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 355661351.0137 - val_loss: 585866857.4521\n",
      "Epoch 3230/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 404545515.8356 - val_loss: 631546694.7945\n",
      "Epoch 3231/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 376834489.4247 - val_loss: 566037556.6027\n",
      "Epoch 3232/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 355167729.3151 - val_loss: 558550170.7397\n",
      "Epoch 3233/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 378291725.1507 - val_loss: 585306827.8356\n",
      "Epoch 3234/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 362917674.5205 - val_loss: 561105183.6712\n",
      "Epoch 3235/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 365915858.8493 - val_loss: 580967700.8219\n",
      "Epoch 3236/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 360279978.7397 - val_loss: 593961266.7397\n",
      "Epoch 3237/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 350704136.9863 - val_loss: 601725796.7534\n",
      "Epoch 3238/3350\n",
      "1168/1168 [==============================] - 0s 65us/step - loss: 354258206.6849 - val_loss: 568644533.3699\n",
      "Epoch 3239/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 350157226.9589 - val_loss: 540084201.4247\n",
      "Epoch 3240/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 362947787.3973 - val_loss: 563863672.7123\n",
      "Epoch 3241/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 368461339.3973 - val_loss: 546468507.8904\n",
      "Epoch 3242/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 348866153.4247 - val_loss: 598296369.2055\n",
      "Epoch 3243/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 394965925.4795 - val_loss: 554372178.5274\n",
      "Epoch 3244/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 371551732.1644 - val_loss: 602407815.5616\n",
      "Epoch 3245/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 363462282.0822 - val_loss: 569664808.8219\n",
      "Epoch 3246/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 358809320.1096 - val_loss: 570165775.8904\n",
      "Epoch 3247/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 352460508.0548 - val_loss: 554296268.0685\n",
      "Epoch 3248/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 354751519.3425 - val_loss: 553903663.1233\n",
      "Epoch 3249/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 357011005.5890 - val_loss: 545362117.2603\n",
      "Epoch 3250/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 355439899.1781 - val_loss: 552513311.3425\n",
      "Epoch 3251/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 365237434.3014 - val_loss: 613090678.9041\n",
      "Epoch 3252/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 359761813.4795 - val_loss: 549582722.0822\n",
      "Epoch 3253/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 358859538.6301 - val_loss: 557287628.0000\n",
      "Epoch 3254/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 371513587.9452 - val_loss: 637747163.3973\n",
      "Epoch 3255/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 354646128.7671 - val_loss: 598752974.2466\n",
      "Epoch 3256/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 349600489.6438 - val_loss: 575156181.2329\n",
      "Epoch 3257/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 374532007.4521 - val_loss: 571551629.3699\n",
      "Epoch 3258/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 344080414.6849 - val_loss: 574617511.1233\n",
      "Epoch 3259/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 352012565.2603 - val_loss: 581759666.7671\n",
      "Epoch 3260/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 388447846.5753 - val_loss: 575422995.8356\n",
      "Epoch 3261/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 366238450.1918 - val_loss: 576568752.8219\n",
      "Epoch 3262/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 349533634.8493 - val_loss: 563790535.0137\n",
      "Epoch 3263/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 356555706.5205 - val_loss: 596682180.2192\n",
      "Epoch 3264/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 357774329.4247 - val_loss: 563735772.1644\n",
      "Epoch 3265/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 341076957.8082 - val_loss: 564648103.6712\n",
      "Epoch 3266/3350\n",
      "1168/1168 [==============================] - 0s 80us/step - loss: 353539105.7534 - val_loss: 586990845.0411\n",
      "Epoch 3267/3350\n",
      "1168/1168 [==============================] - 0s 47us/step - loss: 348087674.7397 - val_loss: 581902267.1233\n",
      "Epoch 3268/3350\n",
      "1168/1168 [==============================] - 0s 84us/step - loss: 359618055.2329 - val_loss: 621587639.5616\n",
      "Epoch 3269/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 349942594.9589 - val_loss: 569193120.3562\n",
      "Epoch 3270/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 347049638.1370 - val_loss: 591906749.5890\n",
      "Epoch 3271/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 366842679.6712 - val_loss: 543038797.3151\n",
      "Epoch 3272/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 409577357.3699 - val_loss: 546311937.0959\n",
      "Epoch 3273/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 363945187.5068 - val_loss: 554300671.2877\n",
      "Epoch 3274/3350\n",
      "1168/1168 [==============================] - 0s 70us/step - loss: 356901559.4521 - val_loss: 599926724.0548\n",
      "Epoch 3275/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 349963585.0959 - val_loss: 583896236.0000\n",
      "Epoch 3276/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 341437873.3151 - val_loss: 565052294.7397\n",
      "Epoch 3277/3350\n",
      "1168/1168 [==============================] - 0s 78us/step - loss: 368348050.4110 - val_loss: 560245609.6438\n",
      "Epoch 3278/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 359567864.7671 - val_loss: 566799268.0548\n",
      "Epoch 3279/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 343523540.3836 - val_loss: 571900542.3562\n",
      "Epoch 3280/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 356881578.9589 - val_loss: 550407880.2329\n",
      "Epoch 3281/3350\n",
      "1168/1168 [==============================] - 0s 72us/step - loss: 365953556.1644 - val_loss: 542942398.7397\n",
      "Epoch 3282/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 359286400.6575 - val_loss: 592029233.7534\n",
      "Epoch 3283/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 353543573.0411 - val_loss: 602869303.9452\n",
      "Epoch 3284/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 346386365.5890 - val_loss: 604488033.3699\n",
      "Epoch 3285/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 359264971.3973 - val_loss: 576486885.3973\n",
      "Epoch 3286/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 351067884.2740 - val_loss: 563640363.5068\n",
      "Epoch 3287/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 348701911.8904 - val_loss: 570747816.5342\n",
      "Epoch 3288/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 364075690.0822 - val_loss: 579979593.8356\n",
      "Epoch 3289/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 349234247.0137 - val_loss: 592431215.5822\n",
      "Epoch 3290/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 375023466.9589 - val_loss: 545696576.8219\n",
      "Epoch 3291/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 364581114.8493 - val_loss: 558161545.4795\n",
      "Epoch 3292/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 365479635.9452 - val_loss: 604931911.5616\n",
      "Epoch 3293/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 410011203.0685 - val_loss: 610310322.9521\n",
      "Epoch 3294/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 364722797.1507 - val_loss: 585683273.0959\n",
      "Epoch 3295/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 344070143.7808 - val_loss: 582339602.9589\n",
      "Epoch 3296/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 344571910.5753 - val_loss: 577168394.3836\n",
      "Epoch 3297/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 364514392.5479 - val_loss: 610905953.9726\n",
      "Epoch 3298/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 353405932.3836 - val_loss: 574337772.9315\n",
      "Epoch 3299/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 352141313.9726 - val_loss: 552171778.4110\n",
      "Epoch 3300/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 358092722.1918 - val_loss: 553912849.5342\n",
      "Epoch 3301/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 362827945.2055 - val_loss: 571231353.4247\n",
      "Epoch 3302/3350\n",
      "1168/1168 [==============================] - 0s 68us/step - loss: 350126798.9041 - val_loss: 611680147.8630\n",
      "Epoch 3303/3350\n",
      "1168/1168 [==============================] - 0s 80us/step - loss: 371714052.6027 - val_loss: 600009634.6849\n",
      "Epoch 3304/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 368430074.0822 - val_loss: 590106347.8356\n",
      "Epoch 3305/3350\n",
      "1168/1168 [==============================] - 0s 56us/step - loss: 377239827.9452 - val_loss: 589342428.0548\n",
      "Epoch 3306/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 363850448.6575 - val_loss: 548537294.0274\n",
      "Epoch 3307/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 355058956.7123 - val_loss: 538923467.2329\n",
      "Epoch 3308/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 348054090.7397 - val_loss: 564518570.7397\n",
      "Epoch 3309/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 385957811.2877 - val_loss: 570647926.0274\n",
      "Epoch 3310/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 368485941.9178 - val_loss: 556017647.2877\n",
      "Epoch 3311/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 349741747.0685 - val_loss: 565512156.1644\n",
      "Epoch 3312/3350\n",
      "1168/1168 [==============================] - 0s 67us/step - loss: 365833771.6164 - val_loss: 597355921.3151\n",
      "Epoch 3313/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 364620149.9178 - val_loss: 577440490.4658\n",
      "Epoch 3314/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 355660842.5205 - val_loss: 573617217.9726\n",
      "Epoch 3315/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 383819229.3699 - val_loss: 638799269.9178\n",
      "Epoch 3316/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 364497725.8082 - val_loss: 607490756.6027\n",
      "Epoch 3317/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 354274584.1096 - val_loss: 578706707.0753\n",
      "Epoch 3318/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 361565614.2466 - val_loss: 609655470.0548\n",
      "Epoch 3319/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 364898381.3699 - val_loss: 582633256.2192\n",
      "Epoch 3320/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 346763940.1644 - val_loss: 604840392.5479\n",
      "Epoch 3321/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 379455686.1370 - val_loss: 534970906.5205\n",
      "Epoch 3322/3350\n",
      "1168/1168 [==============================] - 0s 63us/step - loss: 338752250.7397 - val_loss: 544514411.6712\n",
      "Epoch 3323/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 344732442.3014 - val_loss: 656058398.9041\n",
      "Epoch 3324/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 351988999.0137 - val_loss: 570951596.8767\n",
      "Epoch 3325/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 352417865.2055 - val_loss: 607880938.8493\n",
      "Epoch 3326/3350\n",
      "1168/1168 [==============================] - 0s 55us/step - loss: 343017934.0274 - val_loss: 582854218.8493\n",
      "Epoch 3327/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 368917023.1233 - val_loss: 606226731.5753\n",
      "Epoch 3328/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 366304096.0000 - val_loss: 602664972.0548\n",
      "Epoch 3329/3350\n",
      "1168/1168 [==============================] - 0s 69us/step - loss: 362216041.6438 - val_loss: 643696073.6438\n",
      "Epoch 3330/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 356562111.1233 - val_loss: 587062770.9863\n",
      "Epoch 3331/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 352793877.2603 - val_loss: 626116753.0959\n",
      "Epoch 3332/3350\n",
      "1168/1168 [==============================] - 0s 66us/step - loss: 350646806.3562 - val_loss: 577187262.1370\n",
      "Epoch 3333/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 339394293.4795 - val_loss: 597598652.6027\n",
      "Epoch 3334/3350\n",
      "1168/1168 [==============================] - 0s 64us/step - loss: 357724212.6027 - val_loss: 596339242.8493\n",
      "Epoch 3335/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 343422744.5479 - val_loss: 582981405.0411\n",
      "Epoch 3336/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 344422610.8493 - val_loss: 579284302.7945\n",
      "Epoch 3337/3350\n",
      "1168/1168 [==============================] - 0s 49us/step - loss: 360423166.4658 - val_loss: 592535580.1644\n",
      "Epoch 3338/3350\n",
      "1168/1168 [==============================] - 0s 62us/step - loss: 354798409.2055 - val_loss: 614699216.8767\n",
      "Epoch 3339/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 338918262.2466 - val_loss: 572586933.8082\n",
      "Epoch 3340/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 343635262.2466 - val_loss: 588397961.2603\n",
      "Epoch 3341/3350\n",
      "1168/1168 [==============================] - 0s 45us/step - loss: 346449736.7671 - val_loss: 595822619.8356\n",
      "Epoch 3342/3350\n",
      "1168/1168 [==============================] - 0s 73us/step - loss: 335882806.7945 - val_loss: 627089490.4521\n",
      "Epoch 3343/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 357908520.3288 - val_loss: 573776845.4795\n",
      "Epoch 3344/3350\n",
      "1168/1168 [==============================] - 0s 60us/step - loss: 339331159.8904 - val_loss: 598837992.5479\n",
      "Epoch 3345/3350\n",
      "1168/1168 [==============================] - 0s 59us/step - loss: 337730520.7671 - val_loss: 575995274.4384\n",
      "Epoch 3346/3350\n",
      "1168/1168 [==============================] - 0s 71us/step - loss: 340944252.4932 - val_loss: 588956795.8904\n",
      "Epoch 3347/3350\n",
      "1168/1168 [==============================] - 0s 61us/step - loss: 342383765.9178 - val_loss: 623896740.3836\n",
      "Epoch 3348/3350\n",
      "1168/1168 [==============================] - 0s 58us/step - loss: 356967860.6027 - val_loss: 588754196.5205\n",
      "Epoch 3349/3350\n",
      "1168/1168 [==============================] - 0s 48us/step - loss: 387569407.1233 - val_loss: 659510088.7945\n",
      "Epoch 3350/3350\n",
      "1168/1168 [==============================] - 0s 74us/step - loss: 359592294.1370 - val_loss: 591296923.5068\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=3350, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
