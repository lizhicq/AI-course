{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-06fffa32dec3>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\lizhi\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\lizhi\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\lizhi\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\lizhi\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\lizhi\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist=input_data.read_data_sets('./MNIST_data',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.validation.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,train_y=mnist.train.next_batch(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x,test_y=mnist.test.next_batch(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 图像的可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAADRCAYAAACZ6CZ9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAGVlJREFUeJzt3WuQVMUZxvH/4g25BXWRGClZEzFAUIxCtMSgIl5iRCMCgZQYC6JRShNUVBAjiEgkARFEES9cQgSNFpdAVNAYMClBA2oURAwaUDQqCwEBBUQ2H7bePn12Zndnd2dme2ae35c9dB/ONIed6Xn7dL9dVFZWhoiISGga1HcDREREklEHJSIiQVIHJSIiQVIHJSIiQVIHJSIiQVIHJSIiQVIHJSIiQVIHJSIiQVIHJSIiQTqwJicXFxeXlZSUZKgpuWPDhg2UlpYW1fU6up/ldD/Tb9WqVaVlZWUt6nIN3c9IOu4n6J6aVN/zNeqgSkpKWLlyZe1blSc6deqUluvofpbT/Uy/oqKijXW9hu5nJB33E3RPTarveQ3xiYhIkNRBiYhIkNRBiYhIkNRBiYhIkNRBiYhIkNRBiYhIkNRBiYhIkNRBiYhIkGq0UDfT9u/fD8DChQtd2dSpUwF49tlnK/17zZs3d8dLliwBoHPnzplooohIQenQoYM7XrNmDQB9+/Z1ZXPmzMnYayuCEhGRIAURQS1atAiA4cOHA/Dmm2/W6O9v27bNHZ9//vkAvPvuuwAUFxeno4kijv1uAZx66qkAnHTSSQCMHz/e1Z188snZbViOmjdvnjtesGBBrO7xxx93x8ccc0zsJ0T3vWfPngD88Ic/zFg7C83y5csB2Llzpyuz0arTTjstK21QBCUiIkEKIoKaMGECkFrk1LBhQ3e8d+9eIHp2BfC///0PgOeeew6Ayy+/PG3tLET33nsvAK+88kpCXVlZGQBFRVFS4uOPPx6AgQMHAuXJMfPNN7/5TXds3yiXLl0KwJQpU1zdI488ktV25Zorr7wSgJkzZ6Z0/vvvvx/7CdF9v++++2LXBOjXrx8A5513Xh1bWpgmTpwIwMcff+zK2rZtC8Cvf/3rrLRBEZSIiARJHZSIiAQpiCG+7t27A/D3v/8dgK+++srVHXXUUQAMGTIEgIsvvtjVTZ48GYAHHnjAle3btw+AUaNGAdHDU4BGjRqlve356Prrr3fHTz75JABbtmxJOC/ZEJ9p1aoVAL/85S8z0cR61axZM3d86aWXAtEw9YwZM1zdb3/7W0ATdSrzySefpP2a/v233137CdCjR4+0v2YhGTduXFZfTxGUiIgEKYgIatiwYUA0RdwmOkC04Nb/1mrswai/iNemAP/73/8GYPv27a5OEVQif8r0X/7yFyAekSaLjlJh18rHCMr3u9/9DoimSm/YsMHVjRkzBogmmkjqDjvsMABGjx6dUDdr1ix3bLvT2siJ78svvwTg5ptvdmUtWpTv2p6tadK5YvHixe7Y7temTZsAmD9/vqvL9oQTRVAiIhIkdVAiIhKkIIb4TDpX3tvkisaNG6ftmvnEJj3YWhGA119/HYgmP1THJkJcdNFFrqxNmzYA/OIXv0hLO0N34IHlb6Fu3boBMG3aNFdna/E0xBf39ddfA7Bnz55Kz7EhuGuuucaVNWhQ/n160KBBruy9994DomGop59+2tXZ2r1169a5MhuSnTt3bu3/AXlk9+7dALz99tuuzPLtmWOPPTarbfIpghIRkSAFFUGl03e+8x0g+eSKQmbfWi0b8RtvvOHqbEKE/wD5iy++AKBJkyYADB482NX16tUrs43NIX5+OKmaRTaWBSIZm/jkR5+21MRn7/Obbrop9hOgf//+APzxj390ZfaamzdvBqJJE4Vq/fr1ANx6662u7IADDgCiz4H6/AxVBCUiIkHK6QjKogHLyeezZwISn0puz5z8yMnccccdAIwYMcKV7dixA4im9CtqSu7CCy8EYOTIka7MnuX5U6DtmVUhszyNxn9O3L59+1jdgw8+6I5/8pOfAHDcccel9DodO3YE4hGULWGxSKp3794ptjp/WIIDgHfeeafS8yzf3tFHH53xNlVGEZSIiARJHZSIiAQpp8cb/vnPfwLx1ftGQ1HRRpBXXHGFK7PNHW1ChD9F/Pbbb0+4RtOmTYH45AhJjQ2fvPzyy66sa9eu9dWceuVvkVFaWhqru/POO92xP8kB4lllLLtEqizrgZ9JwlQ1tJXvHn74YXdsU8q7dOniymxoL4RsG4qgREQkSDkdQSX7ZnTooYcChT199J577gHgtttuS6izB/dDhw4Fonxx1bFISpLr0KFD7CfA6tWr66s5wTnnnHPcsUVQ9vDdz55fUU2jJt/s2bMrrWvdunWtr5urbrzxRiAezRpbdA9w2WWXZa1N1VEEJSIiQcq5CGrVqlXuONlUaVuA6m/LXWgsZVFVmchfeuklAH7zm9+4MnsGdcghh2SwdfnJIveGDRvWc0vCUtVz4ilTpgBw8MEHZ+S1H3/88YQyW3BuOycUgs8++wyAtWvXAlGWd4jufX1OJa+KIigREQmSOigREQlSEEN8tsX7/fffD8BTTz3l6rZu3Ro71/+zZeL13XDDDZloYk45/vjjqz1n+fLlsZ8QDQVMnTo1Mw2TgrB//3537L+XjQ2HppoRoib+8Y9/uONkW8r/4Ac/AKBly5Zpf+2QfPjhh+7YJpP99a9/BaJcexDtPvD73/8+i61LnSIoEREJUtYjKNsLxv9mZRmL7YFqXdgkihNOOKHO18pVNtkh2XbrEyZMAKK9dfw8fY8++igQTaAAWLBgAZBaVFbIPvjgg9jPQvbRRx+542TfzH/1q18B0K5du7S9pn2u+Ln7km0D7093z2crVqxwx1XtfXXttddmozm1pghKRESCpA5KRESClJUhPv+h6aWXXgrAwoULE86z9TedO3d2ZTYRYuXKlSm9lm0RbZuiDRgwwNX5181ndh/91eFm/PjxsT/7Q3w2tOcP0di25Rriq9qnn34KRBNNCpmfPy8Z/z2ZLrYN/Jw5cxLq/JxyFXP95RubHDFx4sSUztcQn4iISC1kNIKyyOmSSy5xZZZh+6CDDnJltlHe1VdfDcCRRx7p6uzbVrIIys7zv7XaJoYPPfQQANOnT3d1t9xyS+z1IDc2kLN/086dOwE44ogj0nbtLVu2uGN7qLxr1y5X5h9L5Sxi97Vt2xaA008/PdvNqVfz589PKPMzuzRv3jxtr2WTeGbNmlXpOcOHD3fH+Z4lZfv27QC8+uqrlZ6T7P8nVIqgREQkSBkNH2zqp0VNPn/6qe0/Yh577DF3XDGfln/uqFGjgHh+Pht7Xbx4MRBfzHvXXXcBMG/ePFdmkVnI36wmTZoEwLRp04DkGZ5nzJjhjlN5XmRTT/1vl1988QUAjRo1cmVnn312zRtcgCz/oc9yIeZClJ5OyfK67d271x3b50Jt+SMm/fv3B+L55Yz97vp7nuWr9evXA9HyGltG4vvTn/4EwIUXXpi9htWRIigREQmSOigREQlS1scebHLEGWeckVBnQ1j+1EfL02fp8f0JDs2aNQPiw1B2PHnyZADGjRvn6jZu3AjEN5K76qqrgGgyhZ+nKhTPPPMMEJ8SXtGpp57qju2+JNt6ZNmyZUCUl8uG9XxPPPGEOw5h22fJLbbVus/Pobl06VIA+vXrV+21/OE8G8b2h/137NgRO9//LEi2lCVf9ezZE4iG9vzPsfbt2wPJl52EThGUiIgEKesRlE1ltkkMEG1RbtMf/RxaHTt2BKJvTYcffnhKr3PdddcBUYQEcPHFFwOwZMkSV2bTU3v06AFA7969U/2nZM3IkSMBGDx4MAD/+te/Es7Ztm2bO7appsnyoNmW7/YA38/qbIsdC+GhsmSOv0zk5JNPBuC1115zZTYKUlxcDMBZZ53l6mxSky24HT16tKvzM3RXZJOchgwZ4soaN25cq/bnA4uaIJpk5Y+y5ApFUCIiEiR1UCIiEqSMDvHZg7pOnTq5Mlt35K+/qejEE090xzYcV9vsCf76pmeffRaACy64wJU9//zzAKxbt65W18+GM888E4ApU6YAsGnTJldnW2R8/vnnrswmPrz11luVXtOGVfxtT9KZoUIKl/+e69atGxAf4rM1O8kmU9SU5de093Yh/Q7bNkUQ/0yA+BBf165ds9amdFMEJSIiQcpoBGVTHm0KJCTPqdewYUMg2vLdz3acbEV0XdvjT9CwVe0hTi+vKNmU7169eiWU2dTbNWvWVHot2+q5kL5xSvbZRB0/k4RtKphsQ8GqWIYKW3ICMHbsWKAwf4/9fHuWp/O4444Dwt3CvaYUQYmISJCyMs182LBhSY/ri02xhvzMk9a0aVNAi2yzyZ7p+YtD+/TpU0+tCY+/P5E9Yx4xYgQQ33/Mnl/Zs9VjjjnG1dmU9SZNmmS2sTnCX1CfrxRBiYhIkNRBiYhIkPJvfEukHti2D/ZTKjdw4MDYT5HKKIISEZEgqYMSEZEgqYMSEZEgqYMSEZEgqYMSEZEgqYMSEZEgFdkGdimdXFS0GdiYuebkjNZlZWUt6noR3U9H9zP96nxPdT9j9DuaXindzxp1UCIiItmiIT4REQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQmSOigREQnSgTU5ubi4uKykpCRDTckdGzZsoLS0tKiu19H9LKf7mX6rVq0qLSsra1GXa+h+RtJxP0H31KT6nq9RB1VSUsLKlStr36o80alTp7RcR/eznO5n+hUVFW2s6zV0PyPpuJ+ge2pSfc9riE9ERIKkDkpERIKkDkpERIKkDkpERIKkDkpERIKkDkpERIKkDkpERIJUo3VQUr9mz54NwJFHHunKunfvXl/NERHJKEVQIiISJEVQOeCxxx4D4JprrgHgRz/6katTBBW+kSNHuuM777wTgLPOOguAv/3tb/XQotwxYMAAd7xo0SIAevXq5cp69uwJ6H2wZcsWAG6++WYAnnrqKVe3c+fOav++f//uv/9+ANq2bZvOJtaKIigREQlSzkVQ9k0BYM6cOQCMGTPGlf33v/+NnX/FFVe448svvxyAc889N5NNTDv7N+/btw+ANWvWuLr//Oc/ABx77LHZb5hUaenSpUAUNSWrs58QRVUCL774IhBFTQClpaUAPPTQQ67skUceAWDo0KEA3HXXXdlqYlB69OgBwPLlyys9p1WrVgC0adPGlb3++usAvPDCC66sS5cuAKxbtw6A4uLi9Da2BhRBiYhIkNRBiYhIkHJmiO/ll18GogkDANOnT084r6govsXIrFmz3HGzZs2A3Bviq+h73/ueO9bQXriSDe1VpCG+5Lp16wbA5MmTXVnfvn0TzmvatCkQDfEVkltuucUdr1ixAoDmzZsD8MYbb7i6li1bAtCgQXk8cuCB0ce+PRLp0KGDK9u6dSsAq1evBur391IRlIiIBCn4CMoekt52221A1Kv77OEfwKBBgwDYvXs3AKNGjXJ17733XsbamUlz586N/fmQQw7J2mvb/fcfVptvf/vbANxwww2u7KCDDspOw3KAHx1VRlFT1SxCqsyPf/xjABo3bpyN5gThnXfeAaLp4ADf+MY3AFi4cCEArVu3TulaDRs2BLL7mVITiqBERCRI6qBERCRIQQ7xjR492h2PHTsWgF27diWcN378eACuvPJKV2bz+n/6058mnP/d7343nc3MmldeeSX250yv9bD1VhDd46qGqzZt2uSOJ02alLF25RMb2tMQX90cccQR9d2ErLPPg1NOOcWV2WMAP09nKizjxKeffpqm1qWXIigREQlSUBHU+vXrAbj77rtd2Z49e4Bo+qRlgwAYPHgwEM8uYbm7/DKzYcOG9DY4Ty1btswdp/Kg38/7ZfkC27dvn/Z25YJU7hfAmWeemdmG5ImPP/7YHZeVlSXUH3XUUdlsThD69esHQJ8+fVzZoYcemvLfX7t2rTv280Qa+4w9/fTTa9nC9FEEJSIiQQoigtq8eTMAZ5xxBhBFTRAtvH3iiScAOO+881ydZem1RX0AH3zwQaWv40dfUrk///nPldb5ub6uvfZaIL4o8NVXXwUKN4JKZXEu6NlTqvyI1D4L/Ocs/vPnQnHwwQfX6u99+eWXQHyBrz17siUjAPfcc0+dXiedFEGJiEiQ1EGJiEiQghjimzlzJgCfffZZQt3EiROBaPjPz8VnD+eTZZcoKSkB4ttt+Bv9SSIL920bE1/v3r0B+P73v+/Kbr31ViB6aAtw3333AYU59ALVT5LQ9PLUfP7550Dy4WabMAVRnjkp9+abbwLxaePPP/88EGWZsEwUPtu2BODoo4/OZBNrRBGUiIgEKYgIys/OXdFzzz0HwIQJE4Bog77qnHDCCUD0LR9qNhUzZA8//LA7vvfee9N23a+++gqIJq34bEGkn7Mr2RTfihtGFoqzzz47pfO0xXtqnn76aSCKpCBxp4JC52/lfvvttwPw4IMPAtF7OVV+cgSbMGGjUPVJEZSIiAQpiAjq7bffBuCAAw4A4Ouvv3Z1zzzzTI2uZXs+2VTJfIia7BmPPRuyBc0hsqzKhcIWOqa6QFeqZktM/EzdpkWLFgBcd911WW1TqMaMGeOO7Vm9fd716tXL1bVr1w6ACy64AICPPvrI1dm99CP7J598EoiPPtUXRVAiIhIkdVAiIhKkIIb4brrpJgC6du0KJA/hLY9eVQ/wAa666iogCmvzwUknnQREQ3xvvfWWq1u3bh2QvUzt/vDr/PnzE+ptY8lCkUrmiBEjRmShJflhyZIlQDw7ibGH9xriK9e3b1933LlzZwA6duwIxDNDVGTnQpRd4mc/+5krGzduHADXX389AI0aNUpTi2tOEZSIiAQpiAjKWM9ecf8jgIEDBwIwffr0hDr/20Ky7Ly5zrIK2wNQPyv7ueeeC0TfPAHatm2b9ja89tprQDyfoeVHLDSpToiwxbj5+DuZKatWrQKiKeV+BnPLlC/lTjzxxKTHNWHJC1q1auXKbH+3F198EYCLLrqotk2sM0VQIiISJHVQIiISpKCG+JKx+fkzZsxIqDvnnHOA+LBfCCni083yENq/d9GiRa7uww8/BOI58vr37w/EM06kYsWKFZXW2TYa9rO6a+RzLr5Ut9TQpoSp8TcXnTZtWqzOz1mo7XLSz/Iadu/e3ZXZZ61lqdEQn4iISAXBR1CWIyrZds+NGzcG4g/48tns2bMB+PnPf+7K5s2bB8Du3btdmWV8f+GFF2p0/WRT+GvKXtsmbbz//vt1vmZoqpok4U8p1+SI1PijI/aA3vgjA5ZpRtLvxhtvdMf2/2GjJRs3bnR1rVu3zmq7FEGJiEiQgoqgbNGYLdiFxAV7/v5OU6dOzU7DAtG0aVMgvieWsUgKYP/+/UDqmd+rYt+YUs1s3KdPHwA6depU59cOTSoZyxU1pa60tBSIFoYmM2TIkGw1p6D5yQ7Mrl27AHj33XddmSIoERER1EGJiEigghjis+2JJ0+eDESryX2Wj86fIOBvnldIDjvsMHdsw30DBgxwZXPnzgWqnjaejG2VMXz4cFfWpk0bIHu5/kKUypYayrdXc3/4wx8A+OSTTxLqbEr5t771ray2qVA9+uijCWW2ddEpp5yS7eY4iqBERCRI9RZB+VmxH3jgAQDuvvvuSs+3h6Wpbq1dKCya8hfT1efCuny0bNmyas/xF5RK1bZt2wbA2LFjgfhW7vYQ/o477sh+w3LEvn37ANi7d68rq23Gcds2fubMmQl1V199NQCHH354ra6dDoqgREQkSOqgREQkSPU2xDd06FB3PH78+ErPmzRpEhDfUEsk0/wJEalMjtAQX+ps/WKyzCWXXHIJEE3OkXL+I5Hzzz8fiA/xvfTSS0B8uNTYusg9e/YAsHbtWldnj1X8jC9NmjQB4hOv6osiKBERCVLWIyjLGWdToZPxsxb07t07000SSVBV1ORHS8ocUXOrV6+utK5Ql45Ux3Z18I/9/KS28WCy+7djx46Ea1TUoEEUq1hG+Xbt2tWhxemhCEpERIKU9QjK9hhJlifOpkraVsMALVu2zE7DRDx+lGT7P1lZVd9EpXr2PHnlypUAtGjRwtUNGzasXtoUOn+/pgULFgDRvm8AixcvrvYaHTp0AGD79u2urEuXLkA0pRzCWsqjCEpERIKkDkpERIKU9SG+QYMGAfEU+6eddhoAl112GZD61g4imeIP8SXbLFNqzx7o20+pmR49egBRRo58pghKRESClPUIqnnz5gBs3bo12y8tIiI5RBGUiIgESR2UiIgESR2UiIgESR2UiIgEqagmU2iLioo2Axsz15yc0bqsrKxF9adVTffT0f1MvzrfU93PGP2OpldK97NGHZSIiEi2aIhPRESCpA5KRESCpA5KRESCpA5KRESCpA5KRESCpA5KRESCpA5KRESCpA5KRESCpA5KRESC9H9QSJQQbfU1MgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rc('image',cmap='binary')\n",
    "for i in range(10):#打印10张图\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.imshow(train_x[i].reshape(28,28))\n",
    "    print(train_y[i])\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "plt.tight_layout()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全连接神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1():\n",
    "    model=Sequential()\n",
    "    model.add(Dense(784,activation='relu'))\n",
    "    model.add(Dense(100,activation='relu'))\n",
    "    model.add(Dense(100,activation='relu'))\n",
    "    model.add(Dense(10,activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=model1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.compile(optimizer='adam',loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "20000/20000 [==============================] - 8s 382us/step - loss: 0.0117\n",
      "Epoch 2/30\n",
      "20000/20000 [==============================] - 8s 384us/step - loss: 0.0063\n",
      "Epoch 3/30\n",
      "20000/20000 [==============================] - 8s 383us/step - loss: 0.0127\n",
      "Epoch 4/30\n",
      "20000/20000 [==============================] - 8s 384us/step - loss: 0.0082 0s -\n",
      "Epoch 5/30\n",
      "20000/20000 [==============================] - 8s 383us/step - loss: 0.0116\n",
      "Epoch 6/30\n",
      "20000/20000 [==============================] - 8s 381us/step - loss: 0.0047\n",
      "Epoch 7/30\n",
      "20000/20000 [==============================] - 8s 384us/step - loss: 0.0025\n",
      "Epoch 8/30\n",
      "20000/20000 [==============================] - 8s 386us/step - loss: 0.0036 0s - loss: 0\n",
      "Epoch 9/30\n",
      "20000/20000 [==============================] - 8s 381us/step - loss: 0.0231\n",
      "Epoch 10/30\n",
      "20000/20000 [==============================] - 8s 382us/step - loss: 0.0067\n",
      "Epoch 11/30\n",
      "20000/20000 [==============================] - 8s 386us/step - loss: 0.0035\n",
      "Epoch 12/30\n",
      "20000/20000 [==============================] - 8s 386us/step - loss: 0.0118\n",
      "Epoch 13/30\n",
      "20000/20000 [==============================] - 8s 385us/step - loss: 0.0054\n",
      "Epoch 14/30\n",
      "20000/20000 [==============================] - 8s 387us/step - loss: 0.0063\n",
      "Epoch 15/30\n",
      "20000/20000 [==============================] - 8s 387us/step - loss: 0.0084\n",
      "Epoch 16/30\n",
      "20000/20000 [==============================] - 8s 384us/step - loss: 0.0016\n",
      "Epoch 17/30\n",
      "20000/20000 [==============================] - 8s 385us/step - loss: 0.0085\n",
      "Epoch 18/30\n",
      "20000/20000 [==============================] - 8s 390us/step - loss: 0.0040\n",
      "Epoch 19/30\n",
      "20000/20000 [==============================] - 8s 388us/step - loss: 0.0031\n",
      "Epoch 20/30\n",
      "20000/20000 [==============================] - 8s 385us/step - loss: 0.0134\n",
      "Epoch 21/30\n",
      "20000/20000 [==============================] - 8s 387us/step - loss: 0.0128\n",
      "Epoch 22/30\n",
      "20000/20000 [==============================] - 8s 385us/step - loss: 0.0057\n",
      "Epoch 23/30\n",
      "20000/20000 [==============================] - 8s 390us/step - loss: 0.0030\n",
      "Epoch 24/30\n",
      "20000/20000 [==============================] - 8s 384us/step - loss: 0.0037\n",
      "Epoch 25/30\n",
      "20000/20000 [==============================] - 8s 385us/step - loss: 5.6623e-04\n",
      "Epoch 26/30\n",
      "20000/20000 [==============================] - 8s 388us/step - loss: 1.6334e-05\n",
      "Epoch 27/30\n",
      "20000/20000 [==============================] - 8s 384us/step - loss: 6.8137e-06\n",
      "Epoch 28/30\n",
      "20000/20000 [==============================] - 8s 384us/step - loss: 4.5278e-06\n",
      "Epoch 29/30\n",
      "20000/20000 [==============================] - 8s 384us/step - loss: 3.3107e-06\n",
      "Epoch 30/30\n",
      "20000/20000 [==============================] - 8s 384us/step - loss: 2.3702e-06\n"
     ]
    }
   ],
   "source": [
    "history=m.fit(train_x,train_y,epochs=30,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=m.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred.argmax(1),test_y.argmax(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trans=transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor()\n",
    "#    transforms.Normalize(()())?<-参数mean和std来自于训练集，但是transform本身会在训练和评测的时候都会使用\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 图像的Normalize\n",
    "\n",
    "目的：将图片进行归一化的缩放|(x-mean)/std\n",
    "\n",
    "思考：图片归一化后，真的不存在小于0或者大于1的outlier了吗？ 不一定\n",
    "\n",
    "思考：归一化哪部分数据？A 训练集 B 评测集 C 训练集+评测集？ A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13251467"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mnist.test.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3104802"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(mnist.test.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trans=transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,),(0.3081,))#参数mean和std来自于训练集，但是transform本身会在训练和评测的时候都会使用\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=datasets.MNIST('data',train=True,download=True,transform=data_trans)\n",
    "test_data=datasets.MNIST('data',train=False,download=True,transform=data_trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train=int(len(train_data)*0.9)\n",
    "n_validation=len(train_data)-n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,valid_data=torch.utils.data.random_split(train_data,[n_train,n_validation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54000 6000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data),len(valid_data),len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前完成了数据集的制作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator=torch.utils.data.DataLoader(train_data,shuffle=True,batch_size=batch_size)\n",
    "valid_iterator=torch.utils.data.DataLoader(valid_data,batch_size=batch_size)\n",
    "test_iterator=torch.utils.data.DataLoader(test_data,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet,self).__init__()\n",
    "        #第一层conv1卷积层，in_channel=1,output_channel=6,kernel_size=5*5,input_size=32*32,output_size=28*28\n",
    "        self.conv1=nn.Conv2d(1,6,5)\n",
    "        #第二层conv2，output_channel=6 ,kernel 5*5,output_size=10*10,input_size=14*14\n",
    "        self.conv2=nn.Conv2d(6,16,5)\n",
    "        \n",
    "        self.fc1=nn.Linear(16*5*5,120)\n",
    "        \n",
    "        self.fc2=nn.Linear(120,80)\n",
    "        \n",
    "        self.fc3=nn.Linear(80,10)#不用增加softmax层，在cross_entropy的Loss中自动增加了Softmax\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.max_pool2d(F.relu(self.conv1(x)),2)\n",
    "        x=F.max_pool2d(F.relu(self.conv2(x)),2)\n",
    "        x=x.view(x.shape[0],-1)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到此，神经网络定义完毕"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入模型并训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LeNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=80, bias=True)\n",
       "  (fc3): Linear(in_features=80, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 如何评测结果--计算精确度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accu(fx,y):\n",
    "    pred=fx.max(1,keepdim=True)[1]\n",
    "    correct=pred.eq(y.view_as(pred)).sum()#得到该batch的准确度\n",
    "    acc=correct.float()/pred.shape[0]\n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,device,iterator,optimizer,criterion):\n",
    "    epoch_loss=0#积累变量\n",
    "    epoch_acc=0#积累变量\n",
    "    model.train()#该函数表示PHASE=Train\n",
    "    \n",
    "    for (x,y) in iterator:#拿去每一个minibatch\n",
    "        x=x.to(device)\n",
    "        y=y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        fx=model(x)#进行forward\n",
    "        loss=criterion(fx,y)#计算Loss,train_loss\n",
    "        type(loss)\n",
    "        acc=accu(fx,y)#计算精确度，train_accu\n",
    "        loss.backward()#进行BP\n",
    "        optimizer.step()#统一更新模型\n",
    "        epoch_loss+=loss.item()\n",
    "        epoch_acc+=acc.item()\n",
    "        \n",
    "    return epoch_loss/len(iterator),epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,device,iterator,criterion):\n",
    "    epoch_loss=0\n",
    "    epoch_acc=0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (x,y) in iterator:\n",
    "            x=x.to(device)\n",
    "            y=y.to(device)\n",
    "            fx=model(x)\n",
    "            loss=criterion(fx,y)\n",
    "            acc=accu(fx,y)\n",
    "            epoch_loss+=loss.item()\n",
    "            epoch_acc+=acc.item()\n",
    "    return epoch_loss/len(iterator),epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "model_dir='models'\n",
    "model_path=os.path.join(model_dir,'lenet_mnist.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss=float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1|Train Loss:0.015132778076252512|Train Acc:0.9951680983412322|Val Loss:0.03702611379087605|Val Acc:0.9915226063829787\n",
      "Epoch:2|Train Loss:0.01339238127130768|Train Acc:0.9957234893364929|Val Loss:0.04205961907381549|Val Acc:0.9878656914893617\n",
      "Epoch:3|Train Loss:0.012011877664969571|Train Acc:0.9958654226143778|Val Loss:0.043958318849500425|Val Acc:0.9900265957446809\n",
      "Epoch:4|Train Loss:0.009996988230918824|Train Acc:0.9966861670616114|Val Loss:0.04160534016579944|Val Acc:0.9905252659574468\n",
      "Epoch:5|Train Loss:0.010914714325007529|Train Acc:0.9964640106635071|Val Loss:0.050777682771985515|Val Acc:0.9872007978723404\n",
      "Epoch:6|Train Loss:0.00818320645874044|Train Acc:0.9968157582938388|Val Loss:0.04803301695179432|Val Acc:0.9891954787234043\n",
      "Epoch:7|Train Loss:0.007379228564782589|Train Acc:0.9977414099526066|Val Loss:0.04465263348786121|Val Acc:0.9902482273730826\n",
      "Epoch:8|Train Loss:0.008862119750671476|Train Acc:0.9970749407582938|Val Loss:0.04056567014095948|Val Acc:0.9903590425531915\n",
      "Epoch:9|Train Loss:0.006629116438170321|Train Acc:0.9978648301973162|Val Loss:0.042162740365304846|Val Acc:0.9876994680851063\n",
      "Epoch:10|Train Loss:0.007267077122395637|Train Acc:0.9977784360189573|Val Loss:0.05748938396573067|Val Acc:0.9874778370907966\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss,train_acc=train(model,device,train_iterator,optimizer,criterion)\n",
    "    valid_loss,valid_acc=evaluate(model,device,valid_iterator,criterion)\n",
    "    if valid_loss<best_valid_loss:#如果是最好的模型就保存到文件夹\n",
    "        best_valid_loss=valid_loss\n",
    "        torch.save(model.state_dict(),model_path)\n",
    "    print('Epoch:{0}|Train Loss:{1}|Train Acc:{2}|Val Loss:{3}|Val Acc:{4}'.format(epoch+1,train_loss,train_acc,valid_loss,valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.036391854048914214 | Test Acc: 0.9892515923566879 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, device, test_iterator, criterion)\n",
    "\n",
    "print('| Test Loss: {0} | Test Acc: {1} |'.format(test_loss,test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
